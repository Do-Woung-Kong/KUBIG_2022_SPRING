{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4팀 병원 폐업 예측.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Lc9z6huX6gCx",
        "R_SfDugC650Q",
        "udDAZBEv8HiF",
        "qTl1XeCq8QHA",
        "4EGPyYKY8-ug"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train Preprocessing"
      ],
      "metadata": {
        "id": "6KIuxrxie2uN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLOkT-gW3ZMz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtORm-Bs4IHw",
        "outputId": "b5b22c8e-ada5-46b3-ce2b-0b7d95751249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data =pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ML/train.csv\")"
      ],
      "metadata": {
        "id": "h7ZAedbM3deA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "qT-SLKPP3e54",
        "outputId": "3b4d9f6d-063c-4d77-c7aa-369ed2ae990f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9bfe6256-160c-44b7-856e-539051dfeb18\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inst_id</th>\n",
              "      <th>OC</th>\n",
              "      <th>sido</th>\n",
              "      <th>sgg</th>\n",
              "      <th>openDate</th>\n",
              "      <th>bedCount</th>\n",
              "      <th>instkind</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>liquidAsset1</th>\n",
              "      <th>quickAsset1</th>\n",
              "      <th>receivableS1</th>\n",
              "      <th>inventoryAsset1</th>\n",
              "      <th>nonCAsset1</th>\n",
              "      <th>tanAsset1</th>\n",
              "      <th>OnonCAsset1</th>\n",
              "      <th>receivableL1</th>\n",
              "      <th>debt1</th>\n",
              "      <th>liquidLiabilities1</th>\n",
              "      <th>shortLoan1</th>\n",
              "      <th>NCLiabilities1</th>\n",
              "      <th>longLoan1</th>\n",
              "      <th>netAsset1</th>\n",
              "      <th>surplus1</th>\n",
              "      <th>revenue2</th>\n",
              "      <th>salescost2</th>\n",
              "      <th>sga2</th>\n",
              "      <th>salary2</th>\n",
              "      <th>noi2</th>\n",
              "      <th>noe2</th>\n",
              "      <th>interest2</th>\n",
              "      <th>ctax2</th>\n",
              "      <th>profit2</th>\n",
              "      <th>liquidAsset2</th>\n",
              "      <th>quickAsset2</th>\n",
              "      <th>receivableS2</th>\n",
              "      <th>inventoryAsset2</th>\n",
              "      <th>nonCAsset2</th>\n",
              "      <th>tanAsset2</th>\n",
              "      <th>OnonCAsset2</th>\n",
              "      <th>receivableL2</th>\n",
              "      <th>debt2</th>\n",
              "      <th>liquidLiabilities2</th>\n",
              "      <th>shortLoan2</th>\n",
              "      <th>NCLiabilities2</th>\n",
              "      <th>longLoan2</th>\n",
              "      <th>netAsset2</th>\n",
              "      <th>surplus2</th>\n",
              "      <th>employee1</th>\n",
              "      <th>employee2</th>\n",
              "      <th>ownerChange</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>open</td>\n",
              "      <td>choongnam</td>\n",
              "      <td>73</td>\n",
              "      <td>20071228</td>\n",
              "      <td>175.0</td>\n",
              "      <td>nursing_hospital</td>\n",
              "      <td>4.217530e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.961135e+09</td>\n",
              "      <td>2.033835e+09</td>\n",
              "      <td>15652441.0</td>\n",
              "      <td>1.523624e+07</td>\n",
              "      <td>1.323624e+07</td>\n",
              "      <td>31641798.0</td>\n",
              "      <td>225169678.0</td>\n",
              "      <td>1.012700e+09</td>\n",
              "      <td>9.976719e+08</td>\n",
              "      <td>4.700557e+08</td>\n",
              "      <td>1.502781e+07</td>\n",
              "      <td>2.514586e+09</td>\n",
              "      <td>2.360684e+09</td>\n",
              "      <td>1.434496e+08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.828260e+08</td>\n",
              "      <td>2.013237e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.815023e+08</td>\n",
              "      <td>3.510000e+08</td>\n",
              "      <td>2.844460e+09</td>\n",
              "      <td>1.496394e+09</td>\n",
              "      <td>4.297848e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.057422e+09</td>\n",
              "      <td>2.063787e+09</td>\n",
              "      <td>16194675.0</td>\n",
              "      <td>2.998335e+07</td>\n",
              "      <td>1.568305e+07</td>\n",
              "      <td>18808074.0</td>\n",
              "      <td>207829685.0</td>\n",
              "      <td>8.301695e+08</td>\n",
              "      <td>8.165705e+08</td>\n",
              "      <td>5.237026e+08</td>\n",
              "      <td>1.359897e+07</td>\n",
              "      <td>2.548115e+09</td>\n",
              "      <td>2.386263e+09</td>\n",
              "      <td>1.458986e+08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.589937e+08</td>\n",
              "      <td>2.228769e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.361169e+08</td>\n",
              "      <td>3.900000e+08</td>\n",
              "      <td>2.619290e+09</td>\n",
              "      <td>1.271224e+09</td>\n",
              "      <td>62.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>same</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>open</td>\n",
              "      <td>gyeongnam</td>\n",
              "      <td>32</td>\n",
              "      <td>19970401</td>\n",
              "      <td>410.0</td>\n",
              "      <td>general_hospital</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>801.0</td>\n",
              "      <td>813.0</td>\n",
              "      <td>same</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>open</td>\n",
              "      <td>gyeonggi</td>\n",
              "      <td>89</td>\n",
              "      <td>20161228</td>\n",
              "      <td>468.0</td>\n",
              "      <td>nursing_hospital</td>\n",
              "      <td>1.004522e+09</td>\n",
              "      <td>515483669.0</td>\n",
              "      <td>4.472197e+08</td>\n",
              "      <td>2.964023e+08</td>\n",
              "      <td>76156.0</td>\n",
              "      <td>3.000000e+04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>41864754.0</td>\n",
              "      <td>2.724421e+08</td>\n",
              "      <td>2.536822e+08</td>\n",
              "      <td>8.095950e+06</td>\n",
              "      <td>1.875997e+07</td>\n",
              "      <td>1.204810e+08</td>\n",
              "      <td>1.204810e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.241434e+07</td>\n",
              "      <td>9.241434e+07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.005088e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>234.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>same</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>open</td>\n",
              "      <td>incheon</td>\n",
              "      <td>141</td>\n",
              "      <td>20000814</td>\n",
              "      <td>353.0</td>\n",
              "      <td>general_hospital</td>\n",
              "      <td>7.250734e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.067740e+10</td>\n",
              "      <td>3.178605e+10</td>\n",
              "      <td>506223059.0</td>\n",
              "      <td>1.259568e+09</td>\n",
              "      <td>1.196881e+09</td>\n",
              "      <td>173769780.0</td>\n",
              "      <td>902830288.0</td>\n",
              "      <td>1.304154e+10</td>\n",
              "      <td>1.153475e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.506793e+09</td>\n",
              "      <td>4.317936e+10</td>\n",
              "      <td>3.832078e+10</td>\n",
              "      <td>3.945208e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.236380e+10</td>\n",
              "      <td>2.799188e+10</td>\n",
              "      <td>1.988520e+10</td>\n",
              "      <td>1.437192e+10</td>\n",
              "      <td>7.253040e+09</td>\n",
              "      <td>1.385710e+10</td>\n",
              "      <td>8.643659e+09</td>\n",
              "      <td>6.685834e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.492419e+10</td>\n",
              "      <td>2.971135e+10</td>\n",
              "      <td>476807804.0</td>\n",
              "      <td>1.353672e+09</td>\n",
              "      <td>1.277422e+09</td>\n",
              "      <td>218891720.0</td>\n",
              "      <td>838387466.0</td>\n",
              "      <td>1.112572e+10</td>\n",
              "      <td>9.890540e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.235181e+09</td>\n",
              "      <td>3.958356e+10</td>\n",
              "      <td>3.485576e+10</td>\n",
              "      <td>3.915906e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.775501e+10</td>\n",
              "      <td>1.701860e+10</td>\n",
              "      <td>9.219427e+09</td>\n",
              "      <td>2.073641e+10</td>\n",
              "      <td>1.510000e+10</td>\n",
              "      <td>1.295427e+10</td>\n",
              "      <td>7.740829e+09</td>\n",
              "      <td>663.0</td>\n",
              "      <td>663.0</td>\n",
              "      <td>same</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>open</td>\n",
              "      <td>gyeongnam</td>\n",
              "      <td>32</td>\n",
              "      <td>20050901</td>\n",
              "      <td>196.0</td>\n",
              "      <td>general_hospital</td>\n",
              "      <td>4.904354e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.765605e+10</td>\n",
              "      <td>2.446078e+10</td>\n",
              "      <td>112352259.0</td>\n",
              "      <td>1.419089e+09</td>\n",
              "      <td>1.307249e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>80749696.0</td>\n",
              "      <td>6.317084e+09</td>\n",
              "      <td>5.873265e+09</td>\n",
              "      <td>4.099320e+09</td>\n",
              "      <td>4.438186e+08</td>\n",
              "      <td>4.366733e+10</td>\n",
              "      <td>4.330613e+10</td>\n",
              "      <td>2.223400e+08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.989811e+10</td>\n",
              "      <td>2.890919e+10</td>\n",
              "      <td>1.793038e+10</td>\n",
              "      <td>2.098892e+10</td>\n",
              "      <td>1.350000e+10</td>\n",
              "      <td>8.631164e+07</td>\n",
              "      <td>9.025550e+09</td>\n",
              "      <td>4.808280e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.712580e+10</td>\n",
              "      <td>2.346004e+10</td>\n",
              "      <td>597748128.0</td>\n",
              "      <td>1.522108e+09</td>\n",
              "      <td>1.349851e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32642585.0</td>\n",
              "      <td>4.906776e+09</td>\n",
              "      <td>4.464017e+09</td>\n",
              "      <td>3.365227e+09</td>\n",
              "      <td>4.427591e+08</td>\n",
              "      <td>4.653138e+10</td>\n",
              "      <td>4.562945e+10</td>\n",
              "      <td>7.893407e+08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.143259e+10</td>\n",
              "      <td>3.007259e+10</td>\n",
              "      <td>1.759375e+10</td>\n",
              "      <td>2.136001e+10</td>\n",
              "      <td>1.410803e+10</td>\n",
              "      <td>5.561941e+06</td>\n",
              "      <td>9.025550e+09</td>\n",
              "      <td>206.0</td>\n",
              "      <td>197.0</td>\n",
              "      <td>same</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9bfe6256-160c-44b7-856e-539051dfeb18')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9bfe6256-160c-44b7-856e-539051dfeb18 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9bfe6256-160c-44b7-856e-539051dfeb18');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   inst_id    OC       sido  ...  employee1  employee2  ownerChange\n",
              "0        1  open  choongnam  ...       62.0       64.0         same\n",
              "1        3  open  gyeongnam  ...      801.0      813.0         same\n",
              "2        4  open   gyeonggi  ...      234.0        1.0         same\n",
              "3        7  open    incheon  ...      663.0      663.0         same\n",
              "4        9  open  gyeongnam  ...      206.0      197.0         same\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep = data.copy()\n",
        "data_prep.drop(['inst_id','sgg'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "oEbkvTgu4o7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['OC'].replace({' close':0,'open':1},inplace=True)"
      ],
      "metadata": {
        "id": "frUdwReL4tQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['OC'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJowbSHx4vqv",
        "outputId": "40f4d08e-03f0-41f8-89b0-2972d3f54065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    286\n",
              "0     15\n",
              "Name: OC, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['sido'].value_counts().index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwUPue-s4xHg",
        "outputId": "faa6f0e9-05b0-4aa3-d58a-e3ec86115953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['gyeonggi', 'gyeongbuk', 'gyeongnam', 'busan', 'choongnam', 'jeonnam',\n",
              "       'seoul', 'jeonbuk', 'daegu', 'choongbuk', 'daejeon', 'incheon',\n",
              "       'gangwon', 'ulsan', 'gwangju', 'sejong'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 경기도 13,571,450 <br>\n",
        "2 서울특별시 9,505,926<br>\n",
        "3 부산광역시 3,348,874<br>\n",
        "3 경상남도 3,311,438<br>\n",
        "3 인천광역시 2,949,150<br>\n",
        "4 경상북도 2,624,310<br>\n",
        "4 대구광역시 2,383,858<br>\n",
        "4 충청남도 2,118,638<br>\n",
        "5 전라남도 1,832,604<br>\n",
        "5 전라북도 1,785,392<br>\n",
        "5 충청북도 1,597,097<br>\n",
        "5 강원도 1,538,660<br>\n",
        "5 대전광역시 1,451,272<br>\n",
        "5 광주광역시 1,441,636<br>\n",
        "6 울산광역시 1,121,100<br>\n",
        "7.제주도 674,484<br>\n",
        "7 세종특별자치시 374,377<br>"
      ],
      "metadata": {
        "id": "8i_qq5EF5bCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['sido'].replace({'gyeonggi':6,'seoul':5,\n",
        "                           'busan':4,'gyeongnam':4,'incheon':4,\n",
        "                          'gyeongbuk':3,'daegu':3,'choongnam':3,\n",
        "                          'jeonnam':2,'jeonbuk':2,\n",
        "                          'choongbuk':2,'gangwon':2,'daejeon':2,'gwangju':2,\n",
        "                          'ulsan':1,'sejong':0,'jeju':0},inplace=True)"
      ],
      "metadata": {
        "id": "ePjE7gsI40SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['sido'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbWD1swB5Udc",
        "outputId": "68a34684-102c-45aa-b78c-1993bad31f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['openDate'] = data['openDate'].apply(lambda x: int(str(x)[:4]))"
      ],
      "metadata": {
        "id": "KyQ9VLN06Uzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['openDate']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAwVKYJb6U6T",
        "outputId": "73718860-a0bc-4b67-da27-b3c64d3dc858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      2007\n",
              "1      1997\n",
              "2      2016\n",
              "3      2000\n",
              "4      2005\n",
              "       ... \n",
              "296    2005\n",
              "297    2014\n",
              "298    1983\n",
              "299    2007\n",
              "300    2001\n",
              "Name: openDate, Length: 301, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep.rename(columns={'openDate':'openYear'},inplace=True)"
      ],
      "metadata": {
        "id": "RYPO8ATG6U8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['openYear'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6kG056K6U_E",
        "outputId": "b482ec94-3757-49e9-fb09-6181dc07fe70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### inst_kind"
      ],
      "metadata": {
        "id": "Lc9z6huX6gCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['instkind'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg6yvjtj6VBZ",
        "outputId": "9c354379-f238-4586-9b31-121cbea46fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep[data_prep['instkind'].isnull()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvef8Ds56dn8",
        "outputId": "83ba6411-0806-4a2f-f0ca-ba07ad38d0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0486b6de-ff46-4e34-8c5a-39dc1f1172dc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OC</th>\n",
              "      <th>sido</th>\n",
              "      <th>openYear</th>\n",
              "      <th>bedCount</th>\n",
              "      <th>instkind</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>liquidAsset1</th>\n",
              "      <th>quickAsset1</th>\n",
              "      <th>receivableS1</th>\n",
              "      <th>inventoryAsset1</th>\n",
              "      <th>nonCAsset1</th>\n",
              "      <th>tanAsset1</th>\n",
              "      <th>OnonCAsset1</th>\n",
              "      <th>receivableL1</th>\n",
              "      <th>debt1</th>\n",
              "      <th>liquidLiabilities1</th>\n",
              "      <th>shortLoan1</th>\n",
              "      <th>NCLiabilities1</th>\n",
              "      <th>longLoan1</th>\n",
              "      <th>netAsset1</th>\n",
              "      <th>surplus1</th>\n",
              "      <th>revenue2</th>\n",
              "      <th>salescost2</th>\n",
              "      <th>sga2</th>\n",
              "      <th>salary2</th>\n",
              "      <th>noi2</th>\n",
              "      <th>noe2</th>\n",
              "      <th>interest2</th>\n",
              "      <th>ctax2</th>\n",
              "      <th>profit2</th>\n",
              "      <th>liquidAsset2</th>\n",
              "      <th>quickAsset2</th>\n",
              "      <th>receivableS2</th>\n",
              "      <th>inventoryAsset2</th>\n",
              "      <th>nonCAsset2</th>\n",
              "      <th>tanAsset2</th>\n",
              "      <th>OnonCAsset2</th>\n",
              "      <th>receivableL2</th>\n",
              "      <th>debt2</th>\n",
              "      <th>liquidLiabilities2</th>\n",
              "      <th>shortLoan2</th>\n",
              "      <th>NCLiabilities2</th>\n",
              "      <th>longLoan2</th>\n",
              "      <th>netAsset2</th>\n",
              "      <th>surplus2</th>\n",
              "      <th>employee1</th>\n",
              "      <th>employee2</th>\n",
              "      <th>ownerChange</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2012</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>305438818.0</td>\n",
              "      <td>22416139.0</td>\n",
              "      <td>467475340.0</td>\n",
              "      <td>254868810.0</td>\n",
              "      <td>13451554.0</td>\n",
              "      <td>90129732.0</td>\n",
              "      <td>4239523.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-261130839.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>534296550.0</td>\n",
              "      <td>64355691.0</td>\n",
              "      <td>869364734.0</td>\n",
              "      <td>675718430.0</td>\n",
              "      <td>1333.0</td>\n",
              "      <td>24317070.0</td>\n",
              "      <td>5929772.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-423739612.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>change</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0486b6de-ff46-4e34-8c5a-39dc1f1172dc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0486b6de-ff46-4e34-8c5a-39dc1f1172dc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0486b6de-ff46-4e34-8c5a-39dc1f1172dc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     OC  sido  openYear  bedCount  ... surplus2  employee1  employee2  ownerChange\n",
              "193   0     6      2012       NaN  ...      0.0       15.0       15.0       change\n",
              "\n",
              "[1 rows x 56 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep.dropna(subset=['instkind'],axis=0,inplace=True)"
      ],
      "metadata": {
        "id": "SM2hBybn6fDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['instkind'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waV0EmYb6jMB",
        "outputId": "08d0c605-f5a7-4102-adca-84ac482cde89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['instkind'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKoY8WSk7_AS",
        "outputId": "c1a0f691-7514-46b3-e679-0641a3a3e459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['nursing_hospital', 'general_hospital', 'hospital',\n",
              "       'traditional_clinic', 'clinic', 'traditional_hospital',\n",
              "       'dental_clinic'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['instkind'].replace({'traditional_hospital':'others','traditional_clinic':'others','dental_clinic':'others'},inplace=True)"
      ],
      "metadata": {
        "id": "_uRcZWxB7_Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['instkind'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjO-BaP37_HP",
        "outputId": "b177aa84-9eb8-4bce-e077-91b86a47818e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['nursing_hospital', 'general_hospital', 'hospital', 'others',\n",
              "       'clinic'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### bedCount"
      ],
      "metadata": {
        "id": "R_SfDugC650Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['bedCount'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS7b42sj6kZR",
        "outputId": "8c2902d2-d801-48d0-d755-0e936dd3e188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep[data_prep['bedCount'].isnull()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "cuHhSXxp66kU",
        "outputId": "702319c3-72a3-4a3e-9674-8d93358d4d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-45aa64d9-4408-429a-a40b-8a84ebe6d6ad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OC</th>\n",
              "      <th>sido</th>\n",
              "      <th>openYear</th>\n",
              "      <th>bedCount</th>\n",
              "      <th>instkind</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>liquidAsset1</th>\n",
              "      <th>quickAsset1</th>\n",
              "      <th>receivableS1</th>\n",
              "      <th>inventoryAsset1</th>\n",
              "      <th>nonCAsset1</th>\n",
              "      <th>tanAsset1</th>\n",
              "      <th>OnonCAsset1</th>\n",
              "      <th>receivableL1</th>\n",
              "      <th>debt1</th>\n",
              "      <th>liquidLiabilities1</th>\n",
              "      <th>shortLoan1</th>\n",
              "      <th>NCLiabilities1</th>\n",
              "      <th>longLoan1</th>\n",
              "      <th>netAsset1</th>\n",
              "      <th>surplus1</th>\n",
              "      <th>revenue2</th>\n",
              "      <th>salescost2</th>\n",
              "      <th>sga2</th>\n",
              "      <th>salary2</th>\n",
              "      <th>noi2</th>\n",
              "      <th>noe2</th>\n",
              "      <th>interest2</th>\n",
              "      <th>ctax2</th>\n",
              "      <th>profit2</th>\n",
              "      <th>liquidAsset2</th>\n",
              "      <th>quickAsset2</th>\n",
              "      <th>receivableS2</th>\n",
              "      <th>inventoryAsset2</th>\n",
              "      <th>nonCAsset2</th>\n",
              "      <th>tanAsset2</th>\n",
              "      <th>OnonCAsset2</th>\n",
              "      <th>receivableL2</th>\n",
              "      <th>debt2</th>\n",
              "      <th>liquidLiabilities2</th>\n",
              "      <th>shortLoan2</th>\n",
              "      <th>NCLiabilities2</th>\n",
              "      <th>longLoan2</th>\n",
              "      <th>netAsset2</th>\n",
              "      <th>surplus2</th>\n",
              "      <th>employee1</th>\n",
              "      <th>employee2</th>\n",
              "      <th>ownerChange</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>others</td>\n",
              "      <td>4.131858e+09</td>\n",
              "      <td>1.436118e+08</td>\n",
              "      <td>3.529097e+09</td>\n",
              "      <td>2.198043e+09</td>\n",
              "      <td>56630198.0</td>\n",
              "      <td>228194537.0</td>\n",
              "      <td>113598011.0</td>\n",
              "      <td>22221340.0</td>\n",
              "      <td>2.653634e+08</td>\n",
              "      <td>1.970952e+09</td>\n",
              "      <td>1.963787e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>7165400.0</td>\n",
              "      <td>3.553691e+09</td>\n",
              "      <td>2.749986e+09</td>\n",
              "      <td>527179060.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.803885e+09</td>\n",
              "      <td>2.793826e+09</td>\n",
              "      <td>2.590000e+09</td>\n",
              "      <td>1.005862e+07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.720758e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.318141e+09</td>\n",
              "      <td>1.443104e+08</td>\n",
              "      <td>2.982098e+09</td>\n",
              "      <td>1.709276e+09</td>\n",
              "      <td>58244152.0</td>\n",
              "      <td>164510269.0</td>\n",
              "      <td>91316429.0</td>\n",
              "      <td>13350570.0</td>\n",
              "      <td>72115788.0</td>\n",
              "      <td>1.694659e+09</td>\n",
              "      <td>1.686307e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>8351300.0</td>\n",
              "      <td>3.451466e+09</td>\n",
              "      <td>2.778451e+09</td>\n",
              "      <td>410506320.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.658296e+09</td>\n",
              "      <td>2.632717e+09</td>\n",
              "      <td>2.442951e+09</td>\n",
              "      <td>2.557878e+07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.487828e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>85.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>same</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2014</td>\n",
              "      <td>NaN</td>\n",
              "      <td>hospital</td>\n",
              "      <td>6.717144e+09</td>\n",
              "      <td>8.200000e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>186359.0</td>\n",
              "      <td>139851326.0</td>\n",
              "      <td>129697525.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.622521e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>79.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>same</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1983</td>\n",
              "      <td>NaN</td>\n",
              "      <td>hospital</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.479428e+07</td>\n",
              "      <td>7.500000e+06</td>\n",
              "      <td>121022160.0</td>\n",
              "      <td>137980960.0</td>\n",
              "      <td>137980960.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-7.175308e+07</td>\n",
              "      <td>3.867238e+09</td>\n",
              "      <td>3.867238e+09</td>\n",
              "      <td>1.132312e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.569636e+09</td>\n",
              "      <td>5.419756e+09</td>\n",
              "      <td>67957340.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.249466e+09</td>\n",
              "      <td>2.050342e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.199124e+09</td>\n",
              "      <td>2.498749e+09</td>\n",
              "      <td>3.187408e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.132198e+09</td>\n",
              "      <td>2.971193e+09</td>\n",
              "      <td>3.529793e+08</td>\n",
              "      <td>1.479400e+08</td>\n",
              "      <td>173212032.0</td>\n",
              "      <td>133064432.0</td>\n",
              "      <td>107972617.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-151827207.0</td>\n",
              "      <td>4.074622e+09</td>\n",
              "      <td>4.069797e+09</td>\n",
              "      <td>1.132312e+09</td>\n",
              "      <td>4825310.0</td>\n",
              "      <td>5.554636e+09</td>\n",
              "      <td>5.419756e+09</td>\n",
              "      <td>52957340.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.370097e+09</td>\n",
              "      <td>2.170973e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.199124e+09</td>\n",
              "      <td>2.498749e+09</td>\n",
              "      <td>3.259161e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>NaN</td>\n",
              "      <td>others</td>\n",
              "      <td>1.340971e+09</td>\n",
              "      <td>8.108450e+08</td>\n",
              "      <td>5.043409e+08</td>\n",
              "      <td>1.318978e+08</td>\n",
              "      <td>2264007.0</td>\n",
              "      <td>73558495.0</td>\n",
              "      <td>73558495.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-4.550929e+07</td>\n",
              "      <td>6.624176e+08</td>\n",
              "      <td>6.197801e+08</td>\n",
              "      <td>1.545963e+07</td>\n",
              "      <td>42637474.0</td>\n",
              "      <td>1.024680e+10</td>\n",
              "      <td>1.011968e+10</td>\n",
              "      <td>86930500.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.418547e+09</td>\n",
              "      <td>3.187619e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.230928e+09</td>\n",
              "      <td>2.190000e+09</td>\n",
              "      <td>5.490668e+09</td>\n",
              "      <td>6.269440e+09</td>\n",
              "      <td>1.499012e+09</td>\n",
              "      <td>8.328015e+08</td>\n",
              "      <td>5.254614e+08</td>\n",
              "      <td>1.333380e+08</td>\n",
              "      <td>4301929.0</td>\n",
              "      <td>94393907.0</td>\n",
              "      <td>94355280.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50657033.0</td>\n",
              "      <td>4.969613e+08</td>\n",
              "      <td>4.526867e+08</td>\n",
              "      <td>1.545534e+07</td>\n",
              "      <td>44274527.0</td>\n",
              "      <td>1.038822e+10</td>\n",
              "      <td>1.026110e+10</td>\n",
              "      <td>86930500.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.349000e+09</td>\n",
              "      <td>3.123072e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.225928e+09</td>\n",
              "      <td>2.190000e+09</td>\n",
              "      <td>5.536178e+09</td>\n",
              "      <td>6.269440e+09</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>same</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45aa64d9-4408-429a-a40b-8a84ebe6d6ad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-45aa64d9-4408-429a-a40b-8a84ebe6d6ad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-45aa64d9-4408-429a-a40b-8a84ebe6d6ad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     OC  sido  openYear  ...  employee1 employee2  ownerChange\n",
              "71    0     2      1999  ...       85.0      74.0         same\n",
              "297   0     3      2014  ...       79.0      79.0         same\n",
              "298   0     3      1983  ...        NaN       NaN          NaN\n",
              "300   1     5      2001  ...       15.0      15.0         same\n",
              "\n",
              "[4 rows x 56 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['bedCount'] = data_prep['bedCount'].fillna(data_prep.groupby('instkind')['bedCount'].transform('mean'))"
      ],
      "metadata": {
        "id": "ycFyCcIO7SxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['bedCount'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XSqfBZI7q67",
        "outputId": "4eaf787c-b9d6-447a-a1ca-4b11e23f201c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 금융데이터"
      ],
      "metadata": {
        "id": "udDAZBEv8HiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep.dropna(subset=['revenue1'],axis=0,inplace=True)"
      ],
      "metadata": {
        "id": "3wCL147i77Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['salescost1'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DW1IkbL8I-V",
        "outputId": "9931d754-1342-4e50-b89f-46aaf018dc36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### employee"
      ],
      "metadata": {
        "id": "qTl1XeCq8QHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep[data_prep['employee1'].isnull()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "ePEiKCKW1v_0",
        "outputId": "4b64dc4e-81a8-4e8f-ddab-ce278c91e72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-51d2a1ba-6474-41c7-8c61-702c2454ecaf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OC</th>\n",
              "      <th>sido</th>\n",
              "      <th>openYear</th>\n",
              "      <th>bedCount</th>\n",
              "      <th>instkind</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>liquidAsset1</th>\n",
              "      <th>quickAsset1</th>\n",
              "      <th>receivableS1</th>\n",
              "      <th>inventoryAsset1</th>\n",
              "      <th>nonCAsset1</th>\n",
              "      <th>tanAsset1</th>\n",
              "      <th>OnonCAsset1</th>\n",
              "      <th>receivableL1</th>\n",
              "      <th>debt1</th>\n",
              "      <th>liquidLiabilities1</th>\n",
              "      <th>shortLoan1</th>\n",
              "      <th>NCLiabilities1</th>\n",
              "      <th>longLoan1</th>\n",
              "      <th>netAsset1</th>\n",
              "      <th>surplus1</th>\n",
              "      <th>revenue2</th>\n",
              "      <th>salescost2</th>\n",
              "      <th>sga2</th>\n",
              "      <th>salary2</th>\n",
              "      <th>noi2</th>\n",
              "      <th>noe2</th>\n",
              "      <th>interest2</th>\n",
              "      <th>ctax2</th>\n",
              "      <th>profit2</th>\n",
              "      <th>liquidAsset2</th>\n",
              "      <th>quickAsset2</th>\n",
              "      <th>receivableS2</th>\n",
              "      <th>inventoryAsset2</th>\n",
              "      <th>nonCAsset2</th>\n",
              "      <th>tanAsset2</th>\n",
              "      <th>OnonCAsset2</th>\n",
              "      <th>receivableL2</th>\n",
              "      <th>debt2</th>\n",
              "      <th>liquidLiabilities2</th>\n",
              "      <th>shortLoan2</th>\n",
              "      <th>NCLiabilities2</th>\n",
              "      <th>longLoan2</th>\n",
              "      <th>netAsset2</th>\n",
              "      <th>surplus2</th>\n",
              "      <th>employee1</th>\n",
              "      <th>employee2</th>\n",
              "      <th>ownerChange</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1994</td>\n",
              "      <td>28.00000</td>\n",
              "      <td>others</td>\n",
              "      <td>8.872427e+09</td>\n",
              "      <td>9.641249e+08</td>\n",
              "      <td>7.638920e+09</td>\n",
              "      <td>4.488958e+09</td>\n",
              "      <td>19153521.0</td>\n",
              "      <td>2.160889e+08</td>\n",
              "      <td>204479315.0</td>\n",
              "      <td>72446781.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.302115e+09</td>\n",
              "      <td>1.186743e+09</td>\n",
              "      <td>2.671059e+06</td>\n",
              "      <td>97648359.0</td>\n",
              "      <td>8.347311e+09</td>\n",
              "      <td>8.178953e+09</td>\n",
              "      <td>2200500.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.749132e+09</td>\n",
              "      <td>5.759132e+09</td>\n",
              "      <td>4.000000e+09</td>\n",
              "      <td>9.900000e+08</td>\n",
              "      <td>9.900000e+08</td>\n",
              "      <td>2.900294e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.632333e+09</td>\n",
              "      <td>8.994174e+08</td>\n",
              "      <td>7.616309e+09</td>\n",
              "      <td>4.283771e+09</td>\n",
              "      <td>132492468.0</td>\n",
              "      <td>2.188810e+08</td>\n",
              "      <td>199357358.0</td>\n",
              "      <td>30218421.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.002794e+09</td>\n",
              "      <td>8.915144e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>80868500.0</td>\n",
              "      <td>8.581435e+09</td>\n",
              "      <td>8.368729e+09</td>\n",
              "      <td>25520500.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.958278e+09</td>\n",
              "      <td>5.956278e+09</td>\n",
              "      <td>4.992758e+09</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.625952e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2007</td>\n",
              "      <td>203.00000</td>\n",
              "      <td>hospital</td>\n",
              "      <td>5.423357e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.212041e+09</td>\n",
              "      <td>2.958250e+09</td>\n",
              "      <td>23843435.0</td>\n",
              "      <td>1.288886e+08</td>\n",
              "      <td>128638631.0</td>\n",
              "      <td>11620200.0</td>\n",
              "      <td>9.465001e+07</td>\n",
              "      <td>2.851343e+09</td>\n",
              "      <td>2.734711e+09</td>\n",
              "      <td>1.085357e+09</td>\n",
              "      <td>116632010.0</td>\n",
              "      <td>5.453158e+09</td>\n",
              "      <td>5.438850e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.457667e+09</td>\n",
              "      <td>1.864667e+09</td>\n",
              "      <td>1.750000e+09</td>\n",
              "      <td>2.590000e+09</td>\n",
              "      <td>2.590000e+09</td>\n",
              "      <td>3.846834e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.073663e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.984086e+09</td>\n",
              "      <td>3.060034e+09</td>\n",
              "      <td>40391187.0</td>\n",
              "      <td>1.292911e+08</td>\n",
              "      <td>128791122.0</td>\n",
              "      <td>10915720.0</td>\n",
              "      <td>-1.023856e+07</td>\n",
              "      <td>1.181463e+09</td>\n",
              "      <td>1.161359e+09</td>\n",
              "      <td>8.314617e+08</td>\n",
              "      <td>20103590.0</td>\n",
              "      <td>5.304341e+09</td>\n",
              "      <td>5.290033e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.733619e+09</td>\n",
              "      <td>1.406195e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.590000e+09</td>\n",
              "      <td>2.590000e+09</td>\n",
              "      <td>3.752184e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2017</td>\n",
              "      <td>160.00000</td>\n",
              "      <td>nursing_hospital</td>\n",
              "      <td>3.279139e+09</td>\n",
              "      <td>3.726966e+08</td>\n",
              "      <td>3.507972e+09</td>\n",
              "      <td>2.659892e+09</td>\n",
              "      <td>1596568.0</td>\n",
              "      <td>8.306566e+07</td>\n",
              "      <td>83065655.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-6.829984e+08</td>\n",
              "      <td>4.237100e+08</td>\n",
              "      <td>3.662513e+08</td>\n",
              "      <td>1.826044e+08</td>\n",
              "      <td>57458700.0</td>\n",
              "      <td>6.990698e+09</td>\n",
              "      <td>6.972698e+09</td>\n",
              "      <td>8000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.478121e+09</td>\n",
              "      <td>6.481208e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.830000e+09</td>\n",
              "      <td>2.830000e+09</td>\n",
              "      <td>3.936287e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>103.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1999</td>\n",
              "      <td>66.00000</td>\n",
              "      <td>nursing_hospital</td>\n",
              "      <td>2.233031e+10</td>\n",
              "      <td>8.484657e+08</td>\n",
              "      <td>1.849255e+10</td>\n",
              "      <td>1.232241e+10</td>\n",
              "      <td>423628901.0</td>\n",
              "      <td>1.304817e+09</td>\n",
              "      <td>417949094.0</td>\n",
              "      <td>415894516.0</td>\n",
              "      <td>1.692204e+09</td>\n",
              "      <td>1.829292e+10</td>\n",
              "      <td>1.818429e+10</td>\n",
              "      <td>3.728960e+09</td>\n",
              "      <td>108635115.0</td>\n",
              "      <td>1.307623e+10</td>\n",
              "      <td>1.265639e+10</td>\n",
              "      <td>312415620.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.582736e+10</td>\n",
              "      <td>1.285973e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.967635e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.554179e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.239509e+10</td>\n",
              "      <td>8.492085e+08</td>\n",
              "      <td>1.805503e+10</td>\n",
              "      <td>1.138885e+10</td>\n",
              "      <td>483447584.0</td>\n",
              "      <td>1.574478e+09</td>\n",
              "      <td>571079590.0</td>\n",
              "      <td>553752070.0</td>\n",
              "      <td>1.846078e+09</td>\n",
              "      <td>1.708819e+10</td>\n",
              "      <td>1.701879e+10</td>\n",
              "      <td>3.084699e+09</td>\n",
              "      <td>69397471.0</td>\n",
              "      <td>1.239357e+10</td>\n",
              "      <td>1.173006e+10</td>\n",
              "      <td>310415620.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.563217e+10</td>\n",
              "      <td>1.288180e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.750371e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.384958e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2017</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>others</td>\n",
              "      <td>6.845503e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.619356e+08</td>\n",
              "      <td>3.014000e+08</td>\n",
              "      <td>8067090.0</td>\n",
              "      <td>2.748281e+06</td>\n",
              "      <td>1364824.0</td>\n",
              "      <td>18640111.0</td>\n",
              "      <td>1.092933e+08</td>\n",
              "      <td>1.332665e+09</td>\n",
              "      <td>1.331507e+09</td>\n",
              "      <td>6.243635e+07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.721058e+08</td>\n",
              "      <td>5.021058e+08</td>\n",
              "      <td>270000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.528087e+07</td>\n",
              "      <td>9.491087e+07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.700000e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.009490e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.745608e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>7.560969e+08</td>\n",
              "      <td>3.638300e+08</td>\n",
              "      <td>15339220.0</td>\n",
              "      <td>4.402666e+06</td>\n",
              "      <td>617047.0</td>\n",
              "      <td>8050806.0</td>\n",
              "      <td>2.134966e+07</td>\n",
              "      <td>1.172968e+09</td>\n",
              "      <td>1.171014e+09</td>\n",
              "      <td>4.916044e+07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.968085e+08</td>\n",
              "      <td>5.268085e+08</td>\n",
              "      <td>270000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.957960e+07</td>\n",
              "      <td>6.920960e+07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.700000e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.900197e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2002</td>\n",
              "      <td>39.00000</td>\n",
              "      <td>hospital</td>\n",
              "      <td>3.007769e+09</td>\n",
              "      <td>1.197756e+09</td>\n",
              "      <td>2.258240e+09</td>\n",
              "      <td>1.775322e+09</td>\n",
              "      <td>611489427.0</td>\n",
              "      <td>5.279657e+07</td>\n",
              "      <td>52076570.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.104651e+08</td>\n",
              "      <td>9.443247e+08</td>\n",
              "      <td>8.065135e+08</td>\n",
              "      <td>4.782392e+08</td>\n",
              "      <td>137811283.0</td>\n",
              "      <td>2.105344e+09</td>\n",
              "      <td>1.999416e+09</td>\n",
              "      <td>100700000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.898375e+09</td>\n",
              "      <td>3.590747e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.539300e+09</td>\n",
              "      <td>1.539300e+09</td>\n",
              "      <td>1.151294e+09</td>\n",
              "      <td>359919226.0</td>\n",
              "      <td>2.890536e+09</td>\n",
              "      <td>1.210112e+09</td>\n",
              "      <td>2.202889e+09</td>\n",
              "      <td>1.781860e+09</td>\n",
              "      <td>672171687.0</td>\n",
              "      <td>5.485612e+07</td>\n",
              "      <td>51537693.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.485052e+07</td>\n",
              "      <td>7.930804e+08</td>\n",
              "      <td>7.381844e+08</td>\n",
              "      <td>5.172024e+08</td>\n",
              "      <td>54896000.0</td>\n",
              "      <td>2.107922e+09</td>\n",
              "      <td>2.001994e+09</td>\n",
              "      <td>100700000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.883430e+09</td>\n",
              "      <td>5.315201e+08</td>\n",
              "      <td>2.000000e+08</td>\n",
              "      <td>1.351910e+09</td>\n",
              "      <td>1.201910e+09</td>\n",
              "      <td>1.017573e+09</td>\n",
              "      <td>226197450.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1988</td>\n",
              "      <td>119.00000</td>\n",
              "      <td>hospital</td>\n",
              "      <td>9.618709e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.751211e+09</td>\n",
              "      <td>4.979714e+09</td>\n",
              "      <td>813200615.0</td>\n",
              "      <td>5.543432e+08</td>\n",
              "      <td>27044684.0</td>\n",
              "      <td>21939910.0</td>\n",
              "      <td>1.044148e+08</td>\n",
              "      <td>3.764172e+09</td>\n",
              "      <td>3.597632e+09</td>\n",
              "      <td>3.138857e+07</td>\n",
              "      <td>166540594.0</td>\n",
              "      <td>6.189316e+09</td>\n",
              "      <td>4.821055e+09</td>\n",
              "      <td>219444548.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.946246e+09</td>\n",
              "      <td>1.736071e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.210175e+09</td>\n",
              "      <td>4.681600e+08</td>\n",
              "      <td>5.007242e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.840289e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>8.680639e+09</td>\n",
              "      <td>4.662577e+09</td>\n",
              "      <td>725834415.0</td>\n",
              "      <td>5.623515e+08</td>\n",
              "      <td>23803837.0</td>\n",
              "      <td>52706310.0</td>\n",
              "      <td>2.704269e+08</td>\n",
              "      <td>3.496540e+09</td>\n",
              "      <td>3.311449e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>185091016.0</td>\n",
              "      <td>5.823204e+09</td>\n",
              "      <td>4.794284e+09</td>\n",
              "      <td>62004548.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.416916e+09</td>\n",
              "      <td>1.524426e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.892490e+09</td>\n",
              "      <td>5.462400e+08</td>\n",
              "      <td>4.902827e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1983</td>\n",
              "      <td>96.22093</td>\n",
              "      <td>hospital</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.479428e+07</td>\n",
              "      <td>7.500000e+06</td>\n",
              "      <td>121022160.0</td>\n",
              "      <td>1.379810e+08</td>\n",
              "      <td>137980960.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-7.175308e+07</td>\n",
              "      <td>3.867238e+09</td>\n",
              "      <td>3.867238e+09</td>\n",
              "      <td>1.132312e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.569636e+09</td>\n",
              "      <td>5.419756e+09</td>\n",
              "      <td>67957340.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.249466e+09</td>\n",
              "      <td>2.050342e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.199124e+09</td>\n",
              "      <td>2.498749e+09</td>\n",
              "      <td>3.187408e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.132198e+09</td>\n",
              "      <td>2.971193e+09</td>\n",
              "      <td>3.529793e+08</td>\n",
              "      <td>1.479400e+08</td>\n",
              "      <td>173212032.0</td>\n",
              "      <td>1.330644e+08</td>\n",
              "      <td>107972617.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.518272e+08</td>\n",
              "      <td>4.074622e+09</td>\n",
              "      <td>4.069797e+09</td>\n",
              "      <td>1.132312e+09</td>\n",
              "      <td>4825310.0</td>\n",
              "      <td>5.554636e+09</td>\n",
              "      <td>5.419756e+09</td>\n",
              "      <td>52957340.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.370097e+09</td>\n",
              "      <td>2.170973e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.199124e+09</td>\n",
              "      <td>2.498749e+09</td>\n",
              "      <td>3.259161e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51d2a1ba-6474-41c7-8c61-702c2454ecaf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51d2a1ba-6474-41c7-8c61-702c2454ecaf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51d2a1ba-6474-41c7-8c61-702c2454ecaf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     OC  sido  openYear  ...  employee1 employee2  ownerChange\n",
              "48    1     5      1994  ...        NaN       NaN          NaN\n",
              "93    0     3      2007  ...        NaN       NaN          NaN\n",
              "206   1     4      2017  ...        NaN     103.0          NaN\n",
              "212   0     2      1999  ...        NaN       NaN          NaN\n",
              "242   1     4      2017  ...        NaN       NaN          NaN\n",
              "263   1     2      2002  ...        NaN       NaN          NaN\n",
              "285   1     3      1988  ...        NaN       NaN          NaN\n",
              "298   0     3      1983  ...        NaN       NaN          NaN\n",
              "\n",
              "[8 rows x 56 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep[data_prep['employee2'].isnull()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "7GKJb_2H19yI",
        "outputId": "4bf897e9-e19d-47bb-ddb3-1ba06fa3fa15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bccbe157-74a0-4c3d-a053-dafabfe5757b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OC</th>\n",
              "      <th>sido</th>\n",
              "      <th>openYear</th>\n",
              "      <th>bedCount</th>\n",
              "      <th>instkind</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>liquidAsset1</th>\n",
              "      <th>quickAsset1</th>\n",
              "      <th>receivableS1</th>\n",
              "      <th>inventoryAsset1</th>\n",
              "      <th>nonCAsset1</th>\n",
              "      <th>tanAsset1</th>\n",
              "      <th>OnonCAsset1</th>\n",
              "      <th>receivableL1</th>\n",
              "      <th>debt1</th>\n",
              "      <th>liquidLiabilities1</th>\n",
              "      <th>shortLoan1</th>\n",
              "      <th>NCLiabilities1</th>\n",
              "      <th>longLoan1</th>\n",
              "      <th>netAsset1</th>\n",
              "      <th>surplus1</th>\n",
              "      <th>revenue2</th>\n",
              "      <th>salescost2</th>\n",
              "      <th>sga2</th>\n",
              "      <th>salary2</th>\n",
              "      <th>noi2</th>\n",
              "      <th>noe2</th>\n",
              "      <th>interest2</th>\n",
              "      <th>ctax2</th>\n",
              "      <th>profit2</th>\n",
              "      <th>liquidAsset2</th>\n",
              "      <th>quickAsset2</th>\n",
              "      <th>receivableS2</th>\n",
              "      <th>inventoryAsset2</th>\n",
              "      <th>nonCAsset2</th>\n",
              "      <th>tanAsset2</th>\n",
              "      <th>OnonCAsset2</th>\n",
              "      <th>receivableL2</th>\n",
              "      <th>debt2</th>\n",
              "      <th>liquidLiabilities2</th>\n",
              "      <th>shortLoan2</th>\n",
              "      <th>NCLiabilities2</th>\n",
              "      <th>longLoan2</th>\n",
              "      <th>netAsset2</th>\n",
              "      <th>surplus2</th>\n",
              "      <th>employee1</th>\n",
              "      <th>employee2</th>\n",
              "      <th>ownerChange</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2013</td>\n",
              "      <td>132.00000</td>\n",
              "      <td>nursing_hospital</td>\n",
              "      <td>2.681766e+09</td>\n",
              "      <td>1.802009e+08</td>\n",
              "      <td>2.014580e+09</td>\n",
              "      <td>9.366451e+08</td>\n",
              "      <td>59924069.0</td>\n",
              "      <td>4.287212e+08</td>\n",
              "      <td>284069476.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.181879e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.093132e+09</td>\n",
              "      <td>1.428541e+08</td>\n",
              "      <td>1.751390e+09</td>\n",
              "      <td>9.164822e+08</td>\n",
              "      <td>21947086.0</td>\n",
              "      <td>1.472983e+08</td>\n",
              "      <td>113100675.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.353718e+07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>change</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1994</td>\n",
              "      <td>28.00000</td>\n",
              "      <td>others</td>\n",
              "      <td>8.872427e+09</td>\n",
              "      <td>9.641249e+08</td>\n",
              "      <td>7.638920e+09</td>\n",
              "      <td>4.488958e+09</td>\n",
              "      <td>19153521.0</td>\n",
              "      <td>2.160889e+08</td>\n",
              "      <td>204479315.0</td>\n",
              "      <td>72446781.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.302115e+09</td>\n",
              "      <td>1.186743e+09</td>\n",
              "      <td>2.671059e+06</td>\n",
              "      <td>97648359.0</td>\n",
              "      <td>8.347311e+09</td>\n",
              "      <td>8.178953e+09</td>\n",
              "      <td>2200500.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.749132e+09</td>\n",
              "      <td>5.759132e+09</td>\n",
              "      <td>4.000000e+09</td>\n",
              "      <td>9.900000e+08</td>\n",
              "      <td>9.900000e+08</td>\n",
              "      <td>2.900294e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.632333e+09</td>\n",
              "      <td>8.994174e+08</td>\n",
              "      <td>7.616309e+09</td>\n",
              "      <td>4.283771e+09</td>\n",
              "      <td>132492468.0</td>\n",
              "      <td>2.188810e+08</td>\n",
              "      <td>199357358.0</td>\n",
              "      <td>30218421.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.002794e+09</td>\n",
              "      <td>8.915144e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>80868500.0</td>\n",
              "      <td>8.581435e+09</td>\n",
              "      <td>8.368729e+09</td>\n",
              "      <td>25520500.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.958278e+09</td>\n",
              "      <td>5.956278e+09</td>\n",
              "      <td>4.992758e+09</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.625952e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2017</td>\n",
              "      <td>128.00000</td>\n",
              "      <td>nursing_hospital</td>\n",
              "      <td>5.236406e+08</td>\n",
              "      <td>2.306773e+07</td>\n",
              "      <td>8.355775e+08</td>\n",
              "      <td>5.644636e+08</td>\n",
              "      <td>7534205.0</td>\n",
              "      <td>5.263060e+07</td>\n",
              "      <td>52630597.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.801011e+08</td>\n",
              "      <td>6.073662e+08</td>\n",
              "      <td>5.953914e+08</td>\n",
              "      <td>1.015000e+06</td>\n",
              "      <td>11974763.0</td>\n",
              "      <td>3.629704e+09</td>\n",
              "      <td>3.629704e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.513950e+09</td>\n",
              "      <td>3.342069e+09</td>\n",
              "      <td>2.918153e+09</td>\n",
              "      <td>1.718817e+08</td>\n",
              "      <td>1.718817e+08</td>\n",
              "      <td>7.231197e+08</td>\n",
              "      <td>723119742.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2007</td>\n",
              "      <td>203.00000</td>\n",
              "      <td>hospital</td>\n",
              "      <td>5.423357e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.212041e+09</td>\n",
              "      <td>2.958250e+09</td>\n",
              "      <td>23843435.0</td>\n",
              "      <td>1.288886e+08</td>\n",
              "      <td>128638631.0</td>\n",
              "      <td>11620200.0</td>\n",
              "      <td>9.465001e+07</td>\n",
              "      <td>2.851343e+09</td>\n",
              "      <td>2.734711e+09</td>\n",
              "      <td>1.085357e+09</td>\n",
              "      <td>116632010.0</td>\n",
              "      <td>5.453158e+09</td>\n",
              "      <td>5.438850e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.457667e+09</td>\n",
              "      <td>1.864667e+09</td>\n",
              "      <td>1.750000e+09</td>\n",
              "      <td>2.590000e+09</td>\n",
              "      <td>2.590000e+09</td>\n",
              "      <td>3.846834e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.073663e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.984086e+09</td>\n",
              "      <td>3.060034e+09</td>\n",
              "      <td>40391187.0</td>\n",
              "      <td>1.292911e+08</td>\n",
              "      <td>128791122.0</td>\n",
              "      <td>10915720.0</td>\n",
              "      <td>-1.023856e+07</td>\n",
              "      <td>1.181463e+09</td>\n",
              "      <td>1.161359e+09</td>\n",
              "      <td>8.314617e+08</td>\n",
              "      <td>20103590.0</td>\n",
              "      <td>5.304341e+09</td>\n",
              "      <td>5.290033e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.733619e+09</td>\n",
              "      <td>1.406195e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.590000e+09</td>\n",
              "      <td>2.590000e+09</td>\n",
              "      <td>3.752184e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1999</td>\n",
              "      <td>66.00000</td>\n",
              "      <td>nursing_hospital</td>\n",
              "      <td>2.233031e+10</td>\n",
              "      <td>8.484657e+08</td>\n",
              "      <td>1.849255e+10</td>\n",
              "      <td>1.232241e+10</td>\n",
              "      <td>423628901.0</td>\n",
              "      <td>1.304817e+09</td>\n",
              "      <td>417949094.0</td>\n",
              "      <td>415894516.0</td>\n",
              "      <td>1.692204e+09</td>\n",
              "      <td>1.829292e+10</td>\n",
              "      <td>1.818429e+10</td>\n",
              "      <td>3.728960e+09</td>\n",
              "      <td>108635115.0</td>\n",
              "      <td>1.307623e+10</td>\n",
              "      <td>1.265639e+10</td>\n",
              "      <td>312415620.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.582736e+10</td>\n",
              "      <td>1.285973e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.967635e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.554179e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.239509e+10</td>\n",
              "      <td>8.492085e+08</td>\n",
              "      <td>1.805503e+10</td>\n",
              "      <td>1.138885e+10</td>\n",
              "      <td>483447584.0</td>\n",
              "      <td>1.574478e+09</td>\n",
              "      <td>571079590.0</td>\n",
              "      <td>553752070.0</td>\n",
              "      <td>1.846078e+09</td>\n",
              "      <td>1.708819e+10</td>\n",
              "      <td>1.701879e+10</td>\n",
              "      <td>3.084699e+09</td>\n",
              "      <td>69397471.0</td>\n",
              "      <td>1.239357e+10</td>\n",
              "      <td>1.173006e+10</td>\n",
              "      <td>310415620.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.563217e+10</td>\n",
              "      <td>1.288180e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.750371e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.384958e+10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1999</td>\n",
              "      <td>340.00000</td>\n",
              "      <td>general_hospital</td>\n",
              "      <td>2.115853e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.794506e+07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.736403e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>461.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>same</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2017</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>others</td>\n",
              "      <td>6.845503e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.619356e+08</td>\n",
              "      <td>3.014000e+08</td>\n",
              "      <td>8067090.0</td>\n",
              "      <td>2.748281e+06</td>\n",
              "      <td>1364824.0</td>\n",
              "      <td>18640111.0</td>\n",
              "      <td>1.092933e+08</td>\n",
              "      <td>1.332665e+09</td>\n",
              "      <td>1.331507e+09</td>\n",
              "      <td>6.243635e+07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.721058e+08</td>\n",
              "      <td>5.021058e+08</td>\n",
              "      <td>270000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.528087e+07</td>\n",
              "      <td>9.491087e+07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.700000e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.009490e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.745608e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>7.560969e+08</td>\n",
              "      <td>3.638300e+08</td>\n",
              "      <td>15339220.0</td>\n",
              "      <td>4.402666e+06</td>\n",
              "      <td>617047.0</td>\n",
              "      <td>8050806.0</td>\n",
              "      <td>2.134966e+07</td>\n",
              "      <td>1.172968e+09</td>\n",
              "      <td>1.171014e+09</td>\n",
              "      <td>4.916044e+07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.968085e+08</td>\n",
              "      <td>5.268085e+08</td>\n",
              "      <td>270000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.957960e+07</td>\n",
              "      <td>6.920960e+07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.700000e+05</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.900197e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2002</td>\n",
              "      <td>39.00000</td>\n",
              "      <td>hospital</td>\n",
              "      <td>3.007769e+09</td>\n",
              "      <td>1.197756e+09</td>\n",
              "      <td>2.258240e+09</td>\n",
              "      <td>1.775322e+09</td>\n",
              "      <td>611489427.0</td>\n",
              "      <td>5.279657e+07</td>\n",
              "      <td>52076570.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.104651e+08</td>\n",
              "      <td>9.443247e+08</td>\n",
              "      <td>8.065135e+08</td>\n",
              "      <td>4.782392e+08</td>\n",
              "      <td>137811283.0</td>\n",
              "      <td>2.105344e+09</td>\n",
              "      <td>1.999416e+09</td>\n",
              "      <td>100700000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.898375e+09</td>\n",
              "      <td>3.590747e+08</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.539300e+09</td>\n",
              "      <td>1.539300e+09</td>\n",
              "      <td>1.151294e+09</td>\n",
              "      <td>359919226.0</td>\n",
              "      <td>2.890536e+09</td>\n",
              "      <td>1.210112e+09</td>\n",
              "      <td>2.202889e+09</td>\n",
              "      <td>1.781860e+09</td>\n",
              "      <td>672171687.0</td>\n",
              "      <td>5.485612e+07</td>\n",
              "      <td>51537693.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.485052e+07</td>\n",
              "      <td>7.930804e+08</td>\n",
              "      <td>7.381844e+08</td>\n",
              "      <td>5.172024e+08</td>\n",
              "      <td>54896000.0</td>\n",
              "      <td>2.107922e+09</td>\n",
              "      <td>2.001994e+09</td>\n",
              "      <td>100700000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.883430e+09</td>\n",
              "      <td>5.315201e+08</td>\n",
              "      <td>2.000000e+08</td>\n",
              "      <td>1.351910e+09</td>\n",
              "      <td>1.201910e+09</td>\n",
              "      <td>1.017573e+09</td>\n",
              "      <td>226197450.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1988</td>\n",
              "      <td>119.00000</td>\n",
              "      <td>hospital</td>\n",
              "      <td>9.618709e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>9.751211e+09</td>\n",
              "      <td>4.979714e+09</td>\n",
              "      <td>813200615.0</td>\n",
              "      <td>5.543432e+08</td>\n",
              "      <td>27044684.0</td>\n",
              "      <td>21939910.0</td>\n",
              "      <td>1.044148e+08</td>\n",
              "      <td>3.764172e+09</td>\n",
              "      <td>3.597632e+09</td>\n",
              "      <td>3.138857e+07</td>\n",
              "      <td>166540594.0</td>\n",
              "      <td>6.189316e+09</td>\n",
              "      <td>4.821055e+09</td>\n",
              "      <td>219444548.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.946246e+09</td>\n",
              "      <td>1.736071e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.210175e+09</td>\n",
              "      <td>4.681600e+08</td>\n",
              "      <td>5.007242e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.840289e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>8.680639e+09</td>\n",
              "      <td>4.662577e+09</td>\n",
              "      <td>725834415.0</td>\n",
              "      <td>5.623515e+08</td>\n",
              "      <td>23803837.0</td>\n",
              "      <td>52706310.0</td>\n",
              "      <td>2.704269e+08</td>\n",
              "      <td>3.496540e+09</td>\n",
              "      <td>3.311449e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>185091016.0</td>\n",
              "      <td>5.823204e+09</td>\n",
              "      <td>4.794284e+09</td>\n",
              "      <td>62004548.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.416916e+09</td>\n",
              "      <td>1.524426e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.892490e+09</td>\n",
              "      <td>5.462400e+08</td>\n",
              "      <td>4.902827e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1983</td>\n",
              "      <td>96.22093</td>\n",
              "      <td>hospital</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>5.479428e+07</td>\n",
              "      <td>7.500000e+06</td>\n",
              "      <td>121022160.0</td>\n",
              "      <td>1.379810e+08</td>\n",
              "      <td>137980960.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-7.175308e+07</td>\n",
              "      <td>3.867238e+09</td>\n",
              "      <td>3.867238e+09</td>\n",
              "      <td>1.132312e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.569636e+09</td>\n",
              "      <td>5.419756e+09</td>\n",
              "      <td>67957340.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.249466e+09</td>\n",
              "      <td>2.050342e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.199124e+09</td>\n",
              "      <td>2.498749e+09</td>\n",
              "      <td>3.187408e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.132198e+09</td>\n",
              "      <td>2.971193e+09</td>\n",
              "      <td>3.529793e+08</td>\n",
              "      <td>1.479400e+08</td>\n",
              "      <td>173212032.0</td>\n",
              "      <td>1.330644e+08</td>\n",
              "      <td>107972617.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.518272e+08</td>\n",
              "      <td>4.074622e+09</td>\n",
              "      <td>4.069797e+09</td>\n",
              "      <td>1.132312e+09</td>\n",
              "      <td>4825310.0</td>\n",
              "      <td>5.554636e+09</td>\n",
              "      <td>5.419756e+09</td>\n",
              "      <td>52957340.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.370097e+09</td>\n",
              "      <td>2.170973e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.199124e+09</td>\n",
              "      <td>2.498749e+09</td>\n",
              "      <td>3.259161e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2007</td>\n",
              "      <td>180.00000</td>\n",
              "      <td>nursing_hospital</td>\n",
              "      <td>2.116892e+09</td>\n",
              "      <td>2.681748e+08</td>\n",
              "      <td>3.286245e+09</td>\n",
              "      <td>2.231944e+09</td>\n",
              "      <td>123665065.0</td>\n",
              "      <td>5.366919e+08</td>\n",
              "      <td>514368798.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.850554e+09</td>\n",
              "      <td>4.929370e+09</td>\n",
              "      <td>4.910840e+09</td>\n",
              "      <td>2.834734e+07</td>\n",
              "      <td>18529330.0</td>\n",
              "      <td>1.196624e+10</td>\n",
              "      <td>4.441451e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73742310.0</td>\n",
              "      <td>5.804268e+08</td>\n",
              "      <td>7.507060e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.096872e+09</td>\n",
              "      <td>4.861985e+08</td>\n",
              "      <td>4.750651e+09</td>\n",
              "      <td>2.896997e+09</td>\n",
              "      <td>10545075.0</td>\n",
              "      <td>1.600243e+09</td>\n",
              "      <td>746156653.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.729675e+09</td>\n",
              "      <td>5.896047e+09</td>\n",
              "      <td>5.840526e+09</td>\n",
              "      <td>2.526034e+07</td>\n",
              "      <td>55521367.0</td>\n",
              "      <td>1.197633e+10</td>\n",
              "      <td>4.441451e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65242310.0</td>\n",
              "      <td>6.392336e+08</td>\n",
              "      <td>6.400000e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bccbe157-74a0-4c3d-a053-dafabfe5757b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bccbe157-74a0-4c3d-a053-dafabfe5757b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bccbe157-74a0-4c3d-a053-dafabfe5757b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     OC  sido  openYear  ...  employee1 employee2  ownerChange\n",
              "35    0     6      2013  ...       51.0       NaN       change\n",
              "48    1     5      1994  ...        NaN       NaN          NaN\n",
              "62    1     4      2017  ...       38.0       NaN          NaN\n",
              "93    0     3      2007  ...        NaN       NaN          NaN\n",
              "212   0     2      1999  ...        NaN       NaN          NaN\n",
              "230   1     2      1999  ...      461.0       NaN         same\n",
              "242   1     4      2017  ...        NaN       NaN          NaN\n",
              "263   1     2      2002  ...        NaN       NaN          NaN\n",
              "285   1     3      1988  ...        NaN       NaN          NaN\n",
              "298   0     3      1983  ...        NaN       NaN          NaN\n",
              "299   0     6      2007  ...      100.0       NaN          NaN\n",
              "\n",
              "[11 rows x 56 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['employee1'] = data_prep['employee1'].fillna(data_prep.groupby('instkind')['employee1'].transform('mean'))\n",
        "data_prep['employee2'] = data_prep['employee2'].fillna(data_prep.groupby('instkind')['employee2'].transform('mean'))"
      ],
      "metadata": {
        "id": "Dt4oY2DB8KCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ownerChange"
      ],
      "metadata": {
        "id": "4EGPyYKY8-ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['ownerChange'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we5Qnxf289yJ",
        "outputId": "16b4313b-2be2-4151-9d78-3b7dabde5f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "round(data_prep['ownerChange'].value_counts(normalize=True),3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbZv3uNb9E0J",
        "outputId": "7405f58d-639a-4a2f-a0cd-a703d4c1c628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "same      0.855\n",
              "change    0.145\n",
              "Name: ownerChange, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['ownerChange'].index[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXjsErns9GRu",
        "outputId": "6dd13088-57d9-4f49-a667-853d95910846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['ownerChange'] = data_prep['ownerChange'].fillna(pd.Series(np.random.choice(['same', 'change'], \n",
        "                                                      p=[0.85, 0.15],size=300)))"
      ],
      "metadata": {
        "id": "a7gOSSn79Iad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep['ownerChange'].isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSZsknjI9JiJ",
        "outputId": "5f2f9539-8418-4371-ad3b-8354b00b7dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 결측치 확인"
      ],
      "metadata": {
        "id": "jhITl85U9Lhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBoMClCX9Kr2",
        "outputId": "7034b176-de95-4cfc-db30-008b78609619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OC                    0\n",
              "sido                  0\n",
              "openYear              0\n",
              "bedCount              0\n",
              "instkind              0\n",
              "revenue1              0\n",
              "salescost1            0\n",
              "sga1                  0\n",
              "salary1               0\n",
              "noi1                  0\n",
              "noe1                  0\n",
              "interest1             0\n",
              "ctax1                 0\n",
              "profit1               0\n",
              "liquidAsset1          0\n",
              "quickAsset1           0\n",
              "receivableS1          0\n",
              "inventoryAsset1       0\n",
              "nonCAsset1            0\n",
              "tanAsset1             0\n",
              "OnonCAsset1           0\n",
              "receivableL1          0\n",
              "debt1                 0\n",
              "liquidLiabilities1    0\n",
              "shortLoan1            0\n",
              "NCLiabilities1        0\n",
              "longLoan1             0\n",
              "netAsset1             0\n",
              "surplus1              0\n",
              "revenue2              0\n",
              "salescost2            0\n",
              "sga2                  0\n",
              "salary2               0\n",
              "noi2                  0\n",
              "noe2                  0\n",
              "interest2             0\n",
              "ctax2                 0\n",
              "profit2               0\n",
              "liquidAsset2          0\n",
              "quickAsset2           0\n",
              "receivableS2          0\n",
              "inventoryAsset2       0\n",
              "nonCAsset2            0\n",
              "tanAsset2             0\n",
              "OnonCAsset2           0\n",
              "receivableL2          0\n",
              "debt2                 0\n",
              "liquidLiabilities2    0\n",
              "shortLoan2            0\n",
              "NCLiabilities2        0\n",
              "longLoan2             0\n",
              "netAsset2             0\n",
              "surplus2              0\n",
              "employee1             0\n",
              "employee2             0\n",
              "ownerChange           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB7dGi8u9OZf",
        "outputId": "1e194100-70ea-484b-d0ec-b79be8cab220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 292 entries, 0 to 300\n",
            "Data columns (total 56 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   OC                  292 non-null    int64  \n",
            " 1   sido                292 non-null    int64  \n",
            " 2   openYear            292 non-null    int64  \n",
            " 3   bedCount            292 non-null    float64\n",
            " 4   instkind            292 non-null    object \n",
            " 5   revenue1            292 non-null    float64\n",
            " 6   salescost1          292 non-null    float64\n",
            " 7   sga1                292 non-null    float64\n",
            " 8   salary1             292 non-null    float64\n",
            " 9   noi1                292 non-null    float64\n",
            " 10  noe1                292 non-null    float64\n",
            " 11  interest1           292 non-null    float64\n",
            " 12  ctax1               292 non-null    float64\n",
            " 13  profit1             292 non-null    float64\n",
            " 14  liquidAsset1        292 non-null    float64\n",
            " 15  quickAsset1         292 non-null    float64\n",
            " 16  receivableS1        292 non-null    float64\n",
            " 17  inventoryAsset1     292 non-null    float64\n",
            " 18  nonCAsset1          292 non-null    float64\n",
            " 19  tanAsset1           292 non-null    float64\n",
            " 20  OnonCAsset1         292 non-null    float64\n",
            " 21  receivableL1        292 non-null    float64\n",
            " 22  debt1               292 non-null    float64\n",
            " 23  liquidLiabilities1  292 non-null    float64\n",
            " 24  shortLoan1          292 non-null    float64\n",
            " 25  NCLiabilities1      292 non-null    float64\n",
            " 26  longLoan1           292 non-null    float64\n",
            " 27  netAsset1           292 non-null    float64\n",
            " 28  surplus1            292 non-null    float64\n",
            " 29  revenue2            292 non-null    float64\n",
            " 30  salescost2          292 non-null    float64\n",
            " 31  sga2                292 non-null    float64\n",
            " 32  salary2             292 non-null    float64\n",
            " 33  noi2                292 non-null    float64\n",
            " 34  noe2                292 non-null    float64\n",
            " 35  interest2           292 non-null    float64\n",
            " 36  ctax2               292 non-null    float64\n",
            " 37  profit2             292 non-null    float64\n",
            " 38  liquidAsset2        292 non-null    float64\n",
            " 39  quickAsset2         292 non-null    float64\n",
            " 40  receivableS2        292 non-null    float64\n",
            " 41  inventoryAsset2     292 non-null    float64\n",
            " 42  nonCAsset2          292 non-null    float64\n",
            " 43  tanAsset2           292 non-null    float64\n",
            " 44  OnonCAsset2         292 non-null    float64\n",
            " 45  receivableL2        292 non-null    float64\n",
            " 46  debt2               292 non-null    float64\n",
            " 47  liquidLiabilities2  292 non-null    float64\n",
            " 48  shortLoan2          292 non-null    float64\n",
            " 49  NCLiabilities2      292 non-null    float64\n",
            " 50  longLoan2           292 non-null    float64\n",
            " 51  netAsset2           292 non-null    float64\n",
            " 52  surplus2            292 non-null    float64\n",
            " 53  employee1           292 non-null    float64\n",
            " 54  employee2           292 non-null    float64\n",
            " 55  ownerChange         292 non-null    object \n",
            "dtypes: float64(51), int64(3), object(2)\n",
            "memory usage: 130.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 원핫 인코딩"
      ],
      "metadata": {
        "id": "TfHRL3kX-BuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_complete1 = pd.get_dummies(data_prep, columns = ['instkind','ownerChange'],drop_first=True)"
      ],
      "metadata": {
        "id": "V-pyeaaA9S3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_complete1.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLlwIjUsNx2X",
        "outputId": "29e6847f-ca48-45b9-8af3-5f73e3bb47e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 292 entries, 0 to 300\n",
            "Data columns (total 59 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   OC                         292 non-null    int64  \n",
            " 1   sido                       292 non-null    int64  \n",
            " 2   openYear                   292 non-null    int64  \n",
            " 3   bedCount                   292 non-null    float64\n",
            " 4   revenue1                   292 non-null    float64\n",
            " 5   salescost1                 292 non-null    float64\n",
            " 6   sga1                       292 non-null    float64\n",
            " 7   salary1                    292 non-null    float64\n",
            " 8   noi1                       292 non-null    float64\n",
            " 9   noe1                       292 non-null    float64\n",
            " 10  interest1                  292 non-null    float64\n",
            " 11  ctax1                      292 non-null    float64\n",
            " 12  profit1                    292 non-null    float64\n",
            " 13  liquidAsset1               292 non-null    float64\n",
            " 14  quickAsset1                292 non-null    float64\n",
            " 15  receivableS1               292 non-null    float64\n",
            " 16  inventoryAsset1            292 non-null    float64\n",
            " 17  nonCAsset1                 292 non-null    float64\n",
            " 18  tanAsset1                  292 non-null    float64\n",
            " 19  OnonCAsset1                292 non-null    float64\n",
            " 20  receivableL1               292 non-null    float64\n",
            " 21  debt1                      292 non-null    float64\n",
            " 22  liquidLiabilities1         292 non-null    float64\n",
            " 23  shortLoan1                 292 non-null    float64\n",
            " 24  NCLiabilities1             292 non-null    float64\n",
            " 25  longLoan1                  292 non-null    float64\n",
            " 26  netAsset1                  292 non-null    float64\n",
            " 27  surplus1                   292 non-null    float64\n",
            " 28  revenue2                   292 non-null    float64\n",
            " 29  salescost2                 292 non-null    float64\n",
            " 30  sga2                       292 non-null    float64\n",
            " 31  salary2                    292 non-null    float64\n",
            " 32  noi2                       292 non-null    float64\n",
            " 33  noe2                       292 non-null    float64\n",
            " 34  interest2                  292 non-null    float64\n",
            " 35  ctax2                      292 non-null    float64\n",
            " 36  profit2                    292 non-null    float64\n",
            " 37  liquidAsset2               292 non-null    float64\n",
            " 38  quickAsset2                292 non-null    float64\n",
            " 39  receivableS2               292 non-null    float64\n",
            " 40  inventoryAsset2            292 non-null    float64\n",
            " 41  nonCAsset2                 292 non-null    float64\n",
            " 42  tanAsset2                  292 non-null    float64\n",
            " 43  OnonCAsset2                292 non-null    float64\n",
            " 44  receivableL2               292 non-null    float64\n",
            " 45  debt2                      292 non-null    float64\n",
            " 46  liquidLiabilities2         292 non-null    float64\n",
            " 47  shortLoan2                 292 non-null    float64\n",
            " 48  NCLiabilities2             292 non-null    float64\n",
            " 49  longLoan2                  292 non-null    float64\n",
            " 50  netAsset2                  292 non-null    float64\n",
            " 51  surplus2                   292 non-null    float64\n",
            " 52  employee1                  292 non-null    float64\n",
            " 53  employee2                  292 non-null    float64\n",
            " 54  instkind_general_hospital  292 non-null    uint8  \n",
            " 55  instkind_hospital          292 non-null    uint8  \n",
            " 56  instkind_nursing_hospital  292 non-null    uint8  \n",
            " 57  instkind_others            292 non-null    uint8  \n",
            " 58  ownerChange_same           292 non-null    uint8  \n",
            "dtypes: float64(51), int64(3), uint8(5)\n",
            "memory usage: 126.9 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파생변수 만들기"
      ],
      "metadata": {
        "id": "-MBiNRxthcPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep4 = data_complete1.copy()"
      ],
      "metadata": {
        "id": "7bKBz7cHRtE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = data_prep4['profit1']/data_prep4['netAsset1'] #총자본경상이익율1\n",
        "b = data_prep4['profit2']/data_prep4['netAsset2'] #총자본경상이익율2\n",
        "\n",
        "c = data_prep4['quickAsset1']/data_prep4['liquidLiabilities1'] #당좌비율1\n",
        "d = data_prep4['quickAsset2']/data_prep4['liquidLiabilities2'] #당좌비율2\n",
        "\n",
        "e = data_prep4['revenue1']/data_prep4['netAsset1'] #총자본회전율1\n",
        "f = data_prep4['revenue2']/data_prep4['netAsset2'] #총자본회전율2\n",
        "\n",
        "\n",
        "data_prep4['평균_총자본경상이익율'] = (a+b)/2\n",
        "data_prep4['총자본경상이익율_증감'] = (a-b)/b\n",
        "\n",
        "data_prep4['평균_당좌비율'] = (c+d)/2\n",
        "data_prep4['당좌비율_증감'] = (c-d)/d\n",
        "\n",
        "data_prep4['평균_총자본회전율'] = (e+f)/2\n",
        "data_prep4['총자본회전율_증감'] = (e-f)/e"
      ],
      "metadata": {
        "id": "NZzjZSj-oH5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep4.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "2HFE3aAooSyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep4.loc[data_prep4['bedCount'] < 30, 'bedCount_sort'] = 'clinic'\n",
        "data_prep4.loc[(data_prep4['bedCount'] >= 30) & (data_prep4['bedCount'] < 100), 'bedCount_sort'] = 'hospital'\n",
        "data_prep4.loc[data_prep4['bedCount'] >= 100, 'bedCount_sort'] = 'general hospital'"
      ],
      "metadata": {
        "id": "Sf4ITfvaoaqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep4.drop(['bedCount'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "znvkmNQzoo25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_prep4.to_csv('병원개폐업_전처리_후4(bedcount범주형&파생변수o).csv')"
      ],
      "metadata": {
        "id": "DFCdr_7wots9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Preprocessing"
      ],
      "metadata": {
        "id": "4Q9001Cufn1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "421bcc46"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('test.csv')\n",
        "train = pd.read_csv('train.csv')\n",
        "data2 = pd.read_csv('병원개폐업_전처리_후2(bedcount실수형&파생변수o).csv',index_col=0)\n",
        "data4 = pd.read_csv('병원개폐업_전처리_후4(bedcount범주형&파생변수o).csv',index_col=0)\n",
        "data2.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "data2.dropna(axis=0,inplace=True)\n",
        "data4.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "data4.dropna(axis=0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "551f88d1"
      },
      "outputs": [],
      "source": [
        "complete = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ace4589"
      },
      "outputs": [],
      "source": [
        "cat_columns = data.columns[data.dtypes=='object']\n",
        "num_columns = data.columns.difference(cat_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ee30b30"
      },
      "outputs": [],
      "source": [
        "num_columns = ['NCLiabilities1', 'NCLiabilities2',  'OnonCAsset1', 'OnonCAsset2',\n",
        "       'bedCount', 'ctax1', 'ctax2', 'debt1', 'debt2', 'interest1',\n",
        "       'interest2', 'inventoryAsset1', 'inventoryAsset2', 'liquidAsset1',\n",
        "       'liquidAsset2', 'liquidLiabilities1', 'liquidLiabilities2', 'longLoan1',\n",
        "       'longLoan2', 'netAsset1', 'netAsset2', 'noe1', 'noe2', 'noi1', 'noi2',\n",
        "       'nonCAsset1', 'nonCAsset2', 'openDate', 'profit1', 'profit2',\n",
        "       'quickAsset1', 'quickAsset2', 'receivableL1', 'receivableL2',\n",
        "       'receivableS1', 'receivableS2', 'revenue1', 'revenue2', 'salary1',\n",
        "       'salary2', 'salescost1', 'salescost2', 'sga1', 'sga2',\n",
        "       'shortLoan1', 'shortLoan2', 'surplus1', 'surplus2', 'tanAsset1',\n",
        "       'tanAsset2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba62fd04"
      },
      "outputs": [],
      "source": [
        "complete.drop(['inst_id','sgg'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29485317"
      },
      "outputs": [],
      "source": [
        "#bedcount /// train instkind 별 평균으로 대치 O\n",
        "#employee /// train instkind 별 평균으로 대치 O\n",
        "#ownerchange /// train 비율로 대치\n",
        "complete['bedCount'] = data['bedCount'].fillna(train.groupby('instkind')['bedCount'].transform('mean'))\n",
        "complete['employee1'] = data['employee1'].fillna(train.groupby('instkind')['employee1'].transform('mean'))\n",
        "complete['employee2'] = data['employee2'].fillna(train.groupby('instkind')['employee2'].transform('mean'))\n",
        "complete['ownerChange'] = data['ownerChange'].fillna(pd.Series(np.random.choice(['same', 'change'], \n",
        "                                                      p=[0.85, 0.15],size=127)))\n",
        "for i in num_columns:\n",
        "    complete[i].fillna(-999,inplace=True)\n",
        "complete['instkind'].fillna('nursing_hospital',inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "392991b4",
        "outputId": "d92bf95e-fbf8-4309-8cf0-a3989d854e5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'nursing_hospital'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['instkind'].value_counts().index[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d522ac3",
        "outputId": "085af60b-2680-4c1b-f290-4c9b845bd768"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OC                    127\n",
              "sido                    0\n",
              "openDate                0\n",
              "bedCount                0\n",
              "instkind                0\n",
              "revenue1                0\n",
              "salescost1              0\n",
              "sga1                    0\n",
              "salary1                 0\n",
              "noi1                    0\n",
              "noe1                    0\n",
              "interest1               0\n",
              "ctax1                   0\n",
              "profit1                 0\n",
              "liquidAsset1            0\n",
              "quickAsset1             0\n",
              "receivableS1            0\n",
              "inventoryAsset1         0\n",
              "nonCAsset1              0\n",
              "tanAsset1               0\n",
              "OnonCAsset1             0\n",
              "receivableL1            0\n",
              "debt1                   0\n",
              "liquidLiabilities1      0\n",
              "shortLoan1              0\n",
              "NCLiabilities1          0\n",
              "longLoan1               0\n",
              "netAsset1               0\n",
              "surplus1                0\n",
              "revenue2                0\n",
              "salescost2              0\n",
              "sga2                    0\n",
              "salary2                 0\n",
              "noi2                    0\n",
              "noe2                    0\n",
              "interest2               0\n",
              "ctax2                   0\n",
              "profit2                 0\n",
              "liquidAsset2            0\n",
              "quickAsset2             0\n",
              "receivableS2            0\n",
              "inventoryAsset2         0\n",
              "nonCAsset2              0\n",
              "tanAsset2               0\n",
              "OnonCAsset2             0\n",
              "receivableL2            0\n",
              "debt2                   0\n",
              "liquidLiabilities2      0\n",
              "shortLoan2              0\n",
              "NCLiabilities2          0\n",
              "longLoan2               0\n",
              "netAsset2               0\n",
              "surplus2                0\n",
              "employee1               0\n",
              "employee2               0\n",
              "ownerChange             0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "complete.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa0a3b53"
      },
      "outputs": [],
      "source": [
        "complete['sido'].replace({'gyeonggi':6,'seoul':5,\n",
        "                           'busan':4,'gyeongnam':4,'incheon':4,\n",
        "                          'gyeongbuk':3,'daegu':3,'choongnam':3,\n",
        "                          'jeonnam':2,'jeonbuk':2,\n",
        "                          'choongbuk':2,'gangwon':2,'daejeon':2,'gwangju':2,\n",
        "                          'ulsan':1,'sejong':0,'jeju':0},inplace=True)\n",
        "complete['openDate'] = complete['openDate'].apply(lambda x: int(str(x)[:4]))\n",
        "complete.rename(columns={'openDate':'openYear'},inplace=True)\n",
        "complete['instkind'].replace({'traditional_hospital':'others','traditional_clinic':'others','dental_clinic':'others'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68be3d44"
      },
      "outputs": [],
      "source": [
        "a = complete['profit1']/complete['netAsset1'] #총자본경상이익율1\n",
        "b = complete['profit2']/complete['netAsset2'] #총자본경상이익율2\n",
        "\n",
        "c = complete['quickAsset1']/complete['liquidLiabilities1'] #당좌비율1\n",
        "d = complete['quickAsset2']/complete['liquidLiabilities2'] #당좌비율2\n",
        "\n",
        "e = complete['revenue1']/complete['netAsset1'] #총자본회전율1\n",
        "f = complete['revenue2']/complete['netAsset2'] #총자본회전율2\n",
        "\n",
        "\n",
        "complete['평균_총자본경상이익율'] = (a+b)/2\n",
        "complete['총자본경상이익율_증감'] = (a-b)/b\n",
        "\n",
        "complete['평균_당좌비율'] = (c+d)/2\n",
        "complete['당좌비율_증감'] = (c-d)/d\n",
        "\n",
        "complete['평균_총자본회전율'] = (e+f)/2\n",
        "complete['총자본회전율_증감'] = (e-f)/e\n",
        "\n",
        "complete.replace([np.inf, -np.inf], -999,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25ddc00c",
        "outputId": "5cf55c68-0d1b-4d9f-ec56-b749c8741501"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 54217 (\\N{HANGUL SYLLABLE PYEONG}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 44512 (\\N{HANGUL SYLLABLE GYUN}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 52509 (\\N{HANGUL SYLLABLE CONG}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 48376 (\\N{HANGUL SYLLABLE BON}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 51061 (\\N{HANGUL SYLLABLE IG}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 50984 (\\N{HANGUL SYLLABLE YUL}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 44048 (\\N{HANGUL SYLLABLE GAM}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 45817 (\\N{HANGUL SYLLABLE DANG}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 51340 (\\N{HANGUL SYLLABLE JWA}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 54924 (\\N{HANGUL SYLLABLE HOE}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:95: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 54217 (\\N{HANGUL SYLLABLE PYEONG}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 44512 (\\N{HANGUL SYLLABLE GYUN}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 52509 (\\N{HANGUL SYLLABLE CONG}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 48376 (\\N{HANGUL SYLLABLE BON}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 51061 (\\N{HANGUL SYLLABLE IG}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 50984 (\\N{HANGUL SYLLABLE YUL}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 44048 (\\N{HANGUL SYLLABLE GAM}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 45817 (\\N{HANGUL SYLLABLE DANG}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 51340 (\\N{HANGUL SYLLABLE JWA}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 54924 (\\N{HANGUL SYLLABLE HOE}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/seaborn/utils.py:660: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54217 (\\N{HANGUL SYLLABLE PYEONG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44512 (\\N{HANGUL SYLLABLE GYUN}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 52509 (\\N{HANGUL SYLLABLE CONG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48376 (\\N{HANGUL SYLLABLE BON}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51061 (\\N{HANGUL SYLLABLE IG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50984 (\\N{HANGUL SYLLABLE YUL}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44048 (\\N{HANGUL SYLLABLE GAM}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 45817 (\\N{HANGUL SYLLABLE DANG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51340 (\\N{HANGUL SYLLABLE JWA}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54924 (\\N{HANGUL SYLLABLE HOE}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/kongdowoung/opt/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1080 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "plt.figure(figsize = (20,15))\n",
        "sns.heatmap(complete.isnull(), cbar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ece8fc"
      },
      "outputs": [],
      "source": [
        "for i in ['평균_총자본경상이익율','총자본경상이익율_증감','평균_당좌비율','당좌비율_증감','평균_총자본회전율','총자본회전율_증감']:\n",
        "    complete[i].fillna(-999,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "962568f0"
      },
      "outputs": [],
      "source": [
        "complete1 = complete.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55bae8f5"
      },
      "outputs": [],
      "source": [
        "complete1.loc[complete1['bedCount'] < 30, 'bedCount_sort'] = 'clinic'\n",
        "complete1.loc[(complete1['bedCount'] >= 30) & (complete1['bedCount'] < 100), 'bedCount_sort'] = 'hospital'\n",
        "complete1.loc[complete1['bedCount'] >= 100, 'bedCount_sort'] = 'general hospital'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec01fda7",
        "outputId": "b0af0aa4-f211-4b11-b691-5d34f7e38c13"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OC</th>\n",
              "      <th>sido</th>\n",
              "      <th>openYear</th>\n",
              "      <th>bedCount</th>\n",
              "      <th>instkind</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>...</th>\n",
              "      <th>employee1</th>\n",
              "      <th>employee2</th>\n",
              "      <th>ownerChange</th>\n",
              "      <th>평균_총자본경상이익율</th>\n",
              "      <th>총자본경상이익율_증감</th>\n",
              "      <th>평균_당좌비율</th>\n",
              "      <th>당좌비율_증감</th>\n",
              "      <th>평균_총자본회전율</th>\n",
              "      <th>총자본회전율_증감</th>\n",
              "      <th>bedCount_sort</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>1998</td>\n",
              "      <td>300.000000</td>\n",
              "      <td>general_hospital</td>\n",
              "      <td>6.682486e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.565709e+10</td>\n",
              "      <td>3.223695e+10</td>\n",
              "      <td>4.314093e+09</td>\n",
              "      <td>...</td>\n",
              "      <td>693.000000</td>\n",
              "      <td>693.000000</td>\n",
              "      <td>same</td>\n",
              "      <td>0.056108</td>\n",
              "      <td>1.483396</td>\n",
              "      <td>0.458319</td>\n",
              "      <td>-0.037978</td>\n",
              "      <td>7.284190</td>\n",
              "      <td>0.627470</td>\n",
              "      <td>general hospital</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2016</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>hospital</td>\n",
              "      <td>3.495758e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.259270e+10</td>\n",
              "      <td>1.672254e+10</td>\n",
              "      <td>6.871024e+07</td>\n",
              "      <td>...</td>\n",
              "      <td>379.000000</td>\n",
              "      <td>371.000000</td>\n",
              "      <td>same</td>\n",
              "      <td>-0.597583</td>\n",
              "      <td>-1.025105</td>\n",
              "      <td>0.122501</td>\n",
              "      <td>0.086460</td>\n",
              "      <td>6.142643</td>\n",
              "      <td>-3.167651</td>\n",
              "      <td>hospital</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>1989</td>\n",
              "      <td>276.000000</td>\n",
              "      <td>general_hospital</td>\n",
              "      <td>2.326031e+10</td>\n",
              "      <td>2.542571e+09</td>\n",
              "      <td>2.308749e+10</td>\n",
              "      <td>1.168734e+10</td>\n",
              "      <td>1.601943e+09</td>\n",
              "      <td>...</td>\n",
              "      <td>97.822695</td>\n",
              "      <td>93.748201</td>\n",
              "      <td>same</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>0.427804</td>\n",
              "      <td>-0.119617</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>general hospital</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>2010</td>\n",
              "      <td>363.000000</td>\n",
              "      <td>general_hospital</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.850652e+09</td>\n",
              "      <td>...</td>\n",
              "      <td>760.000000</td>\n",
              "      <td>760.000000</td>\n",
              "      <td>same</td>\n",
              "      <td>0.066228</td>\n",
              "      <td>-0.188938</td>\n",
              "      <td>0.740908</td>\n",
              "      <td>-0.742936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>general hospital</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2004</td>\n",
              "      <td>213.000000</td>\n",
              "      <td>general_hospital</td>\n",
              "      <td>5.037025e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.855803e+10</td>\n",
              "      <td>2.296346e+10</td>\n",
              "      <td>1.534452e+09</td>\n",
              "      <td>...</td>\n",
              "      <td>437.000000</td>\n",
              "      <td>385.000000</td>\n",
              "      <td>same</td>\n",
              "      <td>0.163385</td>\n",
              "      <td>-0.176732</td>\n",
              "      <td>0.664685</td>\n",
              "      <td>-0.726925</td>\n",
              "      <td>1.954993</td>\n",
              "      <td>0.239316</td>\n",
              "      <td>general hospital</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>others</td>\n",
              "      <td>2.233031e+10</td>\n",
              "      <td>8.484657e+08</td>\n",
              "      <td>1.849255e+10</td>\n",
              "      <td>1.232241e+10</td>\n",
              "      <td>4.236289e+08</td>\n",
              "      <td>...</td>\n",
              "      <td>560.000000</td>\n",
              "      <td>116.011905</td>\n",
              "      <td>same</td>\n",
              "      <td>0.121088</td>\n",
              "      <td>-0.183157</td>\n",
              "      <td>1.367599</td>\n",
              "      <td>0.070317</td>\n",
              "      <td>1.526907</td>\n",
              "      <td>-0.125440</td>\n",
              "      <td>clinic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>2001</td>\n",
              "      <td>172.340278</td>\n",
              "      <td>others</td>\n",
              "      <td>1.833906e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.760117e+10</td>\n",
              "      <td>6.824241e+09</td>\n",
              "      <td>3.145447e+07</td>\n",
              "      <td>...</td>\n",
              "      <td>132.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>same</td>\n",
              "      <td>0.042275</td>\n",
              "      <td>-0.922951</td>\n",
              "      <td>2.645394</td>\n",
              "      <td>-0.412695</td>\n",
              "      <td>2.074332</td>\n",
              "      <td>-0.161464</td>\n",
              "      <td>general hospital</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>2001</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>hospital</td>\n",
              "      <td>1.424266e+10</td>\n",
              "      <td>1.726174e+09</td>\n",
              "      <td>1.148722e+10</td>\n",
              "      <td>6.279155e+09</td>\n",
              "      <td>2.723450e+07</td>\n",
              "      <td>...</td>\n",
              "      <td>97.822695</td>\n",
              "      <td>93.748201</td>\n",
              "      <td>same</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>general hospital</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2001</td>\n",
              "      <td>172.340278</td>\n",
              "      <td>nursing_hospital</td>\n",
              "      <td>4.892710e+10</td>\n",
              "      <td>4.157148e+10</td>\n",
              "      <td>4.721485e+09</td>\n",
              "      <td>1.514547e+09</td>\n",
              "      <td>8.509815e+07</td>\n",
              "      <td>...</td>\n",
              "      <td>363.000000</td>\n",
              "      <td>343.000000</td>\n",
              "      <td>same</td>\n",
              "      <td>-2.317936</td>\n",
              "      <td>-1.201404</td>\n",
              "      <td>0.876984</td>\n",
              "      <td>0.219146</td>\n",
              "      <td>-95.845024</td>\n",
              "      <td>6.749559</td>\n",
              "      <td>general hospital</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>1996</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>hospital</td>\n",
              "      <td>1.092444e+10</td>\n",
              "      <td>1.168731e+09</td>\n",
              "      <td>8.709839e+09</td>\n",
              "      <td>3.784543e+09</td>\n",
              "      <td>4.429113e+07</td>\n",
              "      <td>...</td>\n",
              "      <td>97.822695</td>\n",
              "      <td>93.748201</td>\n",
              "      <td>same</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>hospital</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127 rows × 63 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     OC  sido  openYear    bedCount          instkind      revenue1  \\\n",
              "0   NaN     4      1998  300.000000  general_hospital  6.682486e+10   \n",
              "1   NaN     0      2016   44.000000          hospital  3.495758e+10   \n",
              "2   NaN     2      1989  276.000000  general_hospital  2.326031e+10   \n",
              "3   NaN     4      2010  363.000000  general_hospital  0.000000e+00   \n",
              "4   NaN     2      2004  213.000000  general_hospital  5.037025e+10   \n",
              "..   ..   ...       ...         ...               ...           ...   \n",
              "122 NaN     2      2001    0.000000            others  2.233031e+10   \n",
              "123 NaN     6      2001  172.340278            others  1.833906e+10   \n",
              "124 NaN     6      2001  150.000000          hospital  1.424266e+10   \n",
              "125 NaN     0      2001  172.340278  nursing_hospital  4.892710e+10   \n",
              "126 NaN     2      1996   96.000000          hospital  1.092444e+10   \n",
              "\n",
              "       salescost1          sga1       salary1          noi1  ...   employee1  \\\n",
              "0    0.000000e+00  6.565709e+10  3.223695e+10  4.314093e+09  ...  693.000000   \n",
              "1    0.000000e+00  3.259270e+10  1.672254e+10  6.871024e+07  ...  379.000000   \n",
              "2    2.542571e+09  2.308749e+10  1.168734e+10  1.601943e+09  ...   97.822695   \n",
              "3    0.000000e+00  0.000000e+00  0.000000e+00  4.850652e+09  ...  760.000000   \n",
              "4    0.000000e+00  4.855803e+10  2.296346e+10  1.534452e+09  ...  437.000000   \n",
              "..            ...           ...           ...           ...  ...         ...   \n",
              "122  8.484657e+08  1.849255e+10  1.232241e+10  4.236289e+08  ...  560.000000   \n",
              "123  0.000000e+00  1.760117e+10  6.824241e+09  3.145447e+07  ...  132.000000   \n",
              "124  1.726174e+09  1.148722e+10  6.279155e+09  2.723450e+07  ...   97.822695   \n",
              "125  4.157148e+10  4.721485e+09  1.514547e+09  8.509815e+07  ...  363.000000   \n",
              "126  1.168731e+09  8.709839e+09  3.784543e+09  4.429113e+07  ...   97.822695   \n",
              "\n",
              "      employee2  ownerChange  평균_총자본경상이익율  총자본경상이익율_증감     평균_당좌비율  \\\n",
              "0    693.000000         same     0.056108     1.483396    0.458319   \n",
              "1    371.000000         same    -0.597583    -1.025105    0.122501   \n",
              "2     93.748201         same  -999.000000  -999.000000    0.427804   \n",
              "3    760.000000         same     0.066228    -0.188938    0.740908   \n",
              "4    385.000000         same     0.163385    -0.176732    0.664685   \n",
              "..          ...          ...          ...          ...         ...   \n",
              "122  116.011905         same     0.121088    -0.183157    1.367599   \n",
              "123  137.000000         same     0.042275    -0.922951    2.645394   \n",
              "124   93.748201         same  -999.000000  -999.000000 -999.000000   \n",
              "125  343.000000         same    -2.317936    -1.201404    0.876984   \n",
              "126   93.748201         same  -999.000000  -999.000000 -999.000000   \n",
              "\n",
              "        당좌비율_증감   평균_총자본회전율   총자본회전율_증감     bedCount_sort  \n",
              "0     -0.037978    7.284190    0.627470  general hospital  \n",
              "1      0.086460    6.142643   -3.167651          hospital  \n",
              "2     -0.119617 -999.000000 -999.000000  general hospital  \n",
              "3     -0.742936    0.000000 -999.000000  general hospital  \n",
              "4     -0.726925    1.954993    0.239316  general hospital  \n",
              "..          ...         ...         ...               ...  \n",
              "122    0.070317    1.526907   -0.125440            clinic  \n",
              "123   -0.412695    2.074332   -0.161464  general hospital  \n",
              "124 -999.000000 -999.000000 -999.000000  general hospital  \n",
              "125    0.219146  -95.845024    6.749559  general hospital  \n",
              "126 -999.000000 -999.000000 -999.000000          hospital  \n",
              "\n",
              "[127 rows x 63 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "complete1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2f2bf74"
      },
      "outputs": [],
      "source": [
        "complete1.drop(['bedCount'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bf1c9c1"
      },
      "source": [
        "### one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad626bfe"
      },
      "outputs": [],
      "source": [
        "data_complete1 = pd.get_dummies(complete1, columns = ['instkind','ownerChange','bedCount_sort'],drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6861f0c",
        "outputId": "9073def5-4052-44bb-b35b-d261e50f5ca5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>OC</th>\n",
              "      <th>sido</th>\n",
              "      <th>openYear</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>...</th>\n",
              "      <th>당좌비율_증감</th>\n",
              "      <th>평균_총자본회전율</th>\n",
              "      <th>총자본회전율_증감</th>\n",
              "      <th>instkind_general_hospital</th>\n",
              "      <th>instkind_hospital</th>\n",
              "      <th>instkind_nursing_hospital</th>\n",
              "      <th>instkind_others</th>\n",
              "      <th>ownerChange_same</th>\n",
              "      <th>bedCount_sort_general hospital</th>\n",
              "      <th>bedCount_sort_hospital</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>1998</td>\n",
              "      <td>6.682486e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>6.565709e+10</td>\n",
              "      <td>3.223695e+10</td>\n",
              "      <td>4.314093e+09</td>\n",
              "      <td>4.901517e+09</td>\n",
              "      <td>1.775872e+09</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.037978</td>\n",
              "      <td>7.284190</td>\n",
              "      <td>0.627470</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2016</td>\n",
              "      <td>3.495758e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.259270e+10</td>\n",
              "      <td>1.672254e+10</td>\n",
              "      <td>6.871024e+07</td>\n",
              "      <td>1.981033e+09</td>\n",
              "      <td>1.936455e+09</td>\n",
              "      <td>...</td>\n",
              "      <td>0.086460</td>\n",
              "      <td>6.142643</td>\n",
              "      <td>-3.167651</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>1989</td>\n",
              "      <td>2.326031e+10</td>\n",
              "      <td>2.542571e+09</td>\n",
              "      <td>2.308749e+10</td>\n",
              "      <td>1.168734e+10</td>\n",
              "      <td>1.601943e+09</td>\n",
              "      <td>1.380941e+09</td>\n",
              "      <td>8.101204e+08</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.119617</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>2010</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.850652e+09</td>\n",
              "      <td>2.060989e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.742936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-999.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2004</td>\n",
              "      <td>5.037025e+10</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.855803e+10</td>\n",
              "      <td>2.296346e+10</td>\n",
              "      <td>1.534452e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>2.573804e+09</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.726925</td>\n",
              "      <td>1.954993</td>\n",
              "      <td>0.239316</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 66 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   OC  sido  openYear      revenue1    salescost1          sga1       salary1  \\\n",
              "0 NaN     4      1998  6.682486e+10  0.000000e+00  6.565709e+10  3.223695e+10   \n",
              "1 NaN     0      2016  3.495758e+10  0.000000e+00  3.259270e+10  1.672254e+10   \n",
              "2 NaN     2      1989  2.326031e+10  2.542571e+09  2.308749e+10  1.168734e+10   \n",
              "3 NaN     4      2010  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "4 NaN     2      2004  5.037025e+10  0.000000e+00  4.855803e+10  2.296346e+10   \n",
              "\n",
              "           noi1          noe1     interest1  ...   당좌비율_증감   평균_총자본회전율  \\\n",
              "0  4.314093e+09  4.901517e+09  1.775872e+09  ... -0.037978    7.284190   \n",
              "1  6.871024e+07  1.981033e+09  1.936455e+09  ...  0.086460    6.142643   \n",
              "2  1.601943e+09  1.380941e+09  8.101204e+08  ... -0.119617 -999.000000   \n",
              "3  4.850652e+09  2.060989e+09  0.000000e+00  ... -0.742936    0.000000   \n",
              "4  1.534452e+09  0.000000e+00  2.573804e+09  ... -0.726925    1.954993   \n",
              "\n",
              "    총자본회전율_증감  instkind_general_hospital  instkind_hospital  \\\n",
              "0    0.627470                          1                  0   \n",
              "1   -3.167651                          0                  1   \n",
              "2 -999.000000                          1                  0   \n",
              "3 -999.000000                          1                  0   \n",
              "4    0.239316                          1                  0   \n",
              "\n",
              "   instkind_nursing_hospital  instkind_others  ownerChange_same  \\\n",
              "0                          0                0                 1   \n",
              "1                          0                0                 1   \n",
              "2                          0                0                 1   \n",
              "3                          0                0                 1   \n",
              "4                          0                0                 1   \n",
              "\n",
              "   bedCount_sort_general hospital  bedCount_sort_hospital  \n",
              "0                               1                       0  \n",
              "1                               0                       1  \n",
              "2                               1                       0  \n",
              "3                               1                       0  \n",
              "4                               1                       0  \n",
              "\n",
              "[5 rows x 66 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_complete1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c526c8cf"
      },
      "source": [
        "### target split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b075ed9b"
      },
      "outputs": [],
      "source": [
        "X = data_complete1\n",
        "data4_X = data4.drop(['OC'],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "703395c3"
      },
      "source": [
        "### scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d475c2b"
      },
      "outputs": [],
      "source": [
        "#standard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03931fb7"
      },
      "outputs": [],
      "source": [
        "X_train_num = data4_X[[ 'openYear', 'revenue1', 'salescost1', 'sga1',\n",
        "       'salary1', 'noi1', 'noe1', 'interest1', 'ctax1', 'profit1',\n",
        "       'liquidAsset1', 'quickAsset1', 'receivableS1', 'inventoryAsset1',\n",
        "       'nonCAsset1', 'tanAsset1', 'OnonCAsset1', 'receivableL1', 'debt1',\n",
        "       'liquidLiabilities1', 'shortLoan1', 'NCLiabilities1', 'longLoan1',\n",
        "       'netAsset1', 'surplus1', 'revenue2', 'salescost2', 'sga2', 'salary2',\n",
        "       'noi2', 'noe2', 'interest2', 'ctax2', 'profit2', 'liquidAsset2',\n",
        "       'quickAsset2', 'receivableS2', 'inventoryAsset2', 'nonCAsset2',\n",
        "       'tanAsset2', 'OnonCAsset2', 'receivableL2', 'debt2',\n",
        "       'liquidLiabilities2', 'shortLoan2', 'NCLiabilities2', 'longLoan2',\n",
        "       'netAsset2', 'surplus2', 'employee1', 'employee2',\n",
        "       '평균_총자본경상이익율', '총자본경상이익율_증감', '평균_당좌비율', '당좌비율_증감', '평균_총자본회전율',\n",
        "       '총자본회전율_증감']]\n",
        "\n",
        "X_num = X[[ 'openYear', 'revenue1', 'salescost1', 'sga1',\n",
        "       'salary1', 'noi1', 'noe1', 'interest1', 'ctax1', 'profit1',\n",
        "       'liquidAsset1', 'quickAsset1', 'receivableS1', 'inventoryAsset1',\n",
        "       'nonCAsset1', 'tanAsset1', 'OnonCAsset1', 'receivableL1', 'debt1',\n",
        "       'liquidLiabilities1', 'shortLoan1', 'NCLiabilities1', 'longLoan1',\n",
        "       'netAsset1', 'surplus1', 'revenue2', 'salescost2', 'sga2', 'salary2',\n",
        "       'noi2', 'noe2', 'interest2', 'ctax2', 'profit2', 'liquidAsset2',\n",
        "       'quickAsset2', 'receivableS2', 'inventoryAsset2', 'nonCAsset2',\n",
        "       'tanAsset2', 'OnonCAsset2', 'receivableL2', 'debt2',\n",
        "       'liquidLiabilities2', 'shortLoan2', 'NCLiabilities2', 'longLoan2',\n",
        "       'netAsset2', 'surplus2', 'employee1', 'employee2',\n",
        "       '평균_총자본경상이익율', '총자본경상이익율_증감', '평균_당좌비율', '당좌비율_증감', '평균_총자본회전율',\n",
        "       '총자본회전율_증감']]\n",
        "X_cat = X[['sido','instkind_general_hospital', 'instkind_hospital','instkind_nursing_hospital', 'instkind_others', 'ownerChange_same','OC']]\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "ss.fit(X_train_num)\n",
        "X_stand = ss.transform(X_num)\n",
        "X_stand = pd.DataFrame(data = X_stand, columns = X_num.columns)\n",
        "\n",
        "data_complete2 = pd.concat([X_stand, X_cat],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "679d8adf"
      },
      "outputs": [],
      "source": [
        "data_complete2.to_csv('test_bedCount범주_standard.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b67e7743",
        "outputId": "04fe3a3c-681c-4336-ae89-9d2ed984b822"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>openYear</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>...</th>\n",
              "      <th>당좌비율_증감</th>\n",
              "      <th>평균_총자본회전율</th>\n",
              "      <th>총자본회전율_증감</th>\n",
              "      <th>sido</th>\n",
              "      <th>instkind_general_hospital</th>\n",
              "      <th>instkind_hospital</th>\n",
              "      <th>instkind_nursing_hospital</th>\n",
              "      <th>instkind_others</th>\n",
              "      <th>ownerChange_same</th>\n",
              "      <th>OC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.726079</td>\n",
              "      <td>2.422635</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>3.447613</td>\n",
              "      <td>3.057431</td>\n",
              "      <td>4.594441</td>\n",
              "      <td>3.758111</td>\n",
              "      <td>5.393930</td>\n",
              "      <td>-0.023786</td>\n",
              "      <td>0.207897</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.210372</td>\n",
              "      <td>-0.048298</td>\n",
              "      <td>0.252443</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.298761</td>\n",
              "      <td>0.963042</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>1.359442</td>\n",
              "      <td>1.246483</td>\n",
              "      <td>-0.268227</td>\n",
              "      <td>1.223775</td>\n",
              "      <td>5.941602</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.163014</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>-0.052162</td>\n",
              "      <td>-1.335616</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.738499</td>\n",
              "      <td>0.427281</td>\n",
              "      <td>0.037002</td>\n",
              "      <td>0.759143</td>\n",
              "      <td>0.658739</td>\n",
              "      <td>1.487940</td>\n",
              "      <td>0.703028</td>\n",
              "      <td>2.100202</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-2.121192</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.348341</td>\n",
              "      <td>-3.454428</td>\n",
              "      <td>-418.039191</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.638093</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.698939</td>\n",
              "      <td>-0.705488</td>\n",
              "      <td>5.209016</td>\n",
              "      <td>1.293159</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>2.215229</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.401750</td>\n",
              "      <td>-0.072953</td>\n",
              "      <td>-418.039191</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>1.668977</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>2.367727</td>\n",
              "      <td>1.974966</td>\n",
              "      <td>1.410636</td>\n",
              "      <td>-0.495324</td>\n",
              "      <td>8.115302</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>2.704340</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.374690</td>\n",
              "      <td>-0.066336</td>\n",
              "      <td>0.090021</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>0.384685</td>\n",
              "      <td>-0.171818</td>\n",
              "      <td>0.468952</td>\n",
              "      <td>0.732869</td>\n",
              "      <td>0.138297</td>\n",
              "      <td>0.636969</td>\n",
              "      <td>0.762689</td>\n",
              "      <td>1.343518</td>\n",
              "      <td>1.251549</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027353</td>\n",
              "      <td>-0.067785</td>\n",
              "      <td>-0.062610</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>0.201877</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.412656</td>\n",
              "      <td>0.091085</td>\n",
              "      <td>-0.310900</td>\n",
              "      <td>-0.476904</td>\n",
              "      <td>-0.591269</td>\n",
              "      <td>2.449725</td>\n",
              "      <td>-0.183629</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.843643</td>\n",
              "      <td>-0.065932</td>\n",
              "      <td>-0.077684</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>0.014253</td>\n",
              "      <td>-0.063629</td>\n",
              "      <td>0.026532</td>\n",
              "      <td>0.027459</td>\n",
              "      <td>-0.315734</td>\n",
              "      <td>0.251598</td>\n",
              "      <td>2.226555</td>\n",
              "      <td>-0.258164</td>\n",
              "      <td>-0.078741</td>\n",
              "      <td>...</td>\n",
              "      <td>-1688.454070</td>\n",
              "      <td>-3.454428</td>\n",
              "      <td>-418.039191</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>1.602877</td>\n",
              "      <td>4.847823</td>\n",
              "      <td>-0.400756</td>\n",
              "      <td>-0.528699</td>\n",
              "      <td>-0.249456</td>\n",
              "      <td>0.606826</td>\n",
              "      <td>2.195036</td>\n",
              "      <td>-0.204679</td>\n",
              "      <td>1.010194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.224167</td>\n",
              "      <td>-0.397375</td>\n",
              "      <td>2.814216</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>-0.951061</td>\n",
              "      <td>-0.137729</td>\n",
              "      <td>-0.132341</td>\n",
              "      <td>-0.148872</td>\n",
              "      <td>-0.263730</td>\n",
              "      <td>-0.296197</td>\n",
              "      <td>0.132156</td>\n",
              "      <td>0.776825</td>\n",
              "      <td>-0.228422</td>\n",
              "      <td>0.065198</td>\n",
              "      <td>...</td>\n",
              "      <td>-1688.454070</td>\n",
              "      <td>-3.454428</td>\n",
              "      <td>-418.039191</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127 rows × 64 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     openYear  revenue1  salescost1      sga1   salary1      noi1      noe1  \\\n",
              "0   -0.726079  2.422635   -0.276402  3.447613  3.057431  4.594441  3.758111   \n",
              "1    1.298761  0.963042   -0.276402  1.359442  1.246483 -0.268227  1.223775   \n",
              "2   -1.738499  0.427281    0.037002  0.759143  0.658739  1.487940  0.703028   \n",
              "3    0.623815 -0.638093   -0.276402 -0.698939 -0.705488  5.209016  1.293159   \n",
              "4   -0.051132  1.668977   -0.276402  2.367727  1.974966  1.410636 -0.495324   \n",
              "..        ...       ...         ...       ...       ...       ...       ...   \n",
              "122 -0.388606  0.384685   -0.171818  0.468952  0.732869  0.138297  0.636969   \n",
              "123 -0.388606  0.201877   -0.276402  0.412656  0.091085 -0.310900 -0.476904   \n",
              "124 -0.388606  0.014253   -0.063629  0.026532  0.027459 -0.315734  0.251598   \n",
              "125 -0.388606  1.602877    4.847823 -0.400756 -0.528699 -0.249456  0.606826   \n",
              "126 -0.951061 -0.137729   -0.132341 -0.148872 -0.263730 -0.296197  0.132156   \n",
              "\n",
              "     interest1     ctax1   profit1  ...      당좌비율_증감  평균_총자본회전율   총자본회전율_증감  \\\n",
              "0     5.393930 -0.023786  0.207897  ...    -0.210372  -0.048298    0.252443   \n",
              "1     5.941602 -0.332867  0.163014  ...    -0.000072  -0.052162   -1.335616   \n",
              "2     2.100202 -0.332867 -2.121192  ...    -0.348341  -3.454428 -418.039191   \n",
              "3    -0.662740 -0.332867  2.215229  ...    -1.401750  -0.072953 -418.039191   \n",
              "4     8.115302 -0.332867  2.704340  ...    -1.374690  -0.066336    0.090021   \n",
              "..         ...       ...       ...  ...          ...        ...         ...   \n",
              "122   0.762689  1.343518  1.251549  ...    -0.027353  -0.067785   -0.062610   \n",
              "123  -0.591269  2.449725 -0.183629  ...    -0.843643  -0.065932   -0.077684   \n",
              "124   2.226555 -0.258164 -0.078741  ... -1688.454070  -3.454428 -418.039191   \n",
              "125   2.195036 -0.204679  1.010194  ...     0.224167  -0.397375    2.814216   \n",
              "126   0.776825 -0.228422  0.065198  ... -1688.454070  -3.454428 -418.039191   \n",
              "\n",
              "     sido  instkind_general_hospital  instkind_hospital  \\\n",
              "0       4                          1                  0   \n",
              "1       0                          0                  1   \n",
              "2       2                          1                  0   \n",
              "3       4                          1                  0   \n",
              "4       2                          1                  0   \n",
              "..    ...                        ...                ...   \n",
              "122     2                          0                  0   \n",
              "123     6                          0                  0   \n",
              "124     6                          0                  1   \n",
              "125     0                          0                  0   \n",
              "126     2                          0                  1   \n",
              "\n",
              "     instkind_nursing_hospital  instkind_others  ownerChange_same  OC  \n",
              "0                            0                0                 1 NaN  \n",
              "1                            0                0                 1 NaN  \n",
              "2                            0                0                 1 NaN  \n",
              "3                            0                0                 1 NaN  \n",
              "4                            0                0                 1 NaN  \n",
              "..                         ...              ...               ...  ..  \n",
              "122                          0                1                 1 NaN  \n",
              "123                          0                1                 1 NaN  \n",
              "124                          0                0                 1 NaN  \n",
              "125                          1                0                 1 NaN  \n",
              "126                          0                0                 1 NaN  \n",
              "\n",
              "[127 rows x 64 columns]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_complete2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "BguvU9E0gaRI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iuGrPxhinL_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier ,AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance,XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data4 = pd.read_csv('/content/drive/MyDrive/병원개폐업_전처리_후4(bedcount범주형&파생변수o).csv',index_col=0)\n",
        "data4 = pd.get_dummies(data4,drop_first=True)\n",
        "data4.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "data4.dropna(axis=0,inplace=True)"
      ],
      "metadata": {
        "id": "HdMcj1qijWc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data4.drop([\"OC\"], axis = 1)\n",
        "y = data4[[\"OC\"]]"
      ],
      "metadata": {
        "id": "nKaY9333j_hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
        "                                                   test_size = 0.2, random_state = 42,\n",
        "                                                   stratify = data4['OC'])"
      ],
      "metadata": {
        "id": "mHlPZ-sNkEr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.reset_index(drop = True, inplace = True)\n",
        "X_val.reset_index(drop = True, inplace = True)\n",
        "y_train.reset_index(drop = True, inplace = True)\n",
        "y_val.reset_index(drop = True, inplace = True)"
      ],
      "metadata": {
        "id": "GEUkVHrmkGBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_cat = X_train[['sido','bedCount_sort_general hospital',\t'bedCount_sort_hospital',\t'ownerChange_same']]\n",
        "X_val_cat = X_val[['sido','bedCount_sort_general hospital',\t'bedCount_sort_hospital',\t'ownerChange_same']]\n",
        "X_train_num = X_train.drop(['sido','bedCount_sort_general hospital',\t'bedCount_sort_hospital',\t'ownerChange_same','instkind_general_hospital',\t'instkind_hospital',\t'instkind_nursing_hospital',\t'instkind_others'], axis=1)\n",
        "X_val_num = X_val.drop(['sido','bedCount_sort_general hospital',\t'bedCount_sort_hospital',\t'ownerChange_same','instkind_general_hospital',\t'instkind_hospital',\t'instkind_nursing_hospital',\t'instkind_others'], axis=1)"
      ],
      "metadata": {
        "id": "gaei-pDnkNkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train set scaling"
      ],
      "metadata": {
        "id": "E5igObQlgmZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_num)\n",
        "\n",
        "X_train_stand = scaler.transform(X_train_num)\n",
        "X_val_stand = scaler.transform(X_val_num)"
      ],
      "metadata": {
        "id": "J15hQHk1kSiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_stand = pd.DataFrame(data = X_train_stand, columns = X_train_num.columns)\n",
        "X_val_stand = pd.DataFrame(data = X_val_stand, columns = X_val_num.columns)\n",
        "X_train_stand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "m6OY1l9HkXKk",
        "outputId": "534c4d70-5168-4f5a-90ec-9f428cbd3cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-76c6b50e-7514-488e-95b8-f291edafd282\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>openYear</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>liquidAsset1</th>\n",
              "      <th>quickAsset1</th>\n",
              "      <th>receivableS1</th>\n",
              "      <th>inventoryAsset1</th>\n",
              "      <th>nonCAsset1</th>\n",
              "      <th>tanAsset1</th>\n",
              "      <th>OnonCAsset1</th>\n",
              "      <th>receivableL1</th>\n",
              "      <th>debt1</th>\n",
              "      <th>liquidLiabilities1</th>\n",
              "      <th>shortLoan1</th>\n",
              "      <th>NCLiabilities1</th>\n",
              "      <th>longLoan1</th>\n",
              "      <th>netAsset1</th>\n",
              "      <th>surplus1</th>\n",
              "      <th>revenue2</th>\n",
              "      <th>salescost2</th>\n",
              "      <th>sga2</th>\n",
              "      <th>salary2</th>\n",
              "      <th>noi2</th>\n",
              "      <th>noe2</th>\n",
              "      <th>interest2</th>\n",
              "      <th>ctax2</th>\n",
              "      <th>profit2</th>\n",
              "      <th>liquidAsset2</th>\n",
              "      <th>quickAsset2</th>\n",
              "      <th>receivableS2</th>\n",
              "      <th>inventoryAsset2</th>\n",
              "      <th>nonCAsset2</th>\n",
              "      <th>tanAsset2</th>\n",
              "      <th>OnonCAsset2</th>\n",
              "      <th>receivableL2</th>\n",
              "      <th>debt2</th>\n",
              "      <th>liquidLiabilities2</th>\n",
              "      <th>shortLoan2</th>\n",
              "      <th>NCLiabilities2</th>\n",
              "      <th>longLoan2</th>\n",
              "      <th>netAsset2</th>\n",
              "      <th>surplus2</th>\n",
              "      <th>employee1</th>\n",
              "      <th>employee2</th>\n",
              "      <th>평균_총자본경상이익율</th>\n",
              "      <th>총자본경상이익율_증감</th>\n",
              "      <th>평균_당좌비율</th>\n",
              "      <th>당좌비율_증감</th>\n",
              "      <th>평균_총자본회전율</th>\n",
              "      <th>총자본회전율_증감</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.675594</td>\n",
              "      <td>-0.461025</td>\n",
              "      <td>-0.272222</td>\n",
              "      <td>-0.535046</td>\n",
              "      <td>-0.550807</td>\n",
              "      <td>-0.369162</td>\n",
              "      <td>-0.477837</td>\n",
              "      <td>-0.517173</td>\n",
              "      <td>0.243690</td>\n",
              "      <td>0.365205</td>\n",
              "      <td>-0.252671</td>\n",
              "      <td>-0.241505</td>\n",
              "      <td>-0.567606</td>\n",
              "      <td>-0.438611</td>\n",
              "      <td>-0.510866</td>\n",
              "      <td>-0.517569</td>\n",
              "      <td>-0.339978</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.567790</td>\n",
              "      <td>-0.718035</td>\n",
              "      <td>-0.619724</td>\n",
              "      <td>-0.326478</td>\n",
              "      <td>-0.114915</td>\n",
              "      <td>-0.196733</td>\n",
              "      <td>-0.042360</td>\n",
              "      <td>-0.491391</td>\n",
              "      <td>-0.283392</td>\n",
              "      <td>-0.565804</td>\n",
              "      <td>-0.591588</td>\n",
              "      <td>-0.436937</td>\n",
              "      <td>-0.434797</td>\n",
              "      <td>-0.466237</td>\n",
              "      <td>-0.246469</td>\n",
              "      <td>0.212331</td>\n",
              "      <td>-0.390167</td>\n",
              "      <td>-0.382741</td>\n",
              "      <td>-0.658541</td>\n",
              "      <td>-0.426843</td>\n",
              "      <td>-0.518193</td>\n",
              "      <td>-0.528829</td>\n",
              "      <td>-0.359642</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.587685</td>\n",
              "      <td>-0.699262</td>\n",
              "      <td>-0.584649</td>\n",
              "      <td>-0.386032</td>\n",
              "      <td>-0.233322</td>\n",
              "      <td>-0.288237</td>\n",
              "      <td>-0.134916</td>\n",
              "      <td>-0.572077</td>\n",
              "      <td>-0.589480</td>\n",
              "      <td>0.291220</td>\n",
              "      <td>0.095325</td>\n",
              "      <td>8.305396</td>\n",
              "      <td>3.046400</td>\n",
              "      <td>-0.077023</td>\n",
              "      <td>-0.017182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.417362</td>\n",
              "      <td>-0.515649</td>\n",
              "      <td>-0.244149</td>\n",
              "      <td>-0.572385</td>\n",
              "      <td>-0.530242</td>\n",
              "      <td>-0.319967</td>\n",
              "      <td>-0.478526</td>\n",
              "      <td>-0.549155</td>\n",
              "      <td>-0.303097</td>\n",
              "      <td>-0.227272</td>\n",
              "      <td>-0.467394</td>\n",
              "      <td>-0.462017</td>\n",
              "      <td>-0.567606</td>\n",
              "      <td>-0.454733</td>\n",
              "      <td>-0.521669</td>\n",
              "      <td>-0.531682</td>\n",
              "      <td>-0.274250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.518285</td>\n",
              "      <td>-0.339582</td>\n",
              "      <td>-0.083702</td>\n",
              "      <td>-0.595498</td>\n",
              "      <td>-0.557184</td>\n",
              "      <td>-0.373666</td>\n",
              "      <td>-0.150063</td>\n",
              "      <td>-0.521145</td>\n",
              "      <td>-0.249380</td>\n",
              "      <td>-0.585350</td>\n",
              "      <td>-0.533277</td>\n",
              "      <td>-0.407628</td>\n",
              "      <td>-0.435194</td>\n",
              "      <td>-0.524970</td>\n",
              "      <td>-0.266020</td>\n",
              "      <td>-0.252824</td>\n",
              "      <td>-0.478062</td>\n",
              "      <td>-0.472889</td>\n",
              "      <td>-0.095468</td>\n",
              "      <td>-0.439729</td>\n",
              "      <td>-0.564778</td>\n",
              "      <td>-0.584009</td>\n",
              "      <td>-0.293855</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.561640</td>\n",
              "      <td>-0.290476</td>\n",
              "      <td>-0.020029</td>\n",
              "      <td>-0.672466</td>\n",
              "      <td>-0.695803</td>\n",
              "      <td>-0.461867</td>\n",
              "      <td>-0.207046</td>\n",
              "      <td>-0.632721</td>\n",
              "      <td>-0.546854</td>\n",
              "      <td>-0.049278</td>\n",
              "      <td>0.076798</td>\n",
              "      <td>-0.467525</td>\n",
              "      <td>0.443088</td>\n",
              "      <td>-0.075197</td>\n",
              "      <td>-0.007968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-2.315030</td>\n",
              "      <td>1.181172</td>\n",
              "      <td>0.511588</td>\n",
              "      <td>1.613478</td>\n",
              "      <td>2.163942</td>\n",
              "      <td>3.810013</td>\n",
              "      <td>1.631550</td>\n",
              "      <td>-0.784190</td>\n",
              "      <td>-0.303604</td>\n",
              "      <td>-1.371527</td>\n",
              "      <td>1.649463</td>\n",
              "      <td>1.687248</td>\n",
              "      <td>-0.295004</td>\n",
              "      <td>0.326670</td>\n",
              "      <td>0.173984</td>\n",
              "      <td>0.176543</td>\n",
              "      <td>0.870613</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.305732</td>\n",
              "      <td>-0.211038</td>\n",
              "      <td>-0.619724</td>\n",
              "      <td>2.504943</td>\n",
              "      <td>-0.689418</td>\n",
              "      <td>-0.210355</td>\n",
              "      <td>-0.150063</td>\n",
              "      <td>1.464819</td>\n",
              "      <td>0.643966</td>\n",
              "      <td>2.000756</td>\n",
              "      <td>2.565267</td>\n",
              "      <td>4.913737</td>\n",
              "      <td>2.609063</td>\n",
              "      <td>-0.765415</td>\n",
              "      <td>-0.267796</td>\n",
              "      <td>-2.215381</td>\n",
              "      <td>1.913675</td>\n",
              "      <td>1.957401</td>\n",
              "      <td>-0.596655</td>\n",
              "      <td>0.473188</td>\n",
              "      <td>0.283844</td>\n",
              "      <td>0.296238</td>\n",
              "      <td>0.909340</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.417113</td>\n",
              "      <td>-0.044570</td>\n",
              "      <td>-0.584649</td>\n",
              "      <td>2.321325</td>\n",
              "      <td>-0.695803</td>\n",
              "      <td>-0.109385</td>\n",
              "      <td>-0.207046</td>\n",
              "      <td>2.050763</td>\n",
              "      <td>2.368766</td>\n",
              "      <td>-0.863265</td>\n",
              "      <td>0.090404</td>\n",
              "      <td>0.549641</td>\n",
              "      <td>0.048485</td>\n",
              "      <td>-0.051358</td>\n",
              "      <td>0.062139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.417362</td>\n",
              "      <td>-0.403652</td>\n",
              "      <td>-0.240068</td>\n",
              "      <td>-0.421568</td>\n",
              "      <td>-0.388957</td>\n",
              "      <td>-0.330921</td>\n",
              "      <td>-0.364874</td>\n",
              "      <td>-0.141789</td>\n",
              "      <td>-0.305899</td>\n",
              "      <td>-0.172448</td>\n",
              "      <td>-0.408555</td>\n",
              "      <td>-0.404513</td>\n",
              "      <td>-0.567606</td>\n",
              "      <td>-0.376578</td>\n",
              "      <td>-0.306089</td>\n",
              "      <td>-0.280260</td>\n",
              "      <td>-0.355935</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.333347</td>\n",
              "      <td>-0.418326</td>\n",
              "      <td>-0.163937</td>\n",
              "      <td>-0.194701</td>\n",
              "      <td>0.101727</td>\n",
              "      <td>-0.261569</td>\n",
              "      <td>-0.150063</td>\n",
              "      <td>-0.429486</td>\n",
              "      <td>-0.255214</td>\n",
              "      <td>-0.407212</td>\n",
              "      <td>-0.395312</td>\n",
              "      <td>-0.414163</td>\n",
              "      <td>-0.331255</td>\n",
              "      <td>-0.076458</td>\n",
              "      <td>-0.267796</td>\n",
              "      <td>-0.710510</td>\n",
              "      <td>-0.422132</td>\n",
              "      <td>-0.418531</td>\n",
              "      <td>-0.658541</td>\n",
              "      <td>-0.357240</td>\n",
              "      <td>-0.289145</td>\n",
              "      <td>-0.263449</td>\n",
              "      <td>-0.376368</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.328540</td>\n",
              "      <td>-0.382325</td>\n",
              "      <td>-0.097725</td>\n",
              "      <td>-0.222723</td>\n",
              "      <td>0.030362</td>\n",
              "      <td>-0.309949</td>\n",
              "      <td>-0.207046</td>\n",
              "      <td>-0.435629</td>\n",
              "      <td>-0.427501</td>\n",
              "      <td>-0.204915</td>\n",
              "      <td>0.076843</td>\n",
              "      <td>-0.401703</td>\n",
              "      <td>0.252081</td>\n",
              "      <td>-0.075330</td>\n",
              "      <td>0.032515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.019820</td>\n",
              "      <td>-0.196775</td>\n",
              "      <td>-0.272222</td>\n",
              "      <td>-0.108197</td>\n",
              "      <td>-0.178197</td>\n",
              "      <td>-0.314950</td>\n",
              "      <td>-0.227278</td>\n",
              "      <td>0.296607</td>\n",
              "      <td>-0.233074</td>\n",
              "      <td>-0.146131</td>\n",
              "      <td>-0.396687</td>\n",
              "      <td>-0.403641</td>\n",
              "      <td>-0.567606</td>\n",
              "      <td>-0.145793</td>\n",
              "      <td>-0.135551</td>\n",
              "      <td>-0.094777</td>\n",
              "      <td>-0.323701</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.103910</td>\n",
              "      <td>0.511900</td>\n",
              "      <td>1.550290</td>\n",
              "      <td>-0.298584</td>\n",
              "      <td>-0.218838</td>\n",
              "      <td>-0.481521</td>\n",
              "      <td>-0.225977</td>\n",
              "      <td>-0.193618</td>\n",
              "      <td>-0.283392</td>\n",
              "      <td>-0.092221</td>\n",
              "      <td>-0.180089</td>\n",
              "      <td>-0.293067</td>\n",
              "      <td>-0.220764</td>\n",
              "      <td>0.370079</td>\n",
              "      <td>-0.188756</td>\n",
              "      <td>-0.149467</td>\n",
              "      <td>-0.401317</td>\n",
              "      <td>-0.395826</td>\n",
              "      <td>-0.658541</td>\n",
              "      <td>-0.455410</td>\n",
              "      <td>-0.025712</td>\n",
              "      <td>0.017894</td>\n",
              "      <td>-0.342580</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.103918</td>\n",
              "      <td>-0.433914</td>\n",
              "      <td>-0.258533</td>\n",
              "      <td>0.516201</td>\n",
              "      <td>1.223443</td>\n",
              "      <td>-0.478445</td>\n",
              "      <td>-0.123131</td>\n",
              "      <td>0.201130</td>\n",
              "      <td>0.263040</td>\n",
              "      <td>0.375988</td>\n",
              "      <td>0.154133</td>\n",
              "      <td>-0.435302</td>\n",
              "      <td>-1.454933</td>\n",
              "      <td>-0.017093</td>\n",
              "      <td>0.305227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>0.198771</td>\n",
              "      <td>-0.396880</td>\n",
              "      <td>-0.227681</td>\n",
              "      <td>-0.427636</td>\n",
              "      <td>-0.418464</td>\n",
              "      <td>-0.282668</td>\n",
              "      <td>-0.284126</td>\n",
              "      <td>-0.383941</td>\n",
              "      <td>-0.215328</td>\n",
              "      <td>-0.123284</td>\n",
              "      <td>-0.417591</td>\n",
              "      <td>-0.410991</td>\n",
              "      <td>-0.567606</td>\n",
              "      <td>-0.447967</td>\n",
              "      <td>-0.468958</td>\n",
              "      <td>-0.477307</td>\n",
              "      <td>-0.354752</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.489182</td>\n",
              "      <td>-0.575817</td>\n",
              "      <td>-0.413574</td>\n",
              "      <td>-0.321594</td>\n",
              "      <td>-0.271943</td>\n",
              "      <td>-0.311338</td>\n",
              "      <td>-0.024835</td>\n",
              "      <td>-0.392861</td>\n",
              "      <td>-0.235936</td>\n",
              "      <td>-0.431162</td>\n",
              "      <td>-0.414471</td>\n",
              "      <td>-0.370841</td>\n",
              "      <td>-0.242335</td>\n",
              "      <td>-0.446731</td>\n",
              "      <td>-0.227227</td>\n",
              "      <td>-0.088646</td>\n",
              "      <td>-0.405101</td>\n",
              "      <td>-0.398034</td>\n",
              "      <td>-0.658541</td>\n",
              "      <td>-0.429651</td>\n",
              "      <td>-0.493538</td>\n",
              "      <td>-0.503627</td>\n",
              "      <td>-0.375129</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.496084</td>\n",
              "      <td>-0.611971</td>\n",
              "      <td>-0.508827</td>\n",
              "      <td>-0.308431</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.387041</td>\n",
              "      <td>0.002690</td>\n",
              "      <td>-0.435629</td>\n",
              "      <td>-0.427501</td>\n",
              "      <td>0.082279</td>\n",
              "      <td>0.088605</td>\n",
              "      <td>-0.154916</td>\n",
              "      <td>-0.745881</td>\n",
              "      <td>-0.073480</td>\n",
              "      <td>-0.022653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>-0.019820</td>\n",
              "      <td>0.417552</td>\n",
              "      <td>-0.165847</td>\n",
              "      <td>0.621077</td>\n",
              "      <td>0.938798</td>\n",
              "      <td>0.633278</td>\n",
              "      <td>0.831914</td>\n",
              "      <td>-0.101741</td>\n",
              "      <td>1.143063</td>\n",
              "      <td>0.628114</td>\n",
              "      <td>2.103625</td>\n",
              "      <td>2.172497</td>\n",
              "      <td>-0.567606</td>\n",
              "      <td>-0.114875</td>\n",
              "      <td>0.548035</td>\n",
              "      <td>0.681500</td>\n",
              "      <td>-0.257681</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.226283</td>\n",
              "      <td>0.328184</td>\n",
              "      <td>1.009776</td>\n",
              "      <td>0.090582</td>\n",
              "      <td>-0.689418</td>\n",
              "      <td>1.568057</td>\n",
              "      <td>-0.150063</td>\n",
              "      <td>0.509454</td>\n",
              "      <td>-0.172235</td>\n",
              "      <td>0.695549</td>\n",
              "      <td>1.005450</td>\n",
              "      <td>0.132957</td>\n",
              "      <td>1.207580</td>\n",
              "      <td>0.269902</td>\n",
              "      <td>1.217960</td>\n",
              "      <td>0.569302</td>\n",
              "      <td>2.027254</td>\n",
              "      <td>2.101374</td>\n",
              "      <td>-0.318983</td>\n",
              "      <td>-0.189505</td>\n",
              "      <td>0.592391</td>\n",
              "      <td>0.731821</td>\n",
              "      <td>-0.154528</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.181498</td>\n",
              "      <td>0.459913</td>\n",
              "      <td>1.109035</td>\n",
              "      <td>-0.076822</td>\n",
              "      <td>-0.695803</td>\n",
              "      <td>2.166180</td>\n",
              "      <td>-0.207046</td>\n",
              "      <td>1.467067</td>\n",
              "      <td>1.712325</td>\n",
              "      <td>0.028918</td>\n",
              "      <td>0.094409</td>\n",
              "      <td>0.110581</td>\n",
              "      <td>0.019867</td>\n",
              "      <td>-0.077344</td>\n",
              "      <td>-0.023117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>-2.861508</td>\n",
              "      <td>-0.545452</td>\n",
              "      <td>-0.258957</td>\n",
              "      <td>-0.610400</td>\n",
              "      <td>-0.600306</td>\n",
              "      <td>-0.369092</td>\n",
              "      <td>-0.455470</td>\n",
              "      <td>-0.606421</td>\n",
              "      <td>-0.305899</td>\n",
              "      <td>-0.232942</td>\n",
              "      <td>-0.561861</td>\n",
              "      <td>-0.561732</td>\n",
              "      <td>-0.567606</td>\n",
              "      <td>-0.393630</td>\n",
              "      <td>-0.629877</td>\n",
              "      <td>-0.648656</td>\n",
              "      <td>-0.355747</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.641159</td>\n",
              "      <td>-0.484592</td>\n",
              "      <td>-0.268166</td>\n",
              "      <td>-0.675933</td>\n",
              "      <td>-0.689418</td>\n",
              "      <td>-0.434523</td>\n",
              "      <td>-0.150063</td>\n",
              "      <td>-0.572497</td>\n",
              "      <td>-0.272650</td>\n",
              "      <td>-0.642874</td>\n",
              "      <td>-0.629912</td>\n",
              "      <td>-0.442665</td>\n",
              "      <td>-0.414400</td>\n",
              "      <td>-0.589527</td>\n",
              "      <td>-0.255876</td>\n",
              "      <td>-0.353545</td>\n",
              "      <td>-0.543623</td>\n",
              "      <td>-0.544705</td>\n",
              "      <td>-0.658541</td>\n",
              "      <td>-0.336274</td>\n",
              "      <td>-0.653724</td>\n",
              "      <td>-0.680761</td>\n",
              "      <td>-0.360614</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.635630</td>\n",
              "      <td>-0.439001</td>\n",
              "      <td>-0.219455</td>\n",
              "      <td>-0.672466</td>\n",
              "      <td>-0.695803</td>\n",
              "      <td>-0.547430</td>\n",
              "      <td>-0.207046</td>\n",
              "      <td>-0.716106</td>\n",
              "      <td>-0.768510</td>\n",
              "      <td>-0.195804</td>\n",
              "      <td>0.079375</td>\n",
              "      <td>-0.516069</td>\n",
              "      <td>-0.043299</td>\n",
              "      <td>-0.073769</td>\n",
              "      <td>0.062122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>1.073136</td>\n",
              "      <td>-0.515787</td>\n",
              "      <td>-0.272222</td>\n",
              "      <td>-0.567471</td>\n",
              "      <td>-0.570961</td>\n",
              "      <td>-0.356395</td>\n",
              "      <td>-0.425178</td>\n",
              "      <td>-0.389969</td>\n",
              "      <td>-0.305899</td>\n",
              "      <td>-0.139822</td>\n",
              "      <td>-0.469045</td>\n",
              "      <td>-0.465126</td>\n",
              "      <td>-0.159605</td>\n",
              "      <td>-0.419197</td>\n",
              "      <td>-0.410226</td>\n",
              "      <td>-0.400086</td>\n",
              "      <td>-0.325305</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.596105</td>\n",
              "      <td>-0.663322</td>\n",
              "      <td>-0.619724</td>\n",
              "      <td>-0.428021</td>\n",
              "      <td>-0.312683</td>\n",
              "      <td>-0.159829</td>\n",
              "      <td>-0.150063</td>\n",
              "      <td>-0.508899</td>\n",
              "      <td>-0.283392</td>\n",
              "      <td>-0.563800</td>\n",
              "      <td>-0.571404</td>\n",
              "      <td>-0.444131</td>\n",
              "      <td>-0.425256</td>\n",
              "      <td>-0.509694</td>\n",
              "      <td>-0.267796</td>\n",
              "      <td>-0.089478</td>\n",
              "      <td>-0.424801</td>\n",
              "      <td>-0.419010</td>\n",
              "      <td>0.044264</td>\n",
              "      <td>-0.413463</td>\n",
              "      <td>-0.413809</td>\n",
              "      <td>-0.405166</td>\n",
              "      <td>-0.351729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.572935</td>\n",
              "      <td>-0.618648</td>\n",
              "      <td>-0.584649</td>\n",
              "      <td>-0.427013</td>\n",
              "      <td>-0.350010</td>\n",
              "      <td>-0.170023</td>\n",
              "      <td>-0.207046</td>\n",
              "      <td>-0.655462</td>\n",
              "      <td>-0.589480</td>\n",
              "      <td>0.020457</td>\n",
              "      <td>0.087321</td>\n",
              "      <td>-0.070618</td>\n",
              "      <td>0.091532</td>\n",
              "      <td>-0.078078</td>\n",
              "      <td>-0.052426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>-0.675594</td>\n",
              "      <td>0.075185</td>\n",
              "      <td>-0.272222</td>\n",
              "      <td>0.261190</td>\n",
              "      <td>0.300381</td>\n",
              "      <td>-0.288488</td>\n",
              "      <td>0.326024</td>\n",
              "      <td>2.116409</td>\n",
              "      <td>-0.208659</td>\n",
              "      <td>-0.167452</td>\n",
              "      <td>-0.045523</td>\n",
              "      <td>-0.059799</td>\n",
              "      <td>-0.567486</td>\n",
              "      <td>0.360151</td>\n",
              "      <td>1.222166</td>\n",
              "      <td>1.431323</td>\n",
              "      <td>-0.193284</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.122397</td>\n",
              "      <td>0.579809</td>\n",
              "      <td>0.634813</td>\n",
              "      <td>1.436269</td>\n",
              "      <td>2.733441</td>\n",
              "      <td>0.359989</td>\n",
              "      <td>-0.194717</td>\n",
              "      <td>0.092564</td>\n",
              "      <td>-0.283392</td>\n",
              "      <td>0.308259</td>\n",
              "      <td>0.270605</td>\n",
              "      <td>-0.272457</td>\n",
              "      <td>0.221726</td>\n",
              "      <td>2.372675</td>\n",
              "      <td>-0.256986</td>\n",
              "      <td>-0.173888</td>\n",
              "      <td>-0.000851</td>\n",
              "      <td>-0.016154</td>\n",
              "      <td>-0.653486</td>\n",
              "      <td>0.438691</td>\n",
              "      <td>1.444951</td>\n",
              "      <td>1.687103</td>\n",
              "      <td>-0.217416</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.229112</td>\n",
              "      <td>0.836885</td>\n",
              "      <td>0.719303</td>\n",
              "      <td>1.309909</td>\n",
              "      <td>2.466179</td>\n",
              "      <td>0.570756</td>\n",
              "      <td>-0.302939</td>\n",
              "      <td>-0.079347</td>\n",
              "      <td>0.126637</td>\n",
              "      <td>-0.030896</td>\n",
              "      <td>0.091847</td>\n",
              "      <td>-0.438199</td>\n",
              "      <td>-0.012669</td>\n",
              "      <td>-0.075890</td>\n",
              "      <td>0.009699</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>193 rows × 57 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76c6b50e-7514-488e-95b8-f291edafd282')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-76c6b50e-7514-488e-95b8-f291edafd282 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-76c6b50e-7514-488e-95b8-f291edafd282');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     openYear  revenue1  salescost1  ...   당좌비율_증감  평균_총자본회전율  총자본회전율_증감\n",
              "0   -0.675594 -0.461025   -0.272222  ...  3.046400  -0.077023  -0.017182\n",
              "1    0.417362 -0.515649   -0.244149  ...  0.443088  -0.075197  -0.007968\n",
              "2   -2.315030  1.181172    0.511588  ...  0.048485  -0.051358   0.062139\n",
              "3    0.417362 -0.403652   -0.240068  ...  0.252081  -0.075330   0.032515\n",
              "4   -0.019820 -0.196775   -0.272222  ... -1.454933  -0.017093   0.305227\n",
              "..        ...       ...         ...  ...       ...        ...        ...\n",
              "188  0.198771 -0.396880   -0.227681  ... -0.745881  -0.073480  -0.022653\n",
              "189 -0.019820  0.417552   -0.165847  ...  0.019867  -0.077344  -0.023117\n",
              "190 -2.861508 -0.545452   -0.258957  ... -0.043299  -0.073769   0.062122\n",
              "191  1.073136 -0.515787   -0.272222  ...  0.091532  -0.078078  -0.052426\n",
              "192 -0.675594  0.075185   -0.272222  ... -0.012669  -0.075890   0.009699\n",
              "\n",
              "[193 rows x 57 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_stand1 = pd.concat([X_train_stand, X_train_cat],axis=1)\n",
        "X_val_stand1 = pd.concat([X_val_stand, X_val_cat],axis=1)"
      ],
      "metadata": {
        "id": "td85S-PikYjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Smote"
      ],
      "metadata": {
        "id": "h793RNbKjT9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "hnG2SwlqkciR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standard\n",
        "smote = SMOTE(random_state = 42)\n",
        "X_train_over_stand, y_train_over_stand = smote.fit_resample(X_train_stand1, y_train)\n",
        "\n",
        "print('After OverSampling, shape of X_train: {}'.format(X_train_over_stand.shape))\n",
        "print('After OverSampling, shape of y_train: {} \\n'.format(y_train_over_stand.shape))\n",
        "print('')\n",
        "print(\"After OverSampling, '1': {}\".format(sum(y_train_over_stand[\"OC\"]==1)))\n",
        "print(\"After OverSampling, '0': {}\".format(sum(y_train_over_stand[\"OC\"]==0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt2FBifekaw8",
        "outputId": "c6647baf-c49f-4aa1-c538-9173ab0e6737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After OverSampling, shape of X_train: (374, 61)\n",
            "After OverSampling, shape of y_train: (374, 1) \n",
            "\n",
            "\n",
            "After OverSampling, '1': 187\n",
            "After OverSampling, '0': 187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=10,random_state=42)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3,random_state=1)\n",
        "rfe = RFECV(model,cv=cv)\n",
        "rfe = rfe.fit(X_train_over_stand, y_train_over_stand['OC'])\n",
        "\n",
        "print('Selected features; %s' % list(X_train_over_stand.columns[rfe.support_]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMbs8mVwk2-0",
        "outputId": "21251a08-4aad-49d6-842f-0be4e6502c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features; ['openYear', 'revenue1', 'salescost1', 'sga1', 'salary1', 'noi1', 'noe1', 'interest1', 'ctax1', 'profit1', 'liquidAsset1', 'quickAsset1', 'inventoryAsset1', 'liquidLiabilities1', 'shortLoan1', 'netAsset1', 'surplus1', 'ctax2', 'profit2', 'liquidAsset2', 'receivableS2', 'inventoryAsset2', 'tanAsset2', 'NCLiabilities2', 'longLoan2', 'netAsset2', 'surplus2', 'employee1', 'employee2', '평균_총자본경상이익율', '총자본경상이익율_증감', '평균_당좌비율', '당좌비율_증감', '평균_총자본회전율', 'sido']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_transformed = rfe.transform(X_train_over_stand)\n",
        "X_transformed.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2BfOad5lpnC",
        "outputId": "83d890e8-ca7d-4e6e-fb3c-b5db7a523a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(374, 35)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pycaret"
      ],
      "metadata": {
        "id": "yMGYDDskjKh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pycaret에 없는 모델 설치\n",
        "!pip install xgboost\n",
        "!pip install --upgrade xgboost\n",
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmK46JITnYnx",
        "outputId": "1ca595b7-b3c6-48b9-a4be-4891cd55ffab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.5.4)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.5.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.7)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pycaret 설치\n",
        "!pip install pycaret \n",
        "#!pip install category_encoders\n",
        "# !pip install --upgrade pycaret"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4pZ54PqnXtE",
        "outputId": "2720fbcc-f7d7-4acd-ce54-988dc2f162eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycaret in /usr/local/lib/python3.7/dist-packages (2.3.6)\n",
            "Requirement already satisfied: Boruta in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.3)\n",
            "Requirement already satisfied: scipy<=1.5.4 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.5.4)\n",
            "Requirement already satisfied: lightgbm>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.3.2)\n",
            "Requirement already satisfied: cufflinks>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.17.3)\n",
            "Requirement already satisfied: spacy<2.4.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (2.2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.3.5)\n",
            "Requirement already satisfied: pandas-profiling>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.1.0)\n",
            "Requirement already satisfied: pyyaml<6.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (5.4.1)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.5.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.11.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.2.2)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.24.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from pycaret) (5.5.0)\n",
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.2.2)\n",
            "Requirement already satisfied: plotly>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (5.5.0)\n",
            "Requirement already satisfied: imbalanced-learn==0.7.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.7.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.15.3)\n",
            "Requirement already satisfied: pyod in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.9.7)\n",
            "Requirement already satisfied: yellowbrick>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.3.post1)\n",
            "Requirement already satisfied: kmodes>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.11.1)\n",
            "Requirement already satisfied: gensim<4.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.6.0)\n",
            "Requirement already satisfied: mlxtend>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.19.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.5.0)\n",
            "Requirement already satisfied: scikit-plot in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.3.7)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from pycaret) (7.6.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.2.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.7.0->pycaret) (1.19.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->pycaret) (3.1.0)\n",
            "Requirement already satisfied: colorlover>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=34.4.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks>=0.17.0->pycaret) (57.4.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0.0->pycaret) (5.2.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->pycaret) (5.1.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret) (3.5.2)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret) (4.10.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret) (1.0.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->pycaret) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->pycaret) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->pycaret) (5.1.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=2.3.1->pycaret) (0.37.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pycaret) (0.11.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->pycaret) (4.9.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->pycaret) (4.11.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->pycaret) (0.18.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->pycaret) (3.10.0.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->pycaret) (5.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->pycaret) (21.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->pycaret) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pycaret) (2018.9)\n",
            "Requirement already satisfied: htmlmin>=0.1.12 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret) (0.1.12)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret) (0.5.0)\n",
            "Requirement already satisfied: phik>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret) (0.12.0)\n",
            "Requirement already satisfied: markupsafe~=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret) (2.0.1)\n",
            "Requirement already satisfied: multimethod>=1.4 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret) (1.7)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret) (2.11.3)\n",
            "Requirement already satisfied: visions[type_image_path]==0.7.4 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret) (0.7.4)\n",
            "Requirement already satisfied: tangled-up-in-unicode==0.1.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret) (0.1.0)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret) (4.62.3)\n",
            "Requirement already satisfied: pydantic>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling>=2.8.0->pycaret) (1.9.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret) (2.6.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret) (7.1.2)\n",
            "Requirement already satisfied: imagehash in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret) (4.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.4.1->pycaret) (8.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->pycaret) (0.2.5)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret) (1.24.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret) (0.9.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret) (3.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0->pycaret) (1.1.3)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets->pycaret) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.13.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->pycaret) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.7.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling>=2.8.0->pycaret) (1.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (0.4)\n",
            "Requirement already satisfied: docker>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (5.0.3)\n",
            "Requirement already satisfied: prometheus-flask-exporter in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (0.18.7)\n",
            "Requirement already satisfied: querystring-parser in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (1.2.4)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (0.4.2)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (1.1.4)\n",
            "Requirement already satisfied: gitpython>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (3.1.27)\n",
            "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (3.17.3)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (1.7.6)\n",
            "Requirement already satisfied: databricks-cli>=0.8.7 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (0.16.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (7.1.2)\n",
            "Requirement already satisfied: gunicorn in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (20.1.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (1.3.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (1.4.31)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow->pycaret) (21.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli>=0.8.7->mlflow->pycaret) (0.8.9)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from docker>=4.0.0->mlflow->pycaret) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython>=2.1.0->mlflow->pycaret) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow->pycaret) (5.0.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->mlflow->pycaret) (1.1.6)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->mlflow->pycaret) (1.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow->pycaret) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask->mlflow->pycaret) (1.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (4.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.5.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter->mlflow->pycaret) (0.13.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret) (2.8.1)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret) (1.17)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis->pycaret) (0.16.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod->pycaret) (0.10.2)\n",
            "Requirement already satisfied: numba>=0.35 in /usr/local/lib/python3.7/dist-packages (from pyod->pycaret) (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.35->pyod->pycaret) (0.34.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pyod->pycaret) (0.5.2)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn->pycaret) (0.5.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pycaret\n",
        "from pycaret.classification import *"
      ],
      "metadata": {
        "id": "AO3nuWaunkGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_transformed = pd.DataFrame(X_transformed,columns=['openYear', 'revenue1', 'salescost1', 'sga1', 'salary1', 'noi1', 'noe1', 'interest1', 'ctax1', 'profit1', 'liquidAsset1', 'quickAsset1', 'inventoryAsset1', 'liquidLiabilities1', 'shortLoan1', 'netAsset1', 'surplus1', 'ctax2', 'profit2', 'liquidAsset2', 'receivableS2', 'inventoryAsset2', 'tanAsset2', 'NCLiabilities2', 'longLoan2', 'netAsset2', 'surplus2', 'employee1', 'employee2', '평균_총자본경상이익율', '총자본경상이익율_증감', '평균_당좌비율', '당좌비율_증감', '평균_총자본회전율', 'sido'])"
      ],
      "metadata": {
        "id": "NnWuR20XomHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_stand1 = X_val_stand1[['openYear', 'revenue1', 'salescost1', 'sga1', 'salary1', 'noi1', 'noe1', 'interest1', 'ctax1', 'profit1', 'liquidAsset1', 'quickAsset1', 'inventoryAsset1', 'liquidLiabilities1', 'shortLoan1', 'netAsset1', 'surplus1', 'ctax2', 'profit2', 'liquidAsset2', 'receivableS2', 'inventoryAsset2', 'tanAsset2', 'NCLiabilities2', 'longLoan2', 'netAsset2', 'surplus2', 'employee1', 'employee2', '평균_총자본경상이익율', '총자본경상이익율_증감', '평균_당좌비율', '당좌비율_증감', '평균_총자본회전율', 'sido']]"
      ],
      "metadata": {
        "id": "wwtyZqzIp64J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([X_transformed,y_train_over_stand.reset_index(drop=True)],axis=1)"
      ],
      "metadata": {
        "id": "hFZ4-5Vao1yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_oversampling = train[train['OC']==0]"
      ],
      "metadata": {
        "id": "_Cn_ud5HBafq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([train,train_oversampling],axis=0)"
      ],
      "metadata": {
        "id": "iSF40EE7BlXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 전체 다합쳐서 시도"
      ],
      "metadata": {
        "id": "pHsMAGvmYoVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_over.reset_index(drop = True, inplace = True)\n",
        "y_over.reset_index(drop = True, inplace = True)"
      ],
      "metadata": {
        "id": "ym7kq1BzYOH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([X_over, y_over],axis=1)"
      ],
      "metadata": {
        "id": "5HEzv3BkYCvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rbb7gRAgpIxA",
        "outputId": "95061d55-86a7-44e3-c0c8-771a69245d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c72b0de1-b763-4d6c-b0aa-7902e86f58e7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>openYear</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>quickAsset1</th>\n",
              "      <th>inventoryAsset1</th>\n",
              "      <th>OnonCAsset1</th>\n",
              "      <th>liquidLiabilities1</th>\n",
              "      <th>NCLiabilities1</th>\n",
              "      <th>netAsset1</th>\n",
              "      <th>surplus1</th>\n",
              "      <th>inventoryAsset2</th>\n",
              "      <th>nonCAsset2</th>\n",
              "      <th>tanAsset2</th>\n",
              "      <th>OnonCAsset2</th>\n",
              "      <th>surplus2</th>\n",
              "      <th>employee1</th>\n",
              "      <th>평균_당좌비율</th>\n",
              "      <th>당좌비율_증감</th>\n",
              "      <th>평균_총자본회전율</th>\n",
              "      <th>총자본회전율_증감</th>\n",
              "      <th>sido</th>\n",
              "      <th>ownerChange_same</th>\n",
              "      <th>OC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.617597</td>\n",
              "      <td>-0.205325</td>\n",
              "      <td>-0.036654</td>\n",
              "      <td>-0.445741</td>\n",
              "      <td>-0.432738</td>\n",
              "      <td>-0.253380</td>\n",
              "      <td>-0.671966</td>\n",
              "      <td>-0.583419</td>\n",
              "      <td>-0.290859</td>\n",
              "      <td>0.005048</td>\n",
              "      <td>-0.423697</td>\n",
              "      <td>-0.559528</td>\n",
              "      <td>-0.558825</td>\n",
              "      <td>-0.256366</td>\n",
              "      <td>0.022217</td>\n",
              "      <td>-0.490424</td>\n",
              "      <td>0.632457</td>\n",
              "      <td>0.449676</td>\n",
              "      <td>-0.067667</td>\n",
              "      <td>-0.054746</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.501097</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>3.419262</td>\n",
              "      <td>0.367563</td>\n",
              "      <td>0.558400</td>\n",
              "      <td>1.167249</td>\n",
              "      <td>4.976478</td>\n",
              "      <td>3.487949</td>\n",
              "      <td>3.452640</td>\n",
              "      <td>1.075875</td>\n",
              "      <td>0.514328</td>\n",
              "      <td>0.797258</td>\n",
              "      <td>4.124054</td>\n",
              "      <td>1.563348</td>\n",
              "      <td>1.459804</td>\n",
              "      <td>3.705636</td>\n",
              "      <td>1.286116</td>\n",
              "      <td>3.432879</td>\n",
              "      <td>-0.432617</td>\n",
              "      <td>-0.637886</td>\n",
              "      <td>-0.055363</td>\n",
              "      <td>-0.004410</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>3.795674</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.163469</td>\n",
              "      <td>0.300603</td>\n",
              "      <td>1.122079</td>\n",
              "      <td>-0.175744</td>\n",
              "      <td>3.588785</td>\n",
              "      <td>1.866315</td>\n",
              "      <td>-0.492521</td>\n",
              "      <td>0.839587</td>\n",
              "      <td>1.173996</td>\n",
              "      <td>1.961598</td>\n",
              "      <td>2.129604</td>\n",
              "      <td>0.419844</td>\n",
              "      <td>1.537099</td>\n",
              "      <td>0.449602</td>\n",
              "      <td>-0.522229</td>\n",
              "      <td>0.476809</td>\n",
              "      <td>15.519711</td>\n",
              "      <td>-5.958044</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-2.525937</td>\n",
              "      <td>0.272113</td>\n",
              "      <td>2.546376</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.524769</td>\n",
              "      <td>1.121564</td>\n",
              "      <td>0.133545</td>\n",
              "      <td>0.067749</td>\n",
              "      <td>2.318304</td>\n",
              "      <td>-0.190194</td>\n",
              "      <td>0.555297</td>\n",
              "      <td>0.854295</td>\n",
              "      <td>-0.198369</td>\n",
              "      <td>0.869795</td>\n",
              "      <td>0.970115</td>\n",
              "      <td>0.105405</td>\n",
              "      <td>1.566155</td>\n",
              "      <td>0.547521</td>\n",
              "      <td>-0.378928</td>\n",
              "      <td>-0.705713</td>\n",
              "      <td>-0.068386</td>\n",
              "      <td>0.104751</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.963482</td>\n",
              "      <td>2.282292</td>\n",
              "      <td>0.926513</td>\n",
              "      <td>1.781553</td>\n",
              "      <td>0.163683</td>\n",
              "      <td>1.219979</td>\n",
              "      <td>2.693899</td>\n",
              "      <td>-0.358995</td>\n",
              "      <td>3.092185</td>\n",
              "      <td>1.906199</td>\n",
              "      <td>-0.172511</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>2.738065</td>\n",
              "      <td>1.674385</td>\n",
              "      <td>1.820745</td>\n",
              "      <td>-0.396213</td>\n",
              "      <td>1.566741</td>\n",
              "      <td>2.297014</td>\n",
              "      <td>-0.409102</td>\n",
              "      <td>-0.709298</td>\n",
              "      <td>-0.041466</td>\n",
              "      <td>0.286898</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.501097</td>\n",
              "      <td>0.540174</td>\n",
              "      <td>0.867023</td>\n",
              "      <td>-0.116245</td>\n",
              "      <td>-0.835136</td>\n",
              "      <td>0.002257</td>\n",
              "      <td>1.745185</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>1.769989</td>\n",
              "      <td>1.017079</td>\n",
              "      <td>0.943427</td>\n",
              "      <td>0.844547</td>\n",
              "      <td>1.612173</td>\n",
              "      <td>2.223990</td>\n",
              "      <td>2.298452</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>1.640450</td>\n",
              "      <td>1.520187</td>\n",
              "      <td>-0.513712</td>\n",
              "      <td>0.414152</td>\n",
              "      <td>-0.065557</td>\n",
              "      <td>-0.044455</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>0.813740</td>\n",
              "      <td>4.325947</td>\n",
              "      <td>-0.285140</td>\n",
              "      <td>0.079059</td>\n",
              "      <td>1.111742</td>\n",
              "      <td>0.465331</td>\n",
              "      <td>0.041722</td>\n",
              "      <td>2.923905</td>\n",
              "      <td>2.760236</td>\n",
              "      <td>0.906190</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.793332</td>\n",
              "      <td>2.247521</td>\n",
              "      <td>2.373852</td>\n",
              "      <td>-0.125335</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.549176</td>\n",
              "      <td>-0.454163</td>\n",
              "      <td>0.188948</td>\n",
              "      <td>-0.062550</td>\n",
              "      <td>-0.168377</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.259322</td>\n",
              "      <td>0.571773</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.232834</td>\n",
              "      <td>-0.413541</td>\n",
              "      <td>-0.269543</td>\n",
              "      <td>-0.182693</td>\n",
              "      <td>1.133007</td>\n",
              "      <td>-0.299875</td>\n",
              "      <td>0.082828</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.436053</td>\n",
              "      <td>0.585193</td>\n",
              "      <td>0.662358</td>\n",
              "      <td>-0.147083</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.542648</td>\n",
              "      <td>-0.511707</td>\n",
              "      <td>-1.334592</td>\n",
              "      <td>-0.071850</td>\n",
              "      <td>0.103104</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>3.856058</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.955170</td>\n",
              "      <td>0.898376</td>\n",
              "      <td>4.904688</td>\n",
              "      <td>1.541228</td>\n",
              "      <td>4.486770</td>\n",
              "      <td>2.105641</td>\n",
              "      <td>0.048221</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>4.880046</td>\n",
              "      <td>2.639711</td>\n",
              "      <td>2.430828</td>\n",
              "      <td>4.869975</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.469185</td>\n",
              "      <td>-0.494970</td>\n",
              "      <td>-0.066297</td>\n",
              "      <td>-0.046784</td>\n",
              "      <td>0.103987</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-2.638429</td>\n",
              "      <td>4.129403</td>\n",
              "      <td>2.396504</td>\n",
              "      <td>0.222862</td>\n",
              "      <td>-3.530603</td>\n",
              "      <td>2.290492</td>\n",
              "      <td>0.028943</td>\n",
              "      <td>9.530314</td>\n",
              "      <td>5.141929</td>\n",
              "      <td>3.579425</td>\n",
              "      <td>-0.435326</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.249824</td>\n",
              "      <td>2.561691</td>\n",
              "      <td>2.258991</td>\n",
              "      <td>8.804797</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>5.312931</td>\n",
              "      <td>-0.431466</td>\n",
              "      <td>-0.284875</td>\n",
              "      <td>0.207476</td>\n",
              "      <td>0.341113</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-2.525937</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.427624</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-1.958237</td>\n",
              "      <td>-0.033736</td>\n",
              "      <td>-0.401957</td>\n",
              "      <td>1.245904</td>\n",
              "      <td>1.587122</td>\n",
              "      <td>-0.247918</td>\n",
              "      <td>-0.271551</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.402469</td>\n",
              "      <td>0.404265</td>\n",
              "      <td>0.359636</td>\n",
              "      <td>1.326864</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.299459</td>\n",
              "      <td>-0.505456</td>\n",
              "      <td>-0.097423</td>\n",
              "      <td>-0.059510</td>\n",
              "      <td>0.168267</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>0.765085</td>\n",
              "      <td>2.891795</td>\n",
              "      <td>-0.173120</td>\n",
              "      <td>0.114419</td>\n",
              "      <td>2.824658</td>\n",
              "      <td>1.263253</td>\n",
              "      <td>1.989936</td>\n",
              "      <td>2.337322</td>\n",
              "      <td>3.140412</td>\n",
              "      <td>3.826493</td>\n",
              "      <td>-0.058609</td>\n",
              "      <td>1.662156</td>\n",
              "      <td>3.636578</td>\n",
              "      <td>3.845196</td>\n",
              "      <td>2.201624</td>\n",
              "      <td>-0.123590</td>\n",
              "      <td>1.774777</td>\n",
              "      <td>-0.249669</td>\n",
              "      <td>-0.312353</td>\n",
              "      <td>-0.070796</td>\n",
              "      <td>0.016214</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-1.176044</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.557236</td>\n",
              "      <td>-0.007898</td>\n",
              "      <td>-0.114114</td>\n",
              "      <td>0.230034</td>\n",
              "      <td>0.949135</td>\n",
              "      <td>1.629977</td>\n",
              "      <td>1.313517</td>\n",
              "      <td>-0.419088</td>\n",
              "      <td>0.018965</td>\n",
              "      <td>0.394611</td>\n",
              "      <td>0.952631</td>\n",
              "      <td>0.192789</td>\n",
              "      <td>0.107899</td>\n",
              "      <td>1.752299</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.371266</td>\n",
              "      <td>-0.486769</td>\n",
              "      <td>1.418992</td>\n",
              "      <td>-0.056490</td>\n",
              "      <td>-0.430929</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>-0.134787</td>\n",
              "      <td>0.947165</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.254478</td>\n",
              "      <td>-0.257839</td>\n",
              "      <td>-0.244588</td>\n",
              "      <td>2.457257</td>\n",
              "      <td>0.982147</td>\n",
              "      <td>0.022897</td>\n",
              "      <td>-0.199969</td>\n",
              "      <td>-0.049077</td>\n",
              "      <td>-0.211534</td>\n",
              "      <td>0.373862</td>\n",
              "      <td>0.271525</td>\n",
              "      <td>2.831924</td>\n",
              "      <td>-0.029189</td>\n",
              "      <td>1.422267</td>\n",
              "      <td>-0.506387</td>\n",
              "      <td>-0.603727</td>\n",
              "      <td>-0.060665</td>\n",
              "      <td>0.100306</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>9.028202</td>\n",
              "      <td>0.973668</td>\n",
              "      <td>0.209211</td>\n",
              "      <td>2.194315</td>\n",
              "      <td>3.560993</td>\n",
              "      <td>1.685868</td>\n",
              "      <td>4.922489</td>\n",
              "      <td>5.188743</td>\n",
              "      <td>2.045889</td>\n",
              "      <td>1.050647</td>\n",
              "      <td>2.691613</td>\n",
              "      <td>5.083747</td>\n",
              "      <td>5.275722</td>\n",
              "      <td>0.405878</td>\n",
              "      <td>1.977729</td>\n",
              "      <td>3.060785</td>\n",
              "      <td>-0.468109</td>\n",
              "      <td>1.321384</td>\n",
              "      <td>-0.066483</td>\n",
              "      <td>-0.003440</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>-2.525937</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.125449</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-1.811782</td>\n",
              "      <td>-0.390557</td>\n",
              "      <td>0.029380</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>0.375746</td>\n",
              "      <td>-0.563695</td>\n",
              "      <td>-0.335374</td>\n",
              "      <td>0.506399</td>\n",
              "      <td>-0.074626</td>\n",
              "      <td>-0.206841</td>\n",
              "      <td>-0.176592</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>0.949849</td>\n",
              "      <td>-0.131387</td>\n",
              "      <td>-0.509642</td>\n",
              "      <td>-0.594799</td>\n",
              "      <td>-0.059896</td>\n",
              "      <td>0.050217</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.184720</td>\n",
              "      <td>0.532729</td>\n",
              "      <td>0.599440</td>\n",
              "      <td>0.631646</td>\n",
              "      <td>-0.201404</td>\n",
              "      <td>-0.406969</td>\n",
              "      <td>-0.394095</td>\n",
              "      <td>0.883783</td>\n",
              "      <td>-0.471860</td>\n",
              "      <td>0.374715</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.368705</td>\n",
              "      <td>0.588722</td>\n",
              "      <td>0.670381</td>\n",
              "      <td>-0.409210</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.175427</td>\n",
              "      <td>-0.410905</td>\n",
              "      <td>-1.383260</td>\n",
              "      <td>-0.069470</td>\n",
              "      <td>0.087882</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.951061</td>\n",
              "      <td>0.516099</td>\n",
              "      <td>0.094727</td>\n",
              "      <td>0.039989</td>\n",
              "      <td>0.113957</td>\n",
              "      <td>-0.055739</td>\n",
              "      <td>0.013262</td>\n",
              "      <td>-0.072765</td>\n",
              "      <td>1.008014</td>\n",
              "      <td>-0.247680</td>\n",
              "      <td>-0.212576</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.031022</td>\n",
              "      <td>0.174449</td>\n",
              "      <td>0.231900</td>\n",
              "      <td>-0.149095</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.841279</td>\n",
              "      <td>-0.493273</td>\n",
              "      <td>0.234345</td>\n",
              "      <td>-0.051043</td>\n",
              "      <td>-0.054223</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.253057</td>\n",
              "      <td>-0.257636</td>\n",
              "      <td>-0.152458</td>\n",
              "      <td>-0.422903</td>\n",
              "      <td>-0.188600</td>\n",
              "      <td>-0.362490</td>\n",
              "      <td>0.505165</td>\n",
              "      <td>-0.326209</td>\n",
              "      <td>-0.480146</td>\n",
              "      <td>-0.243329</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.080362</td>\n",
              "      <td>-0.048286</td>\n",
              "      <td>-0.375459</td>\n",
              "      <td>-0.139464</td>\n",
              "      <td>0.123204</td>\n",
              "      <td>-0.462550</td>\n",
              "      <td>-1.462570</td>\n",
              "      <td>-0.002578</td>\n",
              "      <td>0.337940</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-0.838570</td>\n",
              "      <td>0.438402</td>\n",
              "      <td>0.462978</td>\n",
              "      <td>0.315572</td>\n",
              "      <td>-0.186000</td>\n",
              "      <td>-0.278473</td>\n",
              "      <td>0.390213</td>\n",
              "      <td>-0.326750</td>\n",
              "      <td>1.097777</td>\n",
              "      <td>0.519472</td>\n",
              "      <td>0.471761</td>\n",
              "      <td>1.363481</td>\n",
              "      <td>0.303359</td>\n",
              "      <td>0.463723</td>\n",
              "      <td>0.543248</td>\n",
              "      <td>-0.164004</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.691136</td>\n",
              "      <td>-0.527826</td>\n",
              "      <td>0.257833</td>\n",
              "      <td>-0.142841</td>\n",
              "      <td>9.861703</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>-0.163623</td>\n",
              "      <td>-0.096146</td>\n",
              "      <td>0.437162</td>\n",
              "      <td>-0.292900</td>\n",
              "      <td>-0.137870</td>\n",
              "      <td>-0.498448</td>\n",
              "      <td>0.387293</td>\n",
              "      <td>-0.264049</td>\n",
              "      <td>0.597032</td>\n",
              "      <td>0.301352</td>\n",
              "      <td>-0.081644</td>\n",
              "      <td>0.302418</td>\n",
              "      <td>0.236165</td>\n",
              "      <td>0.162919</td>\n",
              "      <td>0.207474</td>\n",
              "      <td>-0.254570</td>\n",
              "      <td>0.568857</td>\n",
              "      <td>0.070980</td>\n",
              "      <td>-0.546373</td>\n",
              "      <td>-0.623826</td>\n",
              "      <td>-0.064227</td>\n",
              "      <td>-0.004891</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>0.710018</td>\n",
              "      <td>0.348849</td>\n",
              "      <td>1.998534</td>\n",
              "      <td>0.536067</td>\n",
              "      <td>0.322413</td>\n",
              "      <td>2.019599</td>\n",
              "      <td>0.385846</td>\n",
              "      <td>1.676109</td>\n",
              "      <td>1.379570</td>\n",
              "      <td>0.219978</td>\n",
              "      <td>-0.160811</td>\n",
              "      <td>1.823645</td>\n",
              "      <td>1.448932</td>\n",
              "      <td>1.494415</td>\n",
              "      <td>0.541394</td>\n",
              "      <td>-0.226123</td>\n",
              "      <td>1.833529</td>\n",
              "      <td>-0.462856</td>\n",
              "      <td>-0.254803</td>\n",
              "      <td>-0.051305</td>\n",
              "      <td>-0.077361</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>0.040210</td>\n",
              "      <td>0.509802</td>\n",
              "      <td>0.872403</td>\n",
              "      <td>0.797390</td>\n",
              "      <td>-0.180503</td>\n",
              "      <td>-0.045281</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>2.446462</td>\n",
              "      <td>0.553623</td>\n",
              "      <td>0.132666</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.260842</td>\n",
              "      <td>1.497176</td>\n",
              "      <td>1.486063</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.730304</td>\n",
              "      <td>-0.540687</td>\n",
              "      <td>0.561391</td>\n",
              "      <td>-0.065686</td>\n",
              "      <td>-0.100760</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>0.007300</td>\n",
              "      <td>-0.073679</td>\n",
              "      <td>-0.057915</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>-0.305346</td>\n",
              "      <td>-0.024241</td>\n",
              "      <td>-0.334184</td>\n",
              "      <td>0.322413</td>\n",
              "      <td>-0.637951</td>\n",
              "      <td>-0.200549</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.097976</td>\n",
              "      <td>-0.199866</td>\n",
              "      <td>-0.203356</td>\n",
              "      <td>-0.284800</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.288058</td>\n",
              "      <td>-0.490049</td>\n",
              "      <td>-0.296517</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>0.064899</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.218258</td>\n",
              "      <td>-0.125105</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.479270</td>\n",
              "      <td>-0.454082</td>\n",
              "      <td>-0.176884</td>\n",
              "      <td>-0.053183</td>\n",
              "      <td>0.373347</td>\n",
              "      <td>-0.495858</td>\n",
              "      <td>-0.308421</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.121233</td>\n",
              "      <td>-0.115490</td>\n",
              "      <td>-0.098125</td>\n",
              "      <td>-0.043888</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.281530</td>\n",
              "      <td>-0.538360</td>\n",
              "      <td>0.241465</td>\n",
              "      <td>-0.064051</td>\n",
              "      <td>-0.008820</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>-0.164877</td>\n",
              "      <td>-0.084477</td>\n",
              "      <td>1.163972</td>\n",
              "      <td>0.620810</td>\n",
              "      <td>2.096655</td>\n",
              "      <td>-0.160225</td>\n",
              "      <td>-0.296827</td>\n",
              "      <td>0.325854</td>\n",
              "      <td>-0.001624</td>\n",
              "      <td>1.372793</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.226125</td>\n",
              "      <td>0.404964</td>\n",
              "      <td>0.479545</td>\n",
              "      <td>-0.184915</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.213373</td>\n",
              "      <td>0.112565</td>\n",
              "      <td>-0.010902</td>\n",
              "      <td>-0.070007</td>\n",
              "      <td>-0.028929</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>1.089551</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.885374</td>\n",
              "      <td>-0.208105</td>\n",
              "      <td>0.737657</td>\n",
              "      <td>-0.134881</td>\n",
              "      <td>0.119062</td>\n",
              "      <td>0.100493</td>\n",
              "      <td>0.378855</td>\n",
              "      <td>0.560232</td>\n",
              "      <td>1.329012</td>\n",
              "      <td>0.487543</td>\n",
              "      <td>0.567017</td>\n",
              "      <td>-0.128790</td>\n",
              "      <td>1.044729</td>\n",
              "      <td>-0.000828</td>\n",
              "      <td>-0.454665</td>\n",
              "      <td>0.245298</td>\n",
              "      <td>-0.070424</td>\n",
              "      <td>-0.051857</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.163623</td>\n",
              "      <td>0.034039</td>\n",
              "      <td>-0.061440</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.724426</td>\n",
              "      <td>-0.269630</td>\n",
              "      <td>0.096048</td>\n",
              "      <td>-0.392522</td>\n",
              "      <td>0.193353</td>\n",
              "      <td>-0.518754</td>\n",
              "      <td>-0.375654</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>1.198819</td>\n",
              "      <td>-0.302478</td>\n",
              "      <td>-0.272182</td>\n",
              "      <td>-0.407530</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.307642</td>\n",
              "      <td>-0.412050</td>\n",
              "      <td>-1.068820</td>\n",
              "      <td>-0.057893</td>\n",
              "      <td>0.196026</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.103861</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-1.366429</td>\n",
              "      <td>-0.329160</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.050113</td>\n",
              "      <td>0.144017</td>\n",
              "      <td>-0.488055</td>\n",
              "      <td>-0.349113</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.432253</td>\n",
              "      <td>-0.173147</td>\n",
              "      <td>-0.153111</td>\n",
              "      <td>-0.013777</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.248890</td>\n",
              "      <td>-0.479855</td>\n",
              "      <td>-0.329874</td>\n",
              "      <td>-0.065517</td>\n",
              "      <td>0.245779</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>11.865412</td>\n",
              "      <td>-0.474031</td>\n",
              "      <td>0.898139</td>\n",
              "      <td>0.637125</td>\n",
              "      <td>5.798874</td>\n",
              "      <td>3.200101</td>\n",
              "      <td>5.006392</td>\n",
              "      <td>3.728543</td>\n",
              "      <td>0.517201</td>\n",
              "      <td>1.655230</td>\n",
              "      <td>-0.156027</td>\n",
              "      <td>3.670407</td>\n",
              "      <td>0.317257</td>\n",
              "      <td>0.024399</td>\n",
              "      <td>4.529687</td>\n",
              "      <td>-0.217690</td>\n",
              "      <td>2.068535</td>\n",
              "      <td>-0.118096</td>\n",
              "      <td>-0.553970</td>\n",
              "      <td>-0.056194</td>\n",
              "      <td>0.018309</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>0.233070</td>\n",
              "      <td>1.696367</td>\n",
              "      <td>0.430139</td>\n",
              "      <td>0.136929</td>\n",
              "      <td>0.113168</td>\n",
              "      <td>0.862865</td>\n",
              "      <td>0.468620</td>\n",
              "      <td>0.678402</td>\n",
              "      <td>1.159005</td>\n",
              "      <td>0.716344</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.627836</td>\n",
              "      <td>1.329996</td>\n",
              "      <td>1.429656</td>\n",
              "      <td>0.573963</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.443074</td>\n",
              "      <td>-0.410695</td>\n",
              "      <td>-0.537122</td>\n",
              "      <td>-0.068750</td>\n",
              "      <td>-0.018790</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>0.024721</td>\n",
              "      <td>-0.434000</td>\n",
              "      <td>2.869688</td>\n",
              "      <td>2.144820</td>\n",
              "      <td>2.497966</td>\n",
              "      <td>-0.290516</td>\n",
              "      <td>2.900726</td>\n",
              "      <td>1.389342</td>\n",
              "      <td>0.745168</td>\n",
              "      <td>1.155797</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.140888</td>\n",
              "      <td>0.938613</td>\n",
              "      <td>0.875608</td>\n",
              "      <td>2.980070</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.475713</td>\n",
              "      <td>-0.238592</td>\n",
              "      <td>0.726185</td>\n",
              "      <td>-0.068580</td>\n",
              "      <td>-0.049965</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.231135</td>\n",
              "      <td>-0.103038</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.401125</td>\n",
              "      <td>-0.008166</td>\n",
              "      <td>-0.411799</td>\n",
              "      <td>-0.374868</td>\n",
              "      <td>0.443549</td>\n",
              "      <td>-0.166627</td>\n",
              "      <td>-0.325390</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.409953</td>\n",
              "      <td>-0.143451</td>\n",
              "      <td>-0.147221</td>\n",
              "      <td>-0.104926</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.039995</td>\n",
              "      <td>-0.443670</td>\n",
              "      <td>0.201484</td>\n",
              "      <td>-0.063848</td>\n",
              "      <td>0.052711</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>-0.206067</td>\n",
              "      <td>-0.240598</td>\n",
              "      <td>0.113011</td>\n",
              "      <td>0.186962</td>\n",
              "      <td>-0.306939</td>\n",
              "      <td>-0.366741</td>\n",
              "      <td>-0.039094</td>\n",
              "      <td>-0.023583</td>\n",
              "      <td>-0.510876</td>\n",
              "      <td>-0.034858</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.369148</td>\n",
              "      <td>-0.139687</td>\n",
              "      <td>-0.111101</td>\n",
              "      <td>-0.110444</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.066107</td>\n",
              "      <td>-0.192604</td>\n",
              "      <td>-1.529325</td>\n",
              "      <td>-0.068656</td>\n",
              "      <td>-0.035237</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>-0.163623</td>\n",
              "      <td>-0.052975</td>\n",
              "      <td>0.102007</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.050497</td>\n",
              "      <td>-0.366199</td>\n",
              "      <td>-0.083230</td>\n",
              "      <td>-0.286335</td>\n",
              "      <td>0.454482</td>\n",
              "      <td>-0.417308</td>\n",
              "      <td>-0.416692</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.004001</td>\n",
              "      <td>-0.218748</td>\n",
              "      <td>-0.207932</td>\n",
              "      <td>-0.310947</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.111803</td>\n",
              "      <td>-0.507230</td>\n",
              "      <td>-0.590689</td>\n",
              "      <td>-0.051317</td>\n",
              "      <td>0.217542</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>-1.850991</td>\n",
              "      <td>0.494343</td>\n",
              "      <td>0.195210</td>\n",
              "      <td>0.100467</td>\n",
              "      <td>-0.165348</td>\n",
              "      <td>0.039473</td>\n",
              "      <td>0.750157</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>0.393561</td>\n",
              "      <td>-0.049036</td>\n",
              "      <td>0.133609</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.400752</td>\n",
              "      <td>0.329554</td>\n",
              "      <td>0.399710</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.030590</td>\n",
              "      <td>-0.434936</td>\n",
              "      <td>0.480001</td>\n",
              "      <td>-0.062497</td>\n",
              "      <td>-0.012396</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.214190</td>\n",
              "      <td>-0.228380</td>\n",
              "      <td>-0.261833</td>\n",
              "      <td>-0.082479</td>\n",
              "      <td>-0.369934</td>\n",
              "      <td>-0.396673</td>\n",
              "      <td>-0.213524</td>\n",
              "      <td>0.049902</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.126658</td>\n",
              "      <td>-0.023600</td>\n",
              "      <td>-0.069405</td>\n",
              "      <td>-0.353728</td>\n",
              "      <td>-0.378384</td>\n",
              "      <td>0.025336</td>\n",
              "      <td>-0.018080</td>\n",
              "      <td>-0.333753</td>\n",
              "      <td>-0.440676</td>\n",
              "      <td>-1.058849</td>\n",
              "      <td>-0.069153</td>\n",
              "      <td>-0.018043</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>-0.726079</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.469471</td>\n",
              "      <td>-0.233295</td>\n",
              "      <td>-0.197694</td>\n",
              "      <td>0.367768</td>\n",
              "      <td>2.537180</td>\n",
              "      <td>1.200157</td>\n",
              "      <td>1.358069</td>\n",
              "      <td>1.493049</td>\n",
              "      <td>0.012570</td>\n",
              "      <td>0.349655</td>\n",
              "      <td>4.048043</td>\n",
              "      <td>1.087575</td>\n",
              "      <td>1.155165</td>\n",
              "      <td>0.474590</td>\n",
              "      <td>0.689061</td>\n",
              "      <td>1.063230</td>\n",
              "      <td>-0.446779</td>\n",
              "      <td>-0.077613</td>\n",
              "      <td>-0.053631</td>\n",
              "      <td>0.001408</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.216052</td>\n",
              "      <td>-0.056753</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.125691</td>\n",
              "      <td>-0.476031</td>\n",
              "      <td>-0.350022</td>\n",
              "      <td>-0.314347</td>\n",
              "      <td>0.067410</td>\n",
              "      <td>-0.490423</td>\n",
              "      <td>-0.370047</td>\n",
              "      <td>-0.086998</td>\n",
              "      <td>-0.344697</td>\n",
              "      <td>-0.276198</td>\n",
              "      <td>-0.251438</td>\n",
              "      <td>-0.270825</td>\n",
              "      <td>-0.120205</td>\n",
              "      <td>-0.418617</td>\n",
              "      <td>-0.523084</td>\n",
              "      <td>-0.472010</td>\n",
              "      <td>-0.057431</td>\n",
              "      <td>0.033352</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>-0.726079</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>1.795039</td>\n",
              "      <td>-0.232414</td>\n",
              "      <td>-0.173753</td>\n",
              "      <td>-0.086613</td>\n",
              "      <td>0.275737</td>\n",
              "      <td>-0.232779</td>\n",
              "      <td>0.571446</td>\n",
              "      <td>1.120748</td>\n",
              "      <td>0.280628</td>\n",
              "      <td>-0.209351</td>\n",
              "      <td>0.360241</td>\n",
              "      <td>1.074382</td>\n",
              "      <td>1.185819</td>\n",
              "      <td>-0.248637</td>\n",
              "      <td>-0.325163</td>\n",
              "      <td>-0.118331</td>\n",
              "      <td>-0.465602</td>\n",
              "      <td>-0.042928</td>\n",
              "      <td>-0.068380</td>\n",
              "      <td>0.007737</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.257169</td>\n",
              "      <td>-0.086125</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.153035</td>\n",
              "      <td>-0.383865</td>\n",
              "      <td>-0.477656</td>\n",
              "      <td>-0.359584</td>\n",
              "      <td>0.101884</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.295798</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.463203</td>\n",
              "      <td>-0.317529</td>\n",
              "      <td>-0.287689</td>\n",
              "      <td>-0.386018</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.412089</td>\n",
              "      <td>-0.503594</td>\n",
              "      <td>0.098315</td>\n",
              "      <td>-0.068223</td>\n",
              "      <td>0.006031</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>-0.501097</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.017977</td>\n",
              "      <td>-0.318138</td>\n",
              "      <td>-0.191384</td>\n",
              "      <td>-0.321819</td>\n",
              "      <td>0.525423</td>\n",
              "      <td>-0.382101</td>\n",
              "      <td>0.311311</td>\n",
              "      <td>-0.515168</td>\n",
              "      <td>-0.028167</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.577586</td>\n",
              "      <td>-0.030509</td>\n",
              "      <td>0.024231</td>\n",
              "      <td>-0.396401</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.834751</td>\n",
              "      <td>-0.455017</td>\n",
              "      <td>-1.048649</td>\n",
              "      <td>-0.064993</td>\n",
              "      <td>-0.014502</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.259637</td>\n",
              "      <td>-0.039636</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.708562</td>\n",
              "      <td>-0.446660</td>\n",
              "      <td>-0.429329</td>\n",
              "      <td>-0.232172</td>\n",
              "      <td>-0.137351</td>\n",
              "      <td>-0.043657</td>\n",
              "      <td>-0.430520</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.368470</td>\n",
              "      <td>-0.187950</td>\n",
              "      <td>-0.160652</td>\n",
              "      <td>-0.236292</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.562232</td>\n",
              "      <td>-0.521826</td>\n",
              "      <td>2.863590</td>\n",
              "      <td>-0.060671</td>\n",
              "      <td>0.149910</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.123449</td>\n",
              "      <td>-0.162182</td>\n",
              "      <td>0.277302</td>\n",
              "      <td>0.348843</td>\n",
              "      <td>-0.273271</td>\n",
              "      <td>-0.378530</td>\n",
              "      <td>0.071974</td>\n",
              "      <td>-0.001607</td>\n",
              "      <td>-0.563862</td>\n",
              "      <td>-0.062256</td>\n",
              "      <td>0.148556</td>\n",
              "      <td>-0.354424</td>\n",
              "      <td>-0.189319</td>\n",
              "      <td>-0.164910</td>\n",
              "      <td>-0.268468</td>\n",
              "      <td>0.189388</td>\n",
              "      <td>0.162371</td>\n",
              "      <td>-0.458991</td>\n",
              "      <td>0.312556</td>\n",
              "      <td>-0.065414</td>\n",
              "      <td>-0.058941</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>1.411252</td>\n",
              "      <td>0.225971</td>\n",
              "      <td>0.299881</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.194585</td>\n",
              "      <td>-0.137495</td>\n",
              "      <td>-0.168950</td>\n",
              "      <td>1.204044</td>\n",
              "      <td>0.063477</td>\n",
              "      <td>-0.132171</td>\n",
              "      <td>-0.329557</td>\n",
              "      <td>-0.110909</td>\n",
              "      <td>-0.293790</td>\n",
              "      <td>-0.244635</td>\n",
              "      <td>-0.353809</td>\n",
              "      <td>1.552925</td>\n",
              "      <td>-0.125760</td>\n",
              "      <td>-0.405561</td>\n",
              "      <td>-0.398565</td>\n",
              "      <td>-0.324982</td>\n",
              "      <td>-0.057635</td>\n",
              "      <td>-0.021823</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>-1.288535</td>\n",
              "      <td>-0.187186</td>\n",
              "      <td>0.930632</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.091576</td>\n",
              "      <td>-0.303518</td>\n",
              "      <td>-0.363085</td>\n",
              "      <td>-0.368579</td>\n",
              "      <td>0.077174</td>\n",
              "      <td>0.171364</td>\n",
              "      <td>-0.157413</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.383842</td>\n",
              "      <td>0.127668</td>\n",
              "      <td>0.189375</td>\n",
              "      <td>-0.371452</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.190138</td>\n",
              "      <td>-0.450524</td>\n",
              "      <td>-0.590307</td>\n",
              "      <td>-0.065978</td>\n",
              "      <td>-0.034429</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>-2.413446</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.280551</td>\n",
              "      <td>0.020255</td>\n",
              "      <td>-0.060847</td>\n",
              "      <td>-0.050828</td>\n",
              "      <td>0.602539</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>0.242105</td>\n",
              "      <td>0.441084</td>\n",
              "      <td>0.004122</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.381996</td>\n",
              "      <td>0.338367</td>\n",
              "      <td>0.405393</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.867391</td>\n",
              "      <td>-0.384496</td>\n",
              "      <td>-0.610080</td>\n",
              "      <td>-0.059906</td>\n",
              "      <td>-0.004368</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.266304</td>\n",
              "      <td>-0.026462</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.744164</td>\n",
              "      <td>-0.372857</td>\n",
              "      <td>-0.450242</td>\n",
              "      <td>-0.042860</td>\n",
              "      <td>-0.211022</td>\n",
              "      <td>-0.600263</td>\n",
              "      <td>-0.305130</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.374982</td>\n",
              "      <td>-0.430574</td>\n",
              "      <td>-0.435452</td>\n",
              "      <td>-0.010259</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.607928</td>\n",
              "      <td>-0.434399</td>\n",
              "      <td>-0.427966</td>\n",
              "      <td>-0.070499</td>\n",
              "      <td>0.057306</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.091667</td>\n",
              "      <td>-0.304066</td>\n",
              "      <td>-0.191388</td>\n",
              "      <td>0.111626</td>\n",
              "      <td>-0.402957</td>\n",
              "      <td>-0.451526</td>\n",
              "      <td>-0.345344</td>\n",
              "      <td>-0.136213</td>\n",
              "      <td>-0.562254</td>\n",
              "      <td>-0.322139</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.433867</td>\n",
              "      <td>-0.367670</td>\n",
              "      <td>-0.349997</td>\n",
              "      <td>-0.252056</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.425145</td>\n",
              "      <td>-0.501046</td>\n",
              "      <td>1.531851</td>\n",
              "      <td>-0.065623</td>\n",
              "      <td>0.006396</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>-1.850991</td>\n",
              "      <td>0.003455</td>\n",
              "      <td>-0.360150</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.708183</td>\n",
              "      <td>0.412100</td>\n",
              "      <td>-0.376124</td>\n",
              "      <td>0.660505</td>\n",
              "      <td>0.022068</td>\n",
              "      <td>-0.515281</td>\n",
              "      <td>0.440817</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.364167</td>\n",
              "      <td>0.009405</td>\n",
              "      <td>-0.038838</td>\n",
              "      <td>0.724769</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.053051</td>\n",
              "      <td>-0.174085</td>\n",
              "      <td>-0.301965</td>\n",
              "      <td>-0.070095</td>\n",
              "      <td>-0.056705</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.238415</td>\n",
              "      <td>-0.469249</td>\n",
              "      <td>-0.262642</td>\n",
              "      <td>-0.227424</td>\n",
              "      <td>-0.531692</td>\n",
              "      <td>-0.424058</td>\n",
              "      <td>-0.385488</td>\n",
              "      <td>-0.224573</td>\n",
              "      <td>-0.183224</td>\n",
              "      <td>-0.262260</td>\n",
              "      <td>0.057670</td>\n",
              "      <td>-0.395614</td>\n",
              "      <td>-0.134537</td>\n",
              "      <td>-0.134801</td>\n",
              "      <td>-0.400018</td>\n",
              "      <td>0.157408</td>\n",
              "      <td>-0.457785</td>\n",
              "      <td>-0.533740</td>\n",
              "      <td>-0.119582</td>\n",
              "      <td>-0.068382</td>\n",
              "      <td>0.018691</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>-2.525937</td>\n",
              "      <td>2.105869</td>\n",
              "      <td>1.637513</td>\n",
              "      <td>0.231692</td>\n",
              "      <td>-0.180190</td>\n",
              "      <td>0.164093</td>\n",
              "      <td>0.636417</td>\n",
              "      <td>-0.240114</td>\n",
              "      <td>0.671337</td>\n",
              "      <td>0.317958</td>\n",
              "      <td>-0.030504</td>\n",
              "      <td>0.242711</td>\n",
              "      <td>0.327631</td>\n",
              "      <td>0.368968</td>\n",
              "      <td>0.447287</td>\n",
              "      <td>-0.223633</td>\n",
              "      <td>0.480144</td>\n",
              "      <td>2.414517</td>\n",
              "      <td>-0.420260</td>\n",
              "      <td>-0.161404</td>\n",
              "      <td>-0.045229</td>\n",
              "      <td>-0.007545</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>-0.249935</td>\n",
              "      <td>-0.386751</td>\n",
              "      <td>-0.315209</td>\n",
              "      <td>-0.394770</td>\n",
              "      <td>-0.528626</td>\n",
              "      <td>-0.477874</td>\n",
              "      <td>-0.284784</td>\n",
              "      <td>-0.116197</td>\n",
              "      <td>-0.594862</td>\n",
              "      <td>-0.375562</td>\n",
              "      <td>-0.152324</td>\n",
              "      <td>-0.463230</td>\n",
              "      <td>-0.389354</td>\n",
              "      <td>-0.386082</td>\n",
              "      <td>-0.345443</td>\n",
              "      <td>-0.175479</td>\n",
              "      <td>-0.634039</td>\n",
              "      <td>-0.533926</td>\n",
              "      <td>-0.552660</td>\n",
              "      <td>-0.067936</td>\n",
              "      <td>0.022366</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>-2.188464</td>\n",
              "      <td>-0.015299</td>\n",
              "      <td>1.048615</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.870133</td>\n",
              "      <td>-0.195961</td>\n",
              "      <td>-0.366055</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>0.053687</td>\n",
              "      <td>0.630104</td>\n",
              "      <td>-0.332065</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.377142</td>\n",
              "      <td>0.013516</td>\n",
              "      <td>0.035379</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.358210</td>\n",
              "      <td>-0.417080</td>\n",
              "      <td>-0.351546</td>\n",
              "      <td>-0.057260</td>\n",
              "      <td>0.088078</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>-0.163623</td>\n",
              "      <td>-0.258700</td>\n",
              "      <td>-0.275310</td>\n",
              "      <td>-0.243297</td>\n",
              "      <td>-0.001360</td>\n",
              "      <td>-0.297851</td>\n",
              "      <td>-0.461247</td>\n",
              "      <td>0.124250</td>\n",
              "      <td>-0.287193</td>\n",
              "      <td>-0.639736</td>\n",
              "      <td>-0.299904</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.443234</td>\n",
              "      <td>-0.507748</td>\n",
              "      <td>-0.534443</td>\n",
              "      <td>0.021717</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.340281</td>\n",
              "      <td>-0.383710</td>\n",
              "      <td>0.018406</td>\n",
              "      <td>-0.068126</td>\n",
              "      <td>0.040825</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.258700</td>\n",
              "      <td>-0.275310</td>\n",
              "      <td>-0.243297</td>\n",
              "      <td>-0.001360</td>\n",
              "      <td>-0.297851</td>\n",
              "      <td>-0.461247</td>\n",
              "      <td>0.124250</td>\n",
              "      <td>-0.287193</td>\n",
              "      <td>-0.639736</td>\n",
              "      <td>-0.299904</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.443234</td>\n",
              "      <td>-0.507748</td>\n",
              "      <td>-0.534443</td>\n",
              "      <td>0.021717</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.340281</td>\n",
              "      <td>-0.383710</td>\n",
              "      <td>0.018406</td>\n",
              "      <td>-0.068126</td>\n",
              "      <td>0.040825</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.264175</td>\n",
              "      <td>-0.659188</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.064623</td>\n",
              "      <td>-0.546783</td>\n",
              "      <td>-0.419669</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.578031</td>\n",
              "      <td>-0.638491</td>\n",
              "      <td>-0.385605</td>\n",
              "      <td>0.010835</td>\n",
              "      <td>-0.467941</td>\n",
              "      <td>-0.592624</td>\n",
              "      <td>-0.584690</td>\n",
              "      <td>-0.409170</td>\n",
              "      <td>0.038640</td>\n",
              "      <td>-0.366393</td>\n",
              "      <td>-0.475119</td>\n",
              "      <td>0.570027</td>\n",
              "      <td>-0.069066</td>\n",
              "      <td>0.291294</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>0.882990</td>\n",
              "      <td>0.697981</td>\n",
              "      <td>-0.331150</td>\n",
              "      <td>0.344123</td>\n",
              "      <td>0.259596</td>\n",
              "      <td>0.178184</td>\n",
              "      <td>0.062780</td>\n",
              "      <td>0.181810</td>\n",
              "      <td>0.177525</td>\n",
              "      <td>-0.029314</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.153694</td>\n",
              "      <td>0.150316</td>\n",
              "      <td>0.178835</td>\n",
              "      <td>0.079728</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.196666</td>\n",
              "      <td>-0.354216</td>\n",
              "      <td>0.758812</td>\n",
              "      <td>-0.064838</td>\n",
              "      <td>-0.005972</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>1.298761</td>\n",
              "      <td>-0.255913</td>\n",
              "      <td>-0.394401</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.608742</td>\n",
              "      <td>-0.574589</td>\n",
              "      <td>-0.257114</td>\n",
              "      <td>-0.375614</td>\n",
              "      <td>-0.257503</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.312464</td>\n",
              "      <td>-0.221116</td>\n",
              "      <td>-0.453916</td>\n",
              "      <td>-0.420056</td>\n",
              "      <td>-0.398514</td>\n",
              "      <td>-0.392390</td>\n",
              "      <td>-0.249123</td>\n",
              "      <td>-0.666679</td>\n",
              "      <td>-0.545227</td>\n",
              "      <td>-1.183305</td>\n",
              "      <td>-0.072072</td>\n",
              "      <td>0.408327</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.097083</td>\n",
              "      <td>0.325020</td>\n",
              "      <td>2.859758</td>\n",
              "      <td>-0.077456</td>\n",
              "      <td>-0.239459</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.122295</td>\n",
              "      <td>0.750823</td>\n",
              "      <td>0.632652</td>\n",
              "      <td>0.234122</td>\n",
              "      <td>-0.288826</td>\n",
              "      <td>0.419872</td>\n",
              "      <td>0.284057</td>\n",
              "      <td>3.409404</td>\n",
              "      <td>0.366563</td>\n",
              "      <td>0.554049</td>\n",
              "      <td>0.184936</td>\n",
              "      <td>-1.511974</td>\n",
              "      <td>-0.069688</td>\n",
              "      <td>-0.002248</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>-0.951061</td>\n",
              "      <td>0.371104</td>\n",
              "      <td>-0.289928</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.643577</td>\n",
              "      <td>0.116173</td>\n",
              "      <td>0.197469</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>0.826412</td>\n",
              "      <td>-0.465357</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.227702</td>\n",
              "      <td>0.337411</td>\n",
              "      <td>0.215723</td>\n",
              "      <td>0.163447</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>0.484356</td>\n",
              "      <td>0.769472</td>\n",
              "      <td>-0.451983</td>\n",
              "      <td>0.073614</td>\n",
              "      <td>-0.060851</td>\n",
              "      <td>0.002298</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.250258</td>\n",
              "      <td>-0.287087</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.383024</td>\n",
              "      <td>-0.300096</td>\n",
              "      <td>-0.402160</td>\n",
              "      <td>-0.348297</td>\n",
              "      <td>-0.265237</td>\n",
              "      <td>-0.570458</td>\n",
              "      <td>-0.418733</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.392784</td>\n",
              "      <td>-0.546315</td>\n",
              "      <td>-0.545812</td>\n",
              "      <td>-0.327513</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.333753</td>\n",
              "      <td>-0.370075</td>\n",
              "      <td>-0.397816</td>\n",
              "      <td>-0.057024</td>\n",
              "      <td>0.054227</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>-0.251754</td>\n",
              "      <td>-0.562970</td>\n",
              "      <td>-0.135949</td>\n",
              "      <td>-0.002033</td>\n",
              "      <td>-0.162143</td>\n",
              "      <td>-0.452040</td>\n",
              "      <td>-0.229535</td>\n",
              "      <td>-0.321368</td>\n",
              "      <td>-0.452420</td>\n",
              "      <td>-0.108418</td>\n",
              "      <td>0.041941</td>\n",
              "      <td>-0.439114</td>\n",
              "      <td>-0.331464</td>\n",
              "      <td>-0.308863</td>\n",
              "      <td>-0.283584</td>\n",
              "      <td>0.079540</td>\n",
              "      <td>-0.431673</td>\n",
              "      <td>-0.269778</td>\n",
              "      <td>-0.042078</td>\n",
              "      <td>-0.069133</td>\n",
              "      <td>-0.011423</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.246228</td>\n",
              "      <td>-0.323553</td>\n",
              "      <td>-0.097074</td>\n",
              "      <td>0.052903</td>\n",
              "      <td>-0.266219</td>\n",
              "      <td>-0.428197</td>\n",
              "      <td>-0.359781</td>\n",
              "      <td>-0.357763</td>\n",
              "      <td>-0.614259</td>\n",
              "      <td>-0.229285</td>\n",
              "      <td>-0.088679</td>\n",
              "      <td>-0.338624</td>\n",
              "      <td>-0.473436</td>\n",
              "      <td>-0.457563</td>\n",
              "      <td>-0.372913</td>\n",
              "      <td>-0.162903</td>\n",
              "      <td>-0.627512</td>\n",
              "      <td>-0.329921</td>\n",
              "      <td>0.160586</td>\n",
              "      <td>-0.070317</td>\n",
              "      <td>-0.009512</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.218046</td>\n",
              "      <td>-0.435144</td>\n",
              "      <td>0.523078</td>\n",
              "      <td>-0.236279</td>\n",
              "      <td>0.027907</td>\n",
              "      <td>-0.219537</td>\n",
              "      <td>-0.294712</td>\n",
              "      <td>-0.246645</td>\n",
              "      <td>0.158664</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.174897</td>\n",
              "      <td>-0.203061</td>\n",
              "      <td>-0.314766</td>\n",
              "      <td>-0.307543</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.365993</td>\n",
              "      <td>0.181955</td>\n",
              "      <td>0.248884</td>\n",
              "      <td>-1.339422</td>\n",
              "      <td>-0.068010</td>\n",
              "      <td>-0.039920</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>0.280814</td>\n",
              "      <td>-0.179285</td>\n",
              "      <td>-0.322986</td>\n",
              "      <td>-0.228813</td>\n",
              "      <td>-0.407856</td>\n",
              "      <td>-0.160718</td>\n",
              "      <td>-0.293527</td>\n",
              "      <td>-0.267016</td>\n",
              "      <td>-0.402025</td>\n",
              "      <td>-0.379760</td>\n",
              "      <td>-0.146385</td>\n",
              "      <td>-0.130702</td>\n",
              "      <td>-0.399469</td>\n",
              "      <td>-0.385180</td>\n",
              "      <td>-0.296559</td>\n",
              "      <td>-0.201933</td>\n",
              "      <td>-0.372921</td>\n",
              "      <td>-0.460852</td>\n",
              "      <td>0.125901</td>\n",
              "      <td>-0.059048</td>\n",
              "      <td>-0.038804</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>-0.276115</td>\n",
              "      <td>-0.263383</td>\n",
              "      <td>-0.414951</td>\n",
              "      <td>-0.152464</td>\n",
              "      <td>0.113848</td>\n",
              "      <td>-0.164931</td>\n",
              "      <td>-0.475693</td>\n",
              "      <td>-0.374712</td>\n",
              "      <td>-0.367590</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.260492</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.460981</td>\n",
              "      <td>-0.560557</td>\n",
              "      <td>-0.562830</td>\n",
              "      <td>-0.388510</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.477368</td>\n",
              "      <td>-0.235645</td>\n",
              "      <td>0.018168</td>\n",
              "      <td>-0.068245</td>\n",
              "      <td>-0.031647</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>-0.232584</td>\n",
              "      <td>-0.298732</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.126365</td>\n",
              "      <td>-0.448028</td>\n",
              "      <td>-0.355780</td>\n",
              "      <td>-0.343376</td>\n",
              "      <td>-0.357388</td>\n",
              "      <td>-0.637694</td>\n",
              "      <td>-0.472716</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.203249</td>\n",
              "      <td>-0.611930</td>\n",
              "      <td>-0.608831</td>\n",
              "      <td>-0.355047</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.568760</td>\n",
              "      <td>-0.464866</td>\n",
              "      <td>0.275390</td>\n",
              "      <td>-0.242996</td>\n",
              "      <td>4.153792</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>-0.255539</td>\n",
              "      <td>-0.483342</td>\n",
              "      <td>-0.259757</td>\n",
              "      <td>-0.093121</td>\n",
              "      <td>-0.287623</td>\n",
              "      <td>-0.427095</td>\n",
              "      <td>-0.349070</td>\n",
              "      <td>-0.379088</td>\n",
              "      <td>-0.614983</td>\n",
              "      <td>-0.094902</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.417581</td>\n",
              "      <td>-0.348902</td>\n",
              "      <td>-0.328202</td>\n",
              "      <td>-0.361127</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.503480</td>\n",
              "      <td>-0.329851</td>\n",
              "      <td>0.148004</td>\n",
              "      <td>-0.070572</td>\n",
              "      <td>0.027194</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>-1.850991</td>\n",
              "      <td>-0.187720</td>\n",
              "      <td>0.488945</td>\n",
              "      <td>-0.317987</td>\n",
              "      <td>-0.614185</td>\n",
              "      <td>-0.348426</td>\n",
              "      <td>-0.167470</td>\n",
              "      <td>-0.376639</td>\n",
              "      <td>-0.073555</td>\n",
              "      <td>-0.378134</td>\n",
              "      <td>-0.344823</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.422136</td>\n",
              "      <td>-0.308818</td>\n",
              "      <td>-0.278955</td>\n",
              "      <td>-0.401077</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.549176</td>\n",
              "      <td>-0.462386</td>\n",
              "      <td>-0.182147</td>\n",
              "      <td>-0.066510</td>\n",
              "      <td>0.064609</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>-0.838570</td>\n",
              "      <td>-0.106506</td>\n",
              "      <td>-0.020465</td>\n",
              "      <td>-0.266247</td>\n",
              "      <td>-0.014273</td>\n",
              "      <td>0.007815</td>\n",
              "      <td>0.126503</td>\n",
              "      <td>2.659140</td>\n",
              "      <td>-0.179904</td>\n",
              "      <td>-0.300488</td>\n",
              "      <td>-0.239630</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.332700</td>\n",
              "      <td>-0.380633</td>\n",
              "      <td>-0.547645</td>\n",
              "      <td>2.851344</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.072635</td>\n",
              "      <td>-0.306205</td>\n",
              "      <td>0.628775</td>\n",
              "      <td>-0.054820</td>\n",
              "      <td>-0.027654</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.182407</td>\n",
              "      <td>-0.120413</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.264097</td>\n",
              "      <td>-0.501310</td>\n",
              "      <td>-0.405082</td>\n",
              "      <td>-0.383847</td>\n",
              "      <td>-0.341872</td>\n",
              "      <td>-0.054208</td>\n",
              "      <td>-0.430506</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.390114</td>\n",
              "      <td>-0.268068</td>\n",
              "      <td>-0.233318</td>\n",
              "      <td>-0.398266</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.490424</td>\n",
              "      <td>-0.492245</td>\n",
              "      <td>-0.384453</td>\n",
              "      <td>-0.052355</td>\n",
              "      <td>0.023134</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.151613</td>\n",
              "      <td>-0.332370</td>\n",
              "      <td>-0.224561</td>\n",
              "      <td>-0.375909</td>\n",
              "      <td>-0.422251</td>\n",
              "      <td>-0.389334</td>\n",
              "      <td>-0.139886</td>\n",
              "      <td>-0.437862</td>\n",
              "      <td>-0.169492</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.422942</td>\n",
              "      <td>-0.222148</td>\n",
              "      <td>-0.183167</td>\n",
              "      <td>-0.404125</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.457785</td>\n",
              "      <td>-0.469206</td>\n",
              "      <td>0.032367</td>\n",
              "      <td>-0.069994</td>\n",
              "      <td>-0.015800</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.450795</td>\n",
              "      <td>-0.277490</td>\n",
              "      <td>-0.152243</td>\n",
              "      <td>-0.304880</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>0.017470</td>\n",
              "      <td>-0.356312</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.251327</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.484075</td>\n",
              "      <td>-0.492954</td>\n",
              "      <td>0.029251</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.718903</td>\n",
              "      <td>-0.353315</td>\n",
              "      <td>0.052126</td>\n",
              "      <td>-0.069228</td>\n",
              "      <td>0.136064</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.224014</td>\n",
              "      <td>-0.286028</td>\n",
              "      <td>-0.151263</td>\n",
              "      <td>-0.179840</td>\n",
              "      <td>-0.064316</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.425097</td>\n",
              "      <td>-0.331546</td>\n",
              "      <td>-0.217571</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.399482</td>\n",
              "      <td>-0.401541</td>\n",
              "      <td>-0.378298</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.075977</td>\n",
              "      <td>0.786942</td>\n",
              "      <td>-1.536081</td>\n",
              "      <td>-0.068279</td>\n",
              "      <td>0.006986</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.111725</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.041342</td>\n",
              "      <td>-0.261430</td>\n",
              "      <td>-0.117247</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.287399</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.339359</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.081853</td>\n",
              "      <td>-0.548415</td>\n",
              "      <td>-0.547279</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.490424</td>\n",
              "      <td>-0.370887</td>\n",
              "      <td>0.223990</td>\n",
              "      <td>-0.065609</td>\n",
              "      <td>0.037584</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>1.186270</td>\n",
              "      <td>-0.208545</td>\n",
              "      <td>0.194371</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.207619</td>\n",
              "      <td>-0.440242</td>\n",
              "      <td>0.225691</td>\n",
              "      <td>-0.345334</td>\n",
              "      <td>-0.286343</td>\n",
              "      <td>-0.079945</td>\n",
              "      <td>-0.064004</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.377563</td>\n",
              "      <td>-0.012045</td>\n",
              "      <td>0.043425</td>\n",
              "      <td>-0.394551</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.542648</td>\n",
              "      <td>-0.478470</td>\n",
              "      <td>0.273724</td>\n",
              "      <td>-0.069445</td>\n",
              "      <td>0.009398</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>-0.250467</td>\n",
              "      <td>-0.427030</td>\n",
              "      <td>-0.101319</td>\n",
              "      <td>-0.019928</td>\n",
              "      <td>0.064828</td>\n",
              "      <td>-0.433202</td>\n",
              "      <td>-0.197901</td>\n",
              "      <td>-0.375270</td>\n",
              "      <td>-0.418650</td>\n",
              "      <td>-0.309512</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.392734</td>\n",
              "      <td>-0.605798</td>\n",
              "      <td>-0.609667</td>\n",
              "      <td>-0.228494</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.594872</td>\n",
              "      <td>-0.024003</td>\n",
              "      <td>-0.129328</td>\n",
              "      <td>-0.066385</td>\n",
              "      <td>-0.037666</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.044083</td>\n",
              "      <td>0.195733</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.169675</td>\n",
              "      <td>-0.148024</td>\n",
              "      <td>0.145051</td>\n",
              "      <td>-0.392581</td>\n",
              "      <td>-0.076736</td>\n",
              "      <td>0.427125</td>\n",
              "      <td>-0.153066</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.152328</td>\n",
              "      <td>0.014477</td>\n",
              "      <td>0.039395</td>\n",
              "      <td>-0.087060</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.175427</td>\n",
              "      <td>-0.385367</td>\n",
              "      <td>0.029302</td>\n",
              "      <td>-0.061311</td>\n",
              "      <td>-0.034430</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.172243</td>\n",
              "      <td>-0.061917</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.033416</td>\n",
              "      <td>-0.368658</td>\n",
              "      <td>-0.327212</td>\n",
              "      <td>0.905823</td>\n",
              "      <td>-0.408552</td>\n",
              "      <td>-0.505143</td>\n",
              "      <td>-0.175255</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.381403</td>\n",
              "      <td>-0.459609</td>\n",
              "      <td>-0.470044</td>\n",
              "      <td>-0.118588</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.620984</td>\n",
              "      <td>-0.187483</td>\n",
              "      <td>-1.190629</td>\n",
              "      <td>-0.069046</td>\n",
              "      <td>-0.050156</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>0.007014</td>\n",
              "      <td>2.920838</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.085458</td>\n",
              "      <td>-0.033545</td>\n",
              "      <td>-0.381203</td>\n",
              "      <td>-0.091425</td>\n",
              "      <td>1.027168</td>\n",
              "      <td>1.502577</td>\n",
              "      <td>0.953546</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.271834</td>\n",
              "      <td>1.916621</td>\n",
              "      <td>2.102186</td>\n",
              "      <td>-0.066760</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.755193</td>\n",
              "      <td>-0.497993</td>\n",
              "      <td>0.733304</td>\n",
              "      <td>-0.069956</td>\n",
              "      <td>0.159790</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.246970</td>\n",
              "      <td>-0.463587</td>\n",
              "      <td>-0.329972</td>\n",
              "      <td>-0.233497</td>\n",
              "      <td>-0.479997</td>\n",
              "      <td>-0.472134</td>\n",
              "      <td>-0.313307</td>\n",
              "      <td>-0.325902</td>\n",
              "      <td>-0.573851</td>\n",
              "      <td>-0.382640</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.459688</td>\n",
              "      <td>-0.503628</td>\n",
              "      <td>-0.493294</td>\n",
              "      <td>-0.326089</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.594872</td>\n",
              "      <td>-0.496498</td>\n",
              "      <td>0.405680</td>\n",
              "      <td>-0.067604</td>\n",
              "      <td>-0.012003</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.452207</td>\n",
              "      <td>-0.256527</td>\n",
              "      <td>-0.115071</td>\n",
              "      <td>-0.216715</td>\n",
              "      <td>-0.426346</td>\n",
              "      <td>-0.377820</td>\n",
              "      <td>-0.218700</td>\n",
              "      <td>-0.542458</td>\n",
              "      <td>-0.033194</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.405427</td>\n",
              "      <td>-0.240103</td>\n",
              "      <td>-0.203057</td>\n",
              "      <td>-0.397084</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.555704</td>\n",
              "      <td>-0.377177</td>\n",
              "      <td>0.238201</td>\n",
              "      <td>-0.070345</td>\n",
              "      <td>0.042482</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.223353</td>\n",
              "      <td>0.068817</td>\n",
              "      <td>-0.305398</td>\n",
              "      <td>-0.385848</td>\n",
              "      <td>-0.413269</td>\n",
              "      <td>-0.393448</td>\n",
              "      <td>-0.357935</td>\n",
              "      <td>-0.072557</td>\n",
              "      <td>-0.110854</td>\n",
              "      <td>-0.507866</td>\n",
              "      <td>-0.174534</td>\n",
              "      <td>-0.379812</td>\n",
              "      <td>-0.288279</td>\n",
              "      <td>-0.258663</td>\n",
              "      <td>-0.370300</td>\n",
              "      <td>-0.214285</td>\n",
              "      <td>-0.418617</td>\n",
              "      <td>-0.491868</td>\n",
              "      <td>-0.136255</td>\n",
              "      <td>0.023553</td>\n",
              "      <td>1.275452</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.261049</td>\n",
              "      <td>-0.223124</td>\n",
              "      <td>-0.329995</td>\n",
              "      <td>-0.229214</td>\n",
              "      <td>-0.148175</td>\n",
              "      <td>-0.426428</td>\n",
              "      <td>-0.382740</td>\n",
              "      <td>-0.418125</td>\n",
              "      <td>-0.413971</td>\n",
              "      <td>-0.366833</td>\n",
              "      <td>-0.136327</td>\n",
              "      <td>-0.421430</td>\n",
              "      <td>-0.549154</td>\n",
              "      <td>-0.539003</td>\n",
              "      <td>-0.384473</td>\n",
              "      <td>-0.184118</td>\n",
              "      <td>-0.392505</td>\n",
              "      <td>-0.142783</td>\n",
              "      <td>-0.136210</td>\n",
              "      <td>-0.065854</td>\n",
              "      <td>-0.020633</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>-1.850991</td>\n",
              "      <td>0.063734</td>\n",
              "      <td>-0.491618</td>\n",
              "      <td>0.106601</td>\n",
              "      <td>0.285010</td>\n",
              "      <td>0.380528</td>\n",
              "      <td>0.495570</td>\n",
              "      <td>-0.253778</td>\n",
              "      <td>0.243071</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.197067</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.466252</td>\n",
              "      <td>0.367760</td>\n",
              "      <td>0.290498</td>\n",
              "      <td>-0.250958</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.638912</td>\n",
              "      <td>-0.304773</td>\n",
              "      <td>0.041511</td>\n",
              "      <td>-0.064538</td>\n",
              "      <td>0.032678</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>-1.850991</td>\n",
              "      <td>0.666788</td>\n",
              "      <td>-0.415228</td>\n",
              "      <td>-0.263320</td>\n",
              "      <td>-0.166946</td>\n",
              "      <td>-0.382361</td>\n",
              "      <td>-0.301945</td>\n",
              "      <td>-0.375781</td>\n",
              "      <td>-0.278086</td>\n",
              "      <td>-0.506355</td>\n",
              "      <td>-0.412052</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.285302</td>\n",
              "      <td>-0.482301</td>\n",
              "      <td>-0.483318</td>\n",
              "      <td>-0.281564</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.307642</td>\n",
              "      <td>-0.438989</td>\n",
              "      <td>0.007664</td>\n",
              "      <td>-0.046047</td>\n",
              "      <td>0.067227</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.818721</td>\n",
              "      <td>-0.154085</td>\n",
              "      <td>0.081219</td>\n",
              "      <td>-0.097311</td>\n",
              "      <td>-0.105673</td>\n",
              "      <td>-0.193969</td>\n",
              "      <td>1.007615</td>\n",
              "      <td>-0.321594</td>\n",
              "      <td>0.236941</td>\n",
              "      <td>0.481403</td>\n",
              "      <td>-0.284063</td>\n",
              "      <td>0.602136</td>\n",
              "      <td>0.667489</td>\n",
              "      <td>-0.221768</td>\n",
              "      <td>0.905793</td>\n",
              "      <td>0.410434</td>\n",
              "      <td>-0.485841</td>\n",
              "      <td>-0.371266</td>\n",
              "      <td>-0.066609</td>\n",
              "      <td>0.115192</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.242691</td>\n",
              "      <td>-0.118411</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.178742</td>\n",
              "      <td>-0.423756</td>\n",
              "      <td>-0.400405</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.402759</td>\n",
              "      <td>-0.239565</td>\n",
              "      <td>-0.281297</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.382692</td>\n",
              "      <td>-0.287205</td>\n",
              "      <td>-0.256292</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.425145</td>\n",
              "      <td>-0.427152</td>\n",
              "      <td>0.217670</td>\n",
              "      <td>-0.067753</td>\n",
              "      <td>0.033229</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.211926</td>\n",
              "      <td>-0.248851</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.710158</td>\n",
              "      <td>-0.280342</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.386400</td>\n",
              "      <td>-0.047893</td>\n",
              "      <td>-0.331164</td>\n",
              "      <td>-0.157914</td>\n",
              "      <td>-0.243326</td>\n",
              "      <td>-0.427902</td>\n",
              "      <td>-0.412782</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.115346</td>\n",
              "      <td>-0.869046</td>\n",
              "      <td>-0.123404</td>\n",
              "      <td>-1.095007</td>\n",
              "      <td>-0.067937</td>\n",
              "      <td>-0.067186</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>1.137289</td>\n",
              "      <td>0.537783</td>\n",
              "      <td>-0.005545</td>\n",
              "      <td>-0.174763</td>\n",
              "      <td>0.644135</td>\n",
              "      <td>0.869758</td>\n",
              "      <td>0.938322</td>\n",
              "      <td>0.316612</td>\n",
              "      <td>-0.083377</td>\n",
              "      <td>-0.371800</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.534930</td>\n",
              "      <td>-0.452386</td>\n",
              "      <td>-0.505149</td>\n",
              "      <td>0.768919</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.097092</td>\n",
              "      <td>-0.224262</td>\n",
              "      <td>-0.300922</td>\n",
              "      <td>-0.035063</td>\n",
              "      <td>0.050610</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>-1.288535</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.419969</td>\n",
              "      <td>0.022732</td>\n",
              "      <td>0.018779</td>\n",
              "      <td>0.510892</td>\n",
              "      <td>0.319701</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.152272</td>\n",
              "      <td>-0.147641</td>\n",
              "      <td>-0.078463</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.265709</td>\n",
              "      <td>-0.275880</td>\n",
              "      <td>-0.338584</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.488769</td>\n",
              "      <td>-0.148193</td>\n",
              "      <td>1.247366</td>\n",
              "      <td>-0.061772</td>\n",
              "      <td>0.024804</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.480935</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.189919</td>\n",
              "      <td>-0.387945</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.383801</td>\n",
              "      <td>-0.288896</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.470273</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.602850</td>\n",
              "      <td>-0.597088</td>\n",
              "      <td>-0.398217</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.581816</td>\n",
              "      <td>-0.431934</td>\n",
              "      <td>-0.178348</td>\n",
              "      <td>-0.046988</td>\n",
              "      <td>-0.000202</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>-0.726079</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.169021</td>\n",
              "      <td>-0.307251</td>\n",
              "      <td>-0.162992</td>\n",
              "      <td>-0.481192</td>\n",
              "      <td>-0.454079</td>\n",
              "      <td>-0.383882</td>\n",
              "      <td>-0.472498</td>\n",
              "      <td>-0.452197</td>\n",
              "      <td>-0.418029</td>\n",
              "      <td>-0.026101</td>\n",
              "      <td>-0.438014</td>\n",
              "      <td>-0.518354</td>\n",
              "      <td>-0.504779</td>\n",
              "      <td>-0.398303</td>\n",
              "      <td>0.011306</td>\n",
              "      <td>-0.346809</td>\n",
              "      <td>-0.428105</td>\n",
              "      <td>-0.254751</td>\n",
              "      <td>-0.058510</td>\n",
              "      <td>-0.033823</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>-0.140117</td>\n",
              "      <td>-0.189634</td>\n",
              "      <td>1.241994</td>\n",
              "      <td>0.222011</td>\n",
              "      <td>-0.024515</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.362074</td>\n",
              "      <td>0.036935</td>\n",
              "      <td>0.647283</td>\n",
              "      <td>-0.128951</td>\n",
              "      <td>0.297297</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.317466</td>\n",
              "      <td>-0.331937</td>\n",
              "      <td>0.330893</td>\n",
              "      <td>0.479765</td>\n",
              "      <td>-0.392505</td>\n",
              "      <td>0.624900</td>\n",
              "      <td>-1.673237</td>\n",
              "      <td>-0.067733</td>\n",
              "      <td>-0.028534</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>-0.263871</td>\n",
              "      <td>-0.548668</td>\n",
              "      <td>-0.330626</td>\n",
              "      <td>-0.231631</td>\n",
              "      <td>-0.440634</td>\n",
              "      <td>0.009673</td>\n",
              "      <td>0.210675</td>\n",
              "      <td>-0.453360</td>\n",
              "      <td>-0.636058</td>\n",
              "      <td>-0.382503</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.205479</td>\n",
              "      <td>-0.587692</td>\n",
              "      <td>-0.619070</td>\n",
              "      <td>0.237150</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.477368</td>\n",
              "      <td>-0.404658</td>\n",
              "      <td>-0.034336</td>\n",
              "      <td>-0.065367</td>\n",
              "      <td>-0.049215</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>-0.951061</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.547581</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.186906</td>\n",
              "      <td>-0.507248</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.394254</td>\n",
              "      <td>-0.225944</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.395935</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.480260</td>\n",
              "      <td>-0.465827</td>\n",
              "      <td>-0.409380</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.157499</td>\n",
              "      <td>-0.518765</td>\n",
              "      <td>-0.188275</td>\n",
              "      <td>-0.061281</td>\n",
              "      <td>-0.063723</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.139691</td>\n",
              "      <td>-0.121810</td>\n",
              "      <td>0.028933</td>\n",
              "      <td>0.104304</td>\n",
              "      <td>-0.393875</td>\n",
              "      <td>-0.223628</td>\n",
              "      <td>2.350483</td>\n",
              "      <td>-0.214970</td>\n",
              "      <td>-0.150726</td>\n",
              "      <td>-0.368058</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.129148</td>\n",
              "      <td>-0.345857</td>\n",
              "      <td>-0.382954</td>\n",
              "      <td>0.694640</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.327225</td>\n",
              "      <td>-0.395439</td>\n",
              "      <td>-1.028085</td>\n",
              "      <td>-0.053582</td>\n",
              "      <td>-0.101343</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.391716</td>\n",
              "      <td>-0.277529</td>\n",
              "      <td>-0.174103</td>\n",
              "      <td>-0.218114</td>\n",
              "      <td>-0.219704</td>\n",
              "      <td>-0.107771</td>\n",
              "      <td>-0.504826</td>\n",
              "      <td>-0.509536</td>\n",
              "      <td>-0.279399</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.367508</td>\n",
              "      <td>-0.551189</td>\n",
              "      <td>-0.541242</td>\n",
              "      <td>-0.383926</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.470841</td>\n",
              "      <td>-0.148205</td>\n",
              "      <td>0.895620</td>\n",
              "      <td>-0.068884</td>\n",
              "      <td>-0.003667</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>-2.300955</td>\n",
              "      <td>-0.262495</td>\n",
              "      <td>-0.512110</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.239160</td>\n",
              "      <td>-0.577522</td>\n",
              "      <td>-0.416055</td>\n",
              "      <td>-0.394363</td>\n",
              "      <td>-0.467436</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.437658</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.363122</td>\n",
              "      <td>-0.573468</td>\n",
              "      <td>-0.564826</td>\n",
              "      <td>-0.393732</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.666679</td>\n",
              "      <td>-0.547642</td>\n",
              "      <td>-0.073078</td>\n",
              "      <td>-0.066006</td>\n",
              "      <td>0.066311</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>-2.975902</td>\n",
              "      <td>-0.262495</td>\n",
              "      <td>-0.512110</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.239160</td>\n",
              "      <td>-0.577522</td>\n",
              "      <td>-0.416055</td>\n",
              "      <td>-0.394363</td>\n",
              "      <td>-0.467436</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.437658</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.363122</td>\n",
              "      <td>-0.573468</td>\n",
              "      <td>-0.564826</td>\n",
              "      <td>-0.393732</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.666679</td>\n",
              "      <td>-0.547642</td>\n",
              "      <td>-0.073078</td>\n",
              "      <td>-0.066006</td>\n",
              "      <td>0.066311</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>-0.207376</td>\n",
              "      <td>-0.367538</td>\n",
              "      <td>-0.184592</td>\n",
              "      <td>-0.164113</td>\n",
              "      <td>-0.229274</td>\n",
              "      <td>-0.433062</td>\n",
              "      <td>-0.387169</td>\n",
              "      <td>-0.516850</td>\n",
              "      <td>-0.293442</td>\n",
              "      <td>-0.246890</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.262359</td>\n",
              "      <td>-0.410402</td>\n",
              "      <td>-0.438749</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.327225</td>\n",
              "      <td>-0.031749</td>\n",
              "      <td>-0.140671</td>\n",
              "      <td>-0.066962</td>\n",
              "      <td>0.015200</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>-0.227294</td>\n",
              "      <td>-0.088857</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.206553</td>\n",
              "      <td>-0.386004</td>\n",
              "      <td>-0.399903</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>0.115341</td>\n",
              "      <td>-0.278631</td>\n",
              "      <td>-0.396702</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.382169</td>\n",
              "      <td>-0.203046</td>\n",
              "      <td>-0.169738</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.079163</td>\n",
              "      <td>-0.430446</td>\n",
              "      <td>-1.273218</td>\n",
              "      <td>-0.055434</td>\n",
              "      <td>0.133243</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.548939</td>\n",
              "      <td>-0.277432</td>\n",
              "      <td>-0.042041</td>\n",
              "      <td>-0.475019</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.317789</td>\n",
              "      <td>-0.511221</td>\n",
              "      <td>-0.616812</td>\n",
              "      <td>-0.385735</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.583635</td>\n",
              "      <td>-0.581623</td>\n",
              "      <td>-0.327723</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.542648</td>\n",
              "      <td>-0.413470</td>\n",
              "      <td>0.275554</td>\n",
              "      <td>-0.065647</td>\n",
              "      <td>-0.058046</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.342460</td>\n",
              "      <td>-0.079788</td>\n",
              "      <td>0.051115</td>\n",
              "      <td>-0.408200</td>\n",
              "      <td>-0.417376</td>\n",
              "      <td>-0.139155</td>\n",
              "      <td>-0.458774</td>\n",
              "      <td>-0.406402</td>\n",
              "      <td>0.035820</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.363114</td>\n",
              "      <td>-0.152472</td>\n",
              "      <td>-0.125997</td>\n",
              "      <td>-0.134857</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.209722</td>\n",
              "      <td>-0.379365</td>\n",
              "      <td>0.247184</td>\n",
              "      <td>-0.069204</td>\n",
              "      <td>-0.045262</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.153434</td>\n",
              "      <td>0.090132</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.539346</td>\n",
              "      <td>-0.135059</td>\n",
              "      <td>-0.261852</td>\n",
              "      <td>-0.043878</td>\n",
              "      <td>-0.264309</td>\n",
              "      <td>0.113683</td>\n",
              "      <td>-0.039118</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.248004</td>\n",
              "      <td>0.053952</td>\n",
              "      <td>0.069574</td>\n",
              "      <td>-0.079262</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.214595</td>\n",
              "      <td>-0.336033</td>\n",
              "      <td>0.801569</td>\n",
              "      <td>-0.064302</td>\n",
              "      <td>-0.011699</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.261517</td>\n",
              "      <td>-0.550027</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.131951</td>\n",
              "      <td>-0.488412</td>\n",
              "      <td>-0.483581</td>\n",
              "      <td>-0.275158</td>\n",
              "      <td>-0.551556</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.397602</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.471455</td>\n",
              "      <td>-0.606396</td>\n",
              "      <td>-0.606398</td>\n",
              "      <td>-0.294713</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.640567</td>\n",
              "      <td>-0.391967</td>\n",
              "      <td>0.252909</td>\n",
              "      <td>-0.067006</td>\n",
              "      <td>-0.016049</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.262295</td>\n",
              "      <td>-0.239262</td>\n",
              "      <td>-0.332625</td>\n",
              "      <td>-0.501236</td>\n",
              "      <td>-0.304866</td>\n",
              "      <td>-0.424166</td>\n",
              "      <td>-0.341506</td>\n",
              "      <td>-0.483145</td>\n",
              "      <td>-0.404074</td>\n",
              "      <td>-0.436893</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.394117</td>\n",
              "      <td>-0.551077</td>\n",
              "      <td>-0.543874</td>\n",
              "      <td>-0.354669</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.568760</td>\n",
              "      <td>-0.211680</td>\n",
              "      <td>-0.109041</td>\n",
              "      <td>-0.063751</td>\n",
              "      <td>0.114972</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>-1.738499</td>\n",
              "      <td>-0.157121</td>\n",
              "      <td>0.189625</td>\n",
              "      <td>-0.112252</td>\n",
              "      <td>-0.486282</td>\n",
              "      <td>-0.062500</td>\n",
              "      <td>-0.128470</td>\n",
              "      <td>-0.221294</td>\n",
              "      <td>-0.456840</td>\n",
              "      <td>0.138813</td>\n",
              "      <td>-0.239424</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.160411</td>\n",
              "      <td>-0.232132</td>\n",
              "      <td>-0.281022</td>\n",
              "      <td>-0.224675</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.018756</td>\n",
              "      <td>-0.030095</td>\n",
              "      <td>0.206907</td>\n",
              "      <td>-0.062984</td>\n",
              "      <td>0.005384</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>0.434942</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.109322</td>\n",
              "      <td>-0.451762</td>\n",
              "      <td>-0.376303</td>\n",
              "      <td>-0.035302</td>\n",
              "      <td>-0.523422</td>\n",
              "      <td>-0.503326</td>\n",
              "      <td>-0.431466</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.360929</td>\n",
              "      <td>-0.580864</td>\n",
              "      <td>-0.594599</td>\n",
              "      <td>-0.026054</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.457785</td>\n",
              "      <td>-0.347043</td>\n",
              "      <td>-0.170447</td>\n",
              "      <td>-0.049054</td>\n",
              "      <td>-0.086447</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.264409</td>\n",
              "      <td>0.063228</td>\n",
              "      <td>-0.330480</td>\n",
              "      <td>-0.046898</td>\n",
              "      <td>-0.412164</td>\n",
              "      <td>-0.449790</td>\n",
              "      <td>0.088154</td>\n",
              "      <td>-0.545688</td>\n",
              "      <td>-0.138878</td>\n",
              "      <td>-0.128537</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.432442</td>\n",
              "      <td>-0.174850</td>\n",
              "      <td>-0.193002</td>\n",
              "      <td>0.105785</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.405561</td>\n",
              "      <td>-0.264007</td>\n",
              "      <td>0.036557</td>\n",
              "      <td>-0.070546</td>\n",
              "      <td>-0.009998</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.143387</td>\n",
              "      <td>0.369543</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.096053</td>\n",
              "      <td>-0.472190</td>\n",
              "      <td>0.068306</td>\n",
              "      <td>-0.114085</td>\n",
              "      <td>-0.189836</td>\n",
              "      <td>-0.075620</td>\n",
              "      <td>-0.368849</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.096097</td>\n",
              "      <td>-0.184339</td>\n",
              "      <td>-0.171652</td>\n",
              "      <td>0.094446</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.392505</td>\n",
              "      <td>-0.507965</td>\n",
              "      <td>0.046636</td>\n",
              "      <td>-0.056609</td>\n",
              "      <td>0.055117</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.254780</td>\n",
              "      <td>0.205964</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.177838</td>\n",
              "      <td>-0.372395</td>\n",
              "      <td>-0.408467</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.265293</td>\n",
              "      <td>-0.011795</td>\n",
              "      <td>-0.987218</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.425526</td>\n",
              "      <td>-0.704369</td>\n",
              "      <td>-0.707179</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.555704</td>\n",
              "      <td>-0.424262</td>\n",
              "      <td>-0.296179</td>\n",
              "      <td>-0.075110</td>\n",
              "      <td>0.057989</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.421095</td>\n",
              "      <td>-0.015811</td>\n",
              "      <td>-0.056057</td>\n",
              "      <td>-0.407442</td>\n",
              "      <td>-0.358575</td>\n",
              "      <td>0.281457</td>\n",
              "      <td>-0.512892</td>\n",
              "      <td>-0.509536</td>\n",
              "      <td>-0.171270</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.354969</td>\n",
              "      <td>-0.378896</td>\n",
              "      <td>-0.395553</td>\n",
              "      <td>0.312213</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.275002</td>\n",
              "      <td>-0.314674</td>\n",
              "      <td>0.095424</td>\n",
              "      <td>-0.068608</td>\n",
              "      <td>-0.031261</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>-2.413446</td>\n",
              "      <td>1.385676</td>\n",
              "      <td>0.674407</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.194276</td>\n",
              "      <td>-0.219772</td>\n",
              "      <td>0.029816</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>0.228118</td>\n",
              "      <td>0.447381</td>\n",
              "      <td>-0.224638</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.021546</td>\n",
              "      <td>0.210307</td>\n",
              "      <td>0.282931</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.488769</td>\n",
              "      <td>-0.454971</td>\n",
              "      <td>-0.314102</td>\n",
              "      <td>-0.057660</td>\n",
              "      <td>-0.003519</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>-1.850991</td>\n",
              "      <td>-0.087548</td>\n",
              "      <td>0.074880</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.022675</td>\n",
              "      <td>0.277109</td>\n",
              "      <td>0.357047</td>\n",
              "      <td>-0.393110</td>\n",
              "      <td>1.207769</td>\n",
              "      <td>-0.235982</td>\n",
              "      <td>-0.571930</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.683829</td>\n",
              "      <td>-0.188395</td>\n",
              "      <td>-0.180460</td>\n",
              "      <td>-0.403951</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.079163</td>\n",
              "      <td>-0.446780</td>\n",
              "      <td>-0.156499</td>\n",
              "      <td>-0.101318</td>\n",
              "      <td>0.065701</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>-0.229704</td>\n",
              "      <td>-0.323595</td>\n",
              "      <td>-0.239303</td>\n",
              "      <td>-0.129640</td>\n",
              "      <td>-0.430092</td>\n",
              "      <td>-0.465924</td>\n",
              "      <td>-0.393374</td>\n",
              "      <td>-0.556474</td>\n",
              "      <td>-0.345400</td>\n",
              "      <td>-0.326291</td>\n",
              "      <td>-0.024696</td>\n",
              "      <td>-0.450280</td>\n",
              "      <td>-0.447692</td>\n",
              "      <td>-0.433864</td>\n",
              "      <td>-0.408439</td>\n",
              "      <td>-0.009519</td>\n",
              "      <td>-0.425145</td>\n",
              "      <td>-0.167150</td>\n",
              "      <td>-0.764639</td>\n",
              "      <td>-0.065683</td>\n",
              "      <td>-0.028411</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.253549</td>\n",
              "      <td>-0.446677</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.084542</td>\n",
              "      <td>-0.430518</td>\n",
              "      <td>-0.476738</td>\n",
              "      <td>-0.357017</td>\n",
              "      <td>-0.539625</td>\n",
              "      <td>-0.521481</td>\n",
              "      <td>-0.403736</td>\n",
              "      <td>-0.140964</td>\n",
              "      <td>-0.466674</td>\n",
              "      <td>-0.593287</td>\n",
              "      <td>-0.589155</td>\n",
              "      <td>-0.378278</td>\n",
              "      <td>-0.219265</td>\n",
              "      <td>-0.764599</td>\n",
              "      <td>-0.267995</td>\n",
              "      <td>-0.384059</td>\n",
              "      <td>-0.066069</td>\n",
              "      <td>-0.043193</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.253157</td>\n",
              "      <td>-0.569685</td>\n",
              "      <td>-0.295318</td>\n",
              "      <td>-0.161619</td>\n",
              "      <td>-0.484505</td>\n",
              "      <td>-0.472083</td>\n",
              "      <td>-0.392493</td>\n",
              "      <td>-0.575535</td>\n",
              "      <td>-0.615902</td>\n",
              "      <td>-0.418772</td>\n",
              "      <td>-0.134488</td>\n",
              "      <td>-0.454584</td>\n",
              "      <td>-0.625239</td>\n",
              "      <td>-0.625724</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.195915</td>\n",
              "      <td>-0.686263</td>\n",
              "      <td>-0.338593</td>\n",
              "      <td>0.030980</td>\n",
              "      <td>-0.068232</td>\n",
              "      <td>-0.056308</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>-0.163623</td>\n",
              "      <td>-0.201112</td>\n",
              "      <td>-0.277338</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.151178</td>\n",
              "      <td>-0.212457</td>\n",
              "      <td>-0.442360</td>\n",
              "      <td>-0.389117</td>\n",
              "      <td>-0.408263</td>\n",
              "      <td>-0.313416</td>\n",
              "      <td>-0.415769</td>\n",
              "      <td>-0.134926</td>\n",
              "      <td>-0.434356</td>\n",
              "      <td>-0.506517</td>\n",
              "      <td>-0.491299</td>\n",
              "      <td>-0.409149</td>\n",
              "      <td>-0.176514</td>\n",
              "      <td>-0.503480</td>\n",
              "      <td>-0.193423</td>\n",
              "      <td>-0.330811</td>\n",
              "      <td>-0.057175</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>1.072698</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>1.128974</td>\n",
              "      <td>-0.217762</td>\n",
              "      <td>-0.451159</td>\n",
              "      <td>-0.360106</td>\n",
              "      <td>-0.296089</td>\n",
              "      <td>-0.172526</td>\n",
              "      <td>-0.082112</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.435994</td>\n",
              "      <td>-0.256860</td>\n",
              "      <td>-0.223040</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.177082</td>\n",
              "      <td>-0.341628</td>\n",
              "      <td>0.255335</td>\n",
              "      <td>-0.010590</td>\n",
              "      <td>-9.108514</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>-0.164119</td>\n",
              "      <td>0.255388</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.286670</td>\n",
              "      <td>-0.358855</td>\n",
              "      <td>-0.271533</td>\n",
              "      <td>-0.333535</td>\n",
              "      <td>-0.611337</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>-0.231060</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.241859</td>\n",
              "      <td>-0.153859</td>\n",
              "      <td>-0.121266</td>\n",
              "      <td>-0.364505</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.314170</td>\n",
              "      <td>-0.139492</td>\n",
              "      <td>6.421634</td>\n",
              "      <td>-0.067631</td>\n",
              "      <td>0.130687</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>-0.261669</td>\n",
              "      <td>-0.469152</td>\n",
              "      <td>-0.303170</td>\n",
              "      <td>-0.182032</td>\n",
              "      <td>-0.521523</td>\n",
              "      <td>-0.439161</td>\n",
              "      <td>-0.374868</td>\n",
              "      <td>-0.604427</td>\n",
              "      <td>-0.507025</td>\n",
              "      <td>-0.338649</td>\n",
              "      <td>-0.140549</td>\n",
              "      <td>-0.417684</td>\n",
              "      <td>-0.500092</td>\n",
              "      <td>-0.485541</td>\n",
              "      <td>-0.388677</td>\n",
              "      <td>-0.202056</td>\n",
              "      <td>-0.653623</td>\n",
              "      <td>-0.206720</td>\n",
              "      <td>-1.134761</td>\n",
              "      <td>-0.068909</td>\n",
              "      <td>0.023629</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.448287</td>\n",
              "      <td>-0.331080</td>\n",
              "      <td>-0.475491</td>\n",
              "      <td>-0.558990</td>\n",
              "      <td>-0.276518</td>\n",
              "      <td>-0.355144</td>\n",
              "      <td>-0.535436</td>\n",
              "      <td>-0.127316</td>\n",
              "      <td>-0.380083</td>\n",
              "      <td>-0.140493</td>\n",
              "      <td>-0.244829</td>\n",
              "      <td>-0.541300</td>\n",
              "      <td>-0.531244</td>\n",
              "      <td>-0.378932</td>\n",
              "      <td>-0.136669</td>\n",
              "      <td>-0.575288</td>\n",
              "      <td>-0.390824</td>\n",
              "      <td>-1.470388</td>\n",
              "      <td>-0.068226</td>\n",
              "      <td>-0.035940</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>1.298761</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.033035</td>\n",
              "      <td>-0.270069</td>\n",
              "      <td>-0.037508</td>\n",
              "      <td>-0.320924</td>\n",
              "      <td>-0.005722</td>\n",
              "      <td>-0.278819</td>\n",
              "      <td>-0.561207</td>\n",
              "      <td>-0.343966</td>\n",
              "      <td>-0.191864</td>\n",
              "      <td>-0.124720</td>\n",
              "      <td>-0.159558</td>\n",
              "      <td>-0.349913</td>\n",
              "      <td>-0.328350</td>\n",
              "      <td>-0.305023</td>\n",
              "      <td>-0.206310</td>\n",
              "      <td>-0.320697</td>\n",
              "      <td>-0.200102</td>\n",
              "      <td>2.505661</td>\n",
              "      <td>-0.069182</td>\n",
              "      <td>0.151399</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>-0.238967</td>\n",
              "      <td>-0.288492</td>\n",
              "      <td>-0.296952</td>\n",
              "      <td>-0.112375</td>\n",
              "      <td>-0.367904</td>\n",
              "      <td>-0.211178</td>\n",
              "      <td>-0.370876</td>\n",
              "      <td>-0.571637</td>\n",
              "      <td>-0.322017</td>\n",
              "      <td>-0.277008</td>\n",
              "      <td>-0.080939</td>\n",
              "      <td>-0.277063</td>\n",
              "      <td>-0.411664</td>\n",
              "      <td>-0.389995</td>\n",
              "      <td>-0.382522</td>\n",
              "      <td>-0.085346</td>\n",
              "      <td>-0.275002</td>\n",
              "      <td>-0.086720</td>\n",
              "      <td>-0.181245</td>\n",
              "      <td>-0.067622</td>\n",
              "      <td>-0.019438</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.069751</td>\n",
              "      <td>-0.254329</td>\n",
              "      <td>-0.131281</td>\n",
              "      <td>-0.419945</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.371861</td>\n",
              "      <td>-0.562039</td>\n",
              "      <td>-0.252268</td>\n",
              "      <td>-0.294669</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.460623</td>\n",
              "      <td>-0.462889</td>\n",
              "      <td>-0.445380</td>\n",
              "      <td>-0.385466</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.385977</td>\n",
              "      <td>-0.332493</td>\n",
              "      <td>2.595451</td>\n",
              "      <td>-0.064010</td>\n",
              "      <td>-0.520294</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.452080</td>\n",
              "      <td>-0.288103</td>\n",
              "      <td>0.556651</td>\n",
              "      <td>-0.267549</td>\n",
              "      <td>-0.387314</td>\n",
              "      <td>-0.355970</td>\n",
              "      <td>-0.614554</td>\n",
              "      <td>-0.433943</td>\n",
              "      <td>-0.183552</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.353779</td>\n",
              "      <td>-0.454281</td>\n",
              "      <td>-0.446147</td>\n",
              "      <td>-0.388623</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.568760</td>\n",
              "      <td>0.245472</td>\n",
              "      <td>1.022380</td>\n",
              "      <td>-0.069424</td>\n",
              "      <td>-0.123305</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>1.351978</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.234375</td>\n",
              "      <td>-0.213372</td>\n",
              "      <td>0.116361</td>\n",
              "      <td>-0.363644</td>\n",
              "      <td>-0.520466</td>\n",
              "      <td>-0.020485</td>\n",
              "      <td>-0.323259</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.144133</td>\n",
              "      <td>-0.351941</td>\n",
              "      <td>-0.331044</td>\n",
              "      <td>-0.373638</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.438201</td>\n",
              "      <td>-0.081505</td>\n",
              "      <td>0.565057</td>\n",
              "      <td>-0.065871</td>\n",
              "      <td>0.007701</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.206104</td>\n",
              "      <td>0.098874</td>\n",
              "      <td>0.068507</td>\n",
              "      <td>0.310333</td>\n",
              "      <td>0.257691</td>\n",
              "      <td>-0.396709</td>\n",
              "      <td>0.127990</td>\n",
              "      <td>-0.581004</td>\n",
              "      <td>0.124899</td>\n",
              "      <td>-0.170292</td>\n",
              "      <td>-0.207785</td>\n",
              "      <td>-0.328190</td>\n",
              "      <td>-0.384214</td>\n",
              "      <td>-0.407009</td>\n",
              "      <td>-0.138893</td>\n",
              "      <td>-0.204446</td>\n",
              "      <td>-0.503480</td>\n",
              "      <td>0.686920</td>\n",
              "      <td>3.588337</td>\n",
              "      <td>-0.068538</td>\n",
              "      <td>0.077758</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.160332</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.114937</td>\n",
              "      <td>-0.219067</td>\n",
              "      <td>-0.458633</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.414703</td>\n",
              "      <td>-0.110553</td>\n",
              "      <td>-0.257608</td>\n",
              "      <td>0.152859</td>\n",
              "      <td>-0.280556</td>\n",
              "      <td>-0.283320</td>\n",
              "      <td>-0.249247</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>0.326728</td>\n",
              "      <td>-0.457785</td>\n",
              "      <td>-0.239264</td>\n",
              "      <td>0.138034</td>\n",
              "      <td>-0.068730</td>\n",
              "      <td>0.075662</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.210288</td>\n",
              "      <td>0.217537</td>\n",
              "      <td>-0.261437</td>\n",
              "      <td>0.028739</td>\n",
              "      <td>-0.098397</td>\n",
              "      <td>-0.458390</td>\n",
              "      <td>-0.296363</td>\n",
              "      <td>-0.465628</td>\n",
              "      <td>0.340138</td>\n",
              "      <td>-0.027724</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.463528</td>\n",
              "      <td>0.070167</td>\n",
              "      <td>0.110159</td>\n",
              "      <td>-0.304841</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.255418</td>\n",
              "      <td>-0.094261</td>\n",
              "      <td>0.708003</td>\n",
              "      <td>-0.070082</td>\n",
              "      <td>0.089228</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.265906</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.163669</td>\n",
              "      <td>-0.127712</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>0.189408</td>\n",
              "      <td>-0.268626</td>\n",
              "      <td>1.682420</td>\n",
              "      <td>-0.267769</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>0.867819</td>\n",
              "      <td>0.647536</td>\n",
              "      <td>0.773017</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.507131</td>\n",
              "      <td>-0.362954</td>\n",
              "      <td>2.224427</td>\n",
              "      <td>-0.044775</td>\n",
              "      <td>-0.038893</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>1.298761</td>\n",
              "      <td>-0.217395</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.013750</td>\n",
              "      <td>-0.478444</td>\n",
              "      <td>-0.166061</td>\n",
              "      <td>-0.384765</td>\n",
              "      <td>-0.417699</td>\n",
              "      <td>-0.038568</td>\n",
              "      <td>-0.236033</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.148937</td>\n",
              "      <td>-0.308260</td>\n",
              "      <td>-0.276573</td>\n",
              "      <td>-0.404440</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.686263</td>\n",
              "      <td>-0.462386</td>\n",
              "      <td>0.031472</td>\n",
              "      <td>-0.070768</td>\n",
              "      <td>-0.078119</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>1.186270</td>\n",
              "      <td>-0.213459</td>\n",
              "      <td>-0.287926</td>\n",
              "      <td>-0.325614</td>\n",
              "      <td>-0.229777</td>\n",
              "      <td>-0.522542</td>\n",
              "      <td>-0.298874</td>\n",
              "      <td>-0.375852</td>\n",
              "      <td>-0.450645</td>\n",
              "      <td>-0.231324</td>\n",
              "      <td>-0.189777</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.282776</td>\n",
              "      <td>-0.201721</td>\n",
              "      <td>-0.161738</td>\n",
              "      <td>-0.391829</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.477368</td>\n",
              "      <td>-0.478412</td>\n",
              "      <td>-0.506024</td>\n",
              "      <td>-0.070412</td>\n",
              "      <td>0.085196</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.354209</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-1.469729</td>\n",
              "      <td>-0.571740</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.365026</td>\n",
              "      <td>-0.670031</td>\n",
              "      <td>-0.366189</td>\n",
              "      <td>-0.321164</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.370076</td>\n",
              "      <td>-0.364995</td>\n",
              "      <td>-0.341395</td>\n",
              "      <td>-0.378167</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.895158</td>\n",
              "      <td>0.147759</td>\n",
              "      <td>-1.518663</td>\n",
              "      <td>-0.070382</td>\n",
              "      <td>0.099442</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>-0.276115</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.191190</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.232442</td>\n",
              "      <td>-0.241160</td>\n",
              "      <td>-0.371364</td>\n",
              "      <td>-0.387210</td>\n",
              "      <td>-0.569712</td>\n",
              "      <td>-0.088266</td>\n",
              "      <td>-0.172371</td>\n",
              "      <td>-0.016325</td>\n",
              "      <td>-0.352675</td>\n",
              "      <td>-0.296748</td>\n",
              "      <td>-0.298705</td>\n",
              "      <td>-0.395552</td>\n",
              "      <td>0.028107</td>\n",
              "      <td>0.018756</td>\n",
              "      <td>0.175281</td>\n",
              "      <td>-0.209665</td>\n",
              "      <td>-0.067377</td>\n",
              "      <td>-0.044319</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>-0.212706</td>\n",
              "      <td>-0.484874</td>\n",
              "      <td>-0.293479</td>\n",
              "      <td>-0.178470</td>\n",
              "      <td>-0.374127</td>\n",
              "      <td>-0.141581</td>\n",
              "      <td>-0.282386</td>\n",
              "      <td>-0.570752</td>\n",
              "      <td>-0.513143</td>\n",
              "      <td>-0.364870</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.266510</td>\n",
              "      <td>-0.573846</td>\n",
              "      <td>-0.570663</td>\n",
              "      <td>-0.302001</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.562232</td>\n",
              "      <td>-0.247744</td>\n",
              "      <td>2.426522</td>\n",
              "      <td>-0.067033</td>\n",
              "      <td>-0.016065</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>-0.230302</td>\n",
              "      <td>-0.590690</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.245154</td>\n",
              "      <td>-0.405415</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.392581</td>\n",
              "      <td>-0.642877</td>\n",
              "      <td>-0.598426</td>\n",
              "      <td>-0.409281</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.466689</td>\n",
              "      <td>-0.660383</td>\n",
              "      <td>-0.658276</td>\n",
              "      <td>-0.407593</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.803766</td>\n",
              "      <td>0.393401</td>\n",
              "      <td>-0.399438</td>\n",
              "      <td>-0.068068</td>\n",
              "      <td>-0.153126</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.261188</td>\n",
              "      <td>-0.627753</td>\n",
              "      <td>-0.283627</td>\n",
              "      <td>-0.407303</td>\n",
              "      <td>-0.500558</td>\n",
              "      <td>-0.438795</td>\n",
              "      <td>-0.377495</td>\n",
              "      <td>-0.613756</td>\n",
              "      <td>-0.178554</td>\n",
              "      <td>-0.407600</td>\n",
              "      <td>-0.093333</td>\n",
              "      <td>-0.428869</td>\n",
              "      <td>-0.417360</td>\n",
              "      <td>-0.396045</td>\n",
              "      <td>-0.394635</td>\n",
              "      <td>-0.107192</td>\n",
              "      <td>-0.451257</td>\n",
              "      <td>-0.331092</td>\n",
              "      <td>0.997808</td>\n",
              "      <td>-0.062432</td>\n",
              "      <td>0.094597</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.257342</td>\n",
              "      <td>-0.495542</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.167225</td>\n",
              "      <td>-0.512643</td>\n",
              "      <td>-0.448797</td>\n",
              "      <td>-0.382248</td>\n",
              "      <td>-0.486930</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>-0.414317</td>\n",
              "      <td>-0.120592</td>\n",
              "      <td>-0.434501</td>\n",
              "      <td>-0.590412</td>\n",
              "      <td>-0.582509</td>\n",
              "      <td>-0.405491</td>\n",
              "      <td>-0.170175</td>\n",
              "      <td>-0.549176</td>\n",
              "      <td>-0.479970</td>\n",
              "      <td>0.614238</td>\n",
              "      <td>-0.061359</td>\n",
              "      <td>-0.036261</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>-2.750920</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.629029</td>\n",
              "      <td>-0.302202</td>\n",
              "      <td>-0.241870</td>\n",
              "      <td>-0.443259</td>\n",
              "      <td>-0.250596</td>\n",
              "      <td>-0.368188</td>\n",
              "      <td>-0.592919</td>\n",
              "      <td>-0.637212</td>\n",
              "      <td>-0.416495</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.168030</td>\n",
              "      <td>-0.658774</td>\n",
              "      <td>-0.663328</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.418617</td>\n",
              "      <td>-0.196336</td>\n",
              "      <td>-0.045196</td>\n",
              "      <td>-0.053186</td>\n",
              "      <td>-0.003170</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.173967</td>\n",
              "      <td>-0.653995</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.233908</td>\n",
              "      <td>-0.368696</td>\n",
              "      <td>-0.405431</td>\n",
              "      <td>-0.348493</td>\n",
              "      <td>-0.597802</td>\n",
              "      <td>-0.521994</td>\n",
              "      <td>-0.267874</td>\n",
              "      <td>-0.002773</td>\n",
              "      <td>-0.451777</td>\n",
              "      <td>-0.513775</td>\n",
              "      <td>-0.503297</td>\n",
              "      <td>-0.400843</td>\n",
              "      <td>0.052319</td>\n",
              "      <td>-0.418617</td>\n",
              "      <td>0.026205</td>\n",
              "      <td>-0.143200</td>\n",
              "      <td>-0.067481</td>\n",
              "      <td>-0.027552</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.190396</td>\n",
              "      <td>-0.307175</td>\n",
              "      <td>-0.320226</td>\n",
              "      <td>-0.213453</td>\n",
              "      <td>-0.506987</td>\n",
              "      <td>-0.292999</td>\n",
              "      <td>-0.384709</td>\n",
              "      <td>-0.625715</td>\n",
              "      <td>-0.285122</td>\n",
              "      <td>0.021416</td>\n",
              "      <td>-0.147770</td>\n",
              "      <td>-0.366364</td>\n",
              "      <td>-0.141261</td>\n",
              "      <td>-0.096094</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.207793</td>\n",
              "      <td>-0.379449</td>\n",
              "      <td>-0.053748</td>\n",
              "      <td>-1.061363</td>\n",
              "      <td>-0.070971</td>\n",
              "      <td>0.001353</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>-0.255907</td>\n",
              "      <td>-0.138128</td>\n",
              "      <td>-0.123042</td>\n",
              "      <td>-0.075041</td>\n",
              "      <td>-0.408222</td>\n",
              "      <td>-0.415640</td>\n",
              "      <td>-0.292842</td>\n",
              "      <td>-0.575987</td>\n",
              "      <td>-0.139861</td>\n",
              "      <td>-0.239577</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.366711</td>\n",
              "      <td>-0.345497</td>\n",
              "      <td>-0.310540</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.399033</td>\n",
              "      <td>0.091352</td>\n",
              "      <td>-1.080945</td>\n",
              "      <td>-0.068120</td>\n",
              "      <td>-0.040368</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.208378</td>\n",
              "      <td>-0.438012</td>\n",
              "      <td>-0.303573</td>\n",
              "      <td>-0.197820</td>\n",
              "      <td>-0.451962</td>\n",
              "      <td>-0.419428</td>\n",
              "      <td>-0.380496</td>\n",
              "      <td>-0.682626</td>\n",
              "      <td>-0.443835</td>\n",
              "      <td>-0.189890</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.388099</td>\n",
              "      <td>-0.412021</td>\n",
              "      <td>-0.389663</td>\n",
              "      <td>-0.394687</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.758071</td>\n",
              "      <td>0.966840</td>\n",
              "      <td>1.610789</td>\n",
              "      <td>-0.071460</td>\n",
              "      <td>0.013464</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.662671</td>\n",
              "      <td>-0.260427</td>\n",
              "      <td>-0.120927</td>\n",
              "      <td>-0.369169</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.058720</td>\n",
              "      <td>-0.669663</td>\n",
              "      <td>-0.416336</td>\n",
              "      <td>-0.292979</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.508727</td>\n",
              "      <td>-0.514880</td>\n",
              "      <td>-0.051062</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.836406</td>\n",
              "      <td>1.303784</td>\n",
              "      <td>-0.045248</td>\n",
              "      <td>-0.071208</td>\n",
              "      <td>-0.067427</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.260525</td>\n",
              "      <td>-0.389326</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.213267</td>\n",
              "      <td>-0.544263</td>\n",
              "      <td>-0.123730</td>\n",
              "      <td>-0.375852</td>\n",
              "      <td>-0.635771</td>\n",
              "      <td>-0.453911</td>\n",
              "      <td>-0.459165</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.228966</td>\n",
              "      <td>-0.582329</td>\n",
              "      <td>-0.576966</td>\n",
              "      <td>-0.389727</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.542648</td>\n",
              "      <td>-0.406772</td>\n",
              "      <td>1.673247</td>\n",
              "      <td>-0.058417</td>\n",
              "      <td>-0.036635</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>1.411252</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.389795</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-1.526884</td>\n",
              "      <td>-0.575812</td>\n",
              "      <td>-0.452073</td>\n",
              "      <td>-0.366601</td>\n",
              "      <td>-0.600346</td>\n",
              "      <td>0.152297</td>\n",
              "      <td>-0.539563</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.549957</td>\n",
              "      <td>-0.538381</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.438201</td>\n",
              "      <td>-0.171652</td>\n",
              "      <td>-1.697962</td>\n",
              "      <td>-0.076339</td>\n",
              "      <td>0.408327</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.257292</td>\n",
              "      <td>-0.183139</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.121434</td>\n",
              "      <td>-0.383148</td>\n",
              "      <td>-0.473110</td>\n",
              "      <td>-0.389629</td>\n",
              "      <td>-0.686819</td>\n",
              "      <td>-0.334280</td>\n",
              "      <td>-0.296225</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.454341</td>\n",
              "      <td>-0.471980</td>\n",
              "      <td>-0.454087</td>\n",
              "      <td>-0.404966</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.601400</td>\n",
              "      <td>4.105155</td>\n",
              "      <td>-0.637867</td>\n",
              "      <td>-0.069647</td>\n",
              "      <td>0.005493</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.544127</td>\n",
              "      <td>-0.317669</td>\n",
              "      <td>-0.206046</td>\n",
              "      <td>-0.567063</td>\n",
              "      <td>-0.430083</td>\n",
              "      <td>-0.374868</td>\n",
              "      <td>-0.686624</td>\n",
              "      <td>-0.541789</td>\n",
              "      <td>-0.338832</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.406122</td>\n",
              "      <td>-0.541499</td>\n",
              "      <td>-0.529830</td>\n",
              "      <td>-0.399186</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.888630</td>\n",
              "      <td>-0.009730</td>\n",
              "      <td>-0.163830</td>\n",
              "      <td>-0.070329</td>\n",
              "      <td>0.014024</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>-1.850991</td>\n",
              "      <td>-0.049979</td>\n",
              "      <td>-0.577207</td>\n",
              "      <td>1.857285</td>\n",
              "      <td>1.435703</td>\n",
              "      <td>0.382495</td>\n",
              "      <td>0.515713</td>\n",
              "      <td>0.972972</td>\n",
              "      <td>-0.085100</td>\n",
              "      <td>0.594548</td>\n",
              "      <td>0.353127</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.086147</td>\n",
              "      <td>0.207809</td>\n",
              "      <td>0.174145</td>\n",
              "      <td>0.787269</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.769472</td>\n",
              "      <td>-0.132921</td>\n",
              "      <td>-0.204258</td>\n",
              "      <td>-0.064800</td>\n",
              "      <td>0.039533</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>0.016014</td>\n",
              "      <td>0.288029</td>\n",
              "      <td>-0.025671</td>\n",
              "      <td>1.482471</td>\n",
              "      <td>-0.123995</td>\n",
              "      <td>-0.320350</td>\n",
              "      <td>-0.324812</td>\n",
              "      <td>-0.458072</td>\n",
              "      <td>0.261198</td>\n",
              "      <td>1.159143</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.336768</td>\n",
              "      <td>0.777057</td>\n",
              "      <td>0.872655</td>\n",
              "      <td>-0.002919</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.337404</td>\n",
              "      <td>0.063039</td>\n",
              "      <td>-0.635045</td>\n",
              "      <td>-0.070218</td>\n",
              "      <td>-0.084321</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>0.016014</td>\n",
              "      <td>0.288029</td>\n",
              "      <td>-0.025671</td>\n",
              "      <td>1.482471</td>\n",
              "      <td>-0.123995</td>\n",
              "      <td>-0.320350</td>\n",
              "      <td>-0.324812</td>\n",
              "      <td>-0.458072</td>\n",
              "      <td>0.261198</td>\n",
              "      <td>1.159143</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.336768</td>\n",
              "      <td>0.777057</td>\n",
              "      <td>0.872655</td>\n",
              "      <td>-0.002919</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.337404</td>\n",
              "      <td>0.063039</td>\n",
              "      <td>-0.635045</td>\n",
              "      <td>-0.070218</td>\n",
              "      <td>-0.084321</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>-0.258164</td>\n",
              "      <td>-0.440125</td>\n",
              "      <td>-0.288285</td>\n",
              "      <td>-0.106848</td>\n",
              "      <td>-0.521315</td>\n",
              "      <td>-0.397689</td>\n",
              "      <td>-0.359975</td>\n",
              "      <td>-0.655631</td>\n",
              "      <td>-0.455374</td>\n",
              "      <td>-0.378417</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.380036</td>\n",
              "      <td>-0.542131</td>\n",
              "      <td>-0.539597</td>\n",
              "      <td>-0.368254</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.764599</td>\n",
              "      <td>-0.039095</td>\n",
              "      <td>-0.588557</td>\n",
              "      <td>-0.069478</td>\n",
              "      <td>-0.025244</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>-0.171631</td>\n",
              "      <td>-0.191039</td>\n",
              "      <td>-0.261212</td>\n",
              "      <td>-0.177870</td>\n",
              "      <td>-0.320020</td>\n",
              "      <td>-0.156135</td>\n",
              "      <td>-0.273816</td>\n",
              "      <td>-0.597004</td>\n",
              "      <td>-0.213163</td>\n",
              "      <td>-0.298740</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.065874</td>\n",
              "      <td>-0.407746</td>\n",
              "      <td>-0.396421</td>\n",
              "      <td>-0.282508</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.379449</td>\n",
              "      <td>0.123421</td>\n",
              "      <td>-0.022658</td>\n",
              "      <td>-0.063328</td>\n",
              "      <td>-0.024364</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>-0.251563</td>\n",
              "      <td>-0.355794</td>\n",
              "      <td>-0.328736</td>\n",
              "      <td>-0.199893</td>\n",
              "      <td>-0.489107</td>\n",
              "      <td>-0.468246</td>\n",
              "      <td>-0.357154</td>\n",
              "      <td>-0.528870</td>\n",
              "      <td>-0.353999</td>\n",
              "      <td>-0.288816</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.447111</td>\n",
              "      <td>-0.362951</td>\n",
              "      <td>-0.337713</td>\n",
              "      <td>-0.383422</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.418617</td>\n",
              "      <td>-0.423583</td>\n",
              "      <td>0.491843</td>\n",
              "      <td>-0.068154</td>\n",
              "      <td>0.087284</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.604876</td>\n",
              "      <td>-0.301306</td>\n",
              "      <td>-0.013511</td>\n",
              "      <td>-0.426096</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.618657</td>\n",
              "      <td>-0.589751</td>\n",
              "      <td>-0.231506</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.494303</td>\n",
              "      <td>-0.478019</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.399033</td>\n",
              "      <td>-0.072348</td>\n",
              "      <td>0.334697</td>\n",
              "      <td>-0.068944</td>\n",
              "      <td>-0.031989</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>-2.413446</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.185845</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-1.452491</td>\n",
              "      <td>1.936591</td>\n",
              "      <td>0.495733</td>\n",
              "      <td>0.334054</td>\n",
              "      <td>1.054506</td>\n",
              "      <td>0.345217</td>\n",
              "      <td>0.994887</td>\n",
              "      <td>1.390865</td>\n",
              "      <td>0.468990</td>\n",
              "      <td>0.706308</td>\n",
              "      <td>0.717601</td>\n",
              "      <td>1.004179</td>\n",
              "      <td>2.737997</td>\n",
              "      <td>1.794361</td>\n",
              "      <td>-0.155613</td>\n",
              "      <td>-0.336690</td>\n",
              "      <td>-0.066396</td>\n",
              "      <td>-0.001379</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>2.689982</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.276913</td>\n",
              "      <td>0.706369</td>\n",
              "      <td>0.327481</td>\n",
              "      <td>1.675352</td>\n",
              "      <td>0.761428</td>\n",
              "      <td>2.134933</td>\n",
              "      <td>-1.975476</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.415362</td>\n",
              "      <td>-0.342027</td>\n",
              "      <td>-0.483047</td>\n",
              "      <td>2.201172</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.456130</td>\n",
              "      <td>-0.334393</td>\n",
              "      <td>-0.079938</td>\n",
              "      <td>-0.078491</td>\n",
              "      <td>-0.025955</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>-2.075973</td>\n",
              "      <td>-0.025183</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.354207</td>\n",
              "      <td>0.427099</td>\n",
              "      <td>0.543430</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.614418</td>\n",
              "      <td>-0.640937</td>\n",
              "      <td>1.701303</td>\n",
              "      <td>0.654054</td>\n",
              "      <td>0.525289</td>\n",
              "      <td>0.567921</td>\n",
              "      <td>0.636989</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>1.210093</td>\n",
              "      <td>-0.196666</td>\n",
              "      <td>2.339952</td>\n",
              "      <td>0.193029</td>\n",
              "      <td>-0.071526</td>\n",
              "      <td>-0.006726</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>-2.413446</td>\n",
              "      <td>0.545362</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.330496</td>\n",
              "      <td>-1.376307</td>\n",
              "      <td>1.622064</td>\n",
              "      <td>0.245009</td>\n",
              "      <td>0.825361</td>\n",
              "      <td>-0.200441</td>\n",
              "      <td>2.012078</td>\n",
              "      <td>-0.234997</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.392440</td>\n",
              "      <td>0.162697</td>\n",
              "      <td>0.157504</td>\n",
              "      <td>0.893050</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.716026</td>\n",
              "      <td>0.575136</td>\n",
              "      <td>0.017267</td>\n",
              "      <td>-0.040925</td>\n",
              "      <td>0.066329</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.614447</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.043020</td>\n",
              "      <td>-0.141141</td>\n",
              "      <td>-0.023135</td>\n",
              "      <td>1.422845</td>\n",
              "      <td>0.091051</td>\n",
              "      <td>0.242336</td>\n",
              "      <td>-0.049256</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.318118</td>\n",
              "      <td>0.322013</td>\n",
              "      <td>0.270448</td>\n",
              "      <td>1.520909</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.469185</td>\n",
              "      <td>-0.443667</td>\n",
              "      <td>0.826696</td>\n",
              "      <td>-0.067178</td>\n",
              "      <td>0.111968</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.109922</td>\n",
              "      <td>-0.326684</td>\n",
              "      <td>-0.135826</td>\n",
              "      <td>-0.401479</td>\n",
              "      <td>-0.348914</td>\n",
              "      <td>-0.385890</td>\n",
              "      <td>-0.630918</td>\n",
              "      <td>-0.022512</td>\n",
              "      <td>-0.321039</td>\n",
              "      <td>0.004587</td>\n",
              "      <td>-0.431296</td>\n",
              "      <td>-0.312430</td>\n",
              "      <td>-0.321945</td>\n",
              "      <td>-0.403600</td>\n",
              "      <td>0.043469</td>\n",
              "      <td>-0.464313</td>\n",
              "      <td>0.172148</td>\n",
              "      <td>-0.106414</td>\n",
              "      <td>-0.065299</td>\n",
              "      <td>-0.048081</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>1.298761</td>\n",
              "      <td>0.165375</td>\n",
              "      <td>-0.232811</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.220013</td>\n",
              "      <td>-0.525370</td>\n",
              "      <td>-0.439724</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.466038</td>\n",
              "      <td>-0.308850</td>\n",
              "      <td>-0.213561</td>\n",
              "      <td>-0.211735</td>\n",
              "      <td>-0.459377</td>\n",
              "      <td>-0.263627</td>\n",
              "      <td>-0.227825</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.319072</td>\n",
              "      <td>-0.385977</td>\n",
              "      <td>-0.488460</td>\n",
              "      <td>-0.101919</td>\n",
              "      <td>-0.070444</td>\n",
              "      <td>0.273939</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>0.022002</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>0.809971</td>\n",
              "      <td>0.431208</td>\n",
              "      <td>-0.017844</td>\n",
              "      <td>0.430757</td>\n",
              "      <td>-0.204479</td>\n",
              "      <td>-0.450130</td>\n",
              "      <td>-0.094857</td>\n",
              "      <td>0.316842</td>\n",
              "      <td>0.198258</td>\n",
              "      <td>0.198359</td>\n",
              "      <td>-0.159079</td>\n",
              "      <td>-0.146839</td>\n",
              "      <td>-0.343339</td>\n",
              "      <td>0.204780</td>\n",
              "      <td>-0.105275</td>\n",
              "      <td>0.367275</td>\n",
              "      <td>-0.993319</td>\n",
              "      <td>-0.067899</td>\n",
              "      <td>0.019730</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>-0.260430</td>\n",
              "      <td>-0.538183</td>\n",
              "      <td>-0.323538</td>\n",
              "      <td>-0.297878</td>\n",
              "      <td>-0.366694</td>\n",
              "      <td>-0.477529</td>\n",
              "      <td>-0.379486</td>\n",
              "      <td>-0.537914</td>\n",
              "      <td>-0.491614</td>\n",
              "      <td>-0.351030</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.462153</td>\n",
              "      <td>-0.543969</td>\n",
              "      <td>-0.533463</td>\n",
              "      <td>-0.383057</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.470841</td>\n",
              "      <td>-0.191332</td>\n",
              "      <td>-0.120329</td>\n",
              "      <td>-0.065568</td>\n",
              "      <td>-0.029811</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>-0.276115</td>\n",
              "      <td>-0.271883</td>\n",
              "      <td>-0.643282</td>\n",
              "      <td>-0.327855</td>\n",
              "      <td>-0.241242</td>\n",
              "      <td>-0.590597</td>\n",
              "      <td>-0.457090</td>\n",
              "      <td>-0.262611</td>\n",
              "      <td>-0.685000</td>\n",
              "      <td>-0.623844</td>\n",
              "      <td>-0.477339</td>\n",
              "      <td>-0.167027</td>\n",
              "      <td>-0.458084</td>\n",
              "      <td>-0.678994</td>\n",
              "      <td>-0.688741</td>\n",
              "      <td>-0.270899</td>\n",
              "      <td>-0.235550</td>\n",
              "      <td>-0.849462</td>\n",
              "      <td>-0.426328</td>\n",
              "      <td>-0.529659</td>\n",
              "      <td>-0.068511</td>\n",
              "      <td>0.038894</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>-0.501097</td>\n",
              "      <td>-0.261606</td>\n",
              "      <td>-0.646125</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.219413</td>\n",
              "      <td>-0.580219</td>\n",
              "      <td>-0.471547</td>\n",
              "      <td>-0.374206</td>\n",
              "      <td>-0.674836</td>\n",
              "      <td>-0.627797</td>\n",
              "      <td>-0.473749</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.344071</td>\n",
              "      <td>-0.676878</td>\n",
              "      <td>-0.681753</td>\n",
              "      <td>-0.387970</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.797238</td>\n",
              "      <td>-0.416033</td>\n",
              "      <td>0.585103</td>\n",
              "      <td>-0.063688</td>\n",
              "      <td>0.013679</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.328703</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.146157</td>\n",
              "      <td>-0.483037</td>\n",
              "      <td>-0.439520</td>\n",
              "      <td>-0.364086</td>\n",
              "      <td>-0.641881</td>\n",
              "      <td>-0.434166</td>\n",
              "      <td>-0.189318</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.435171</td>\n",
              "      <td>-0.385090</td>\n",
              "      <td>-0.361069</td>\n",
              "      <td>-0.384729</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.614456</td>\n",
              "      <td>-0.078337</td>\n",
              "      <td>0.059638</td>\n",
              "      <td>-0.070829</td>\n",
              "      <td>-0.061678</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>-1.963482</td>\n",
              "      <td>-0.248113</td>\n",
              "      <td>-0.633813</td>\n",
              "      <td>-0.015690</td>\n",
              "      <td>0.143903</td>\n",
              "      <td>-0.341429</td>\n",
              "      <td>-0.234946</td>\n",
              "      <td>-0.366503</td>\n",
              "      <td>-0.658348</td>\n",
              "      <td>-0.611174</td>\n",
              "      <td>-0.038972</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.212195</td>\n",
              "      <td>-0.416828</td>\n",
              "      <td>-0.436462</td>\n",
              "      <td>-0.379744</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.503480</td>\n",
              "      <td>0.857472</td>\n",
              "      <td>0.314893</td>\n",
              "      <td>-0.070847</td>\n",
              "      <td>-0.034985</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.104722</td>\n",
              "      <td>-0.099839</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.451654</td>\n",
              "      <td>-0.508965</td>\n",
              "      <td>-0.184796</td>\n",
              "      <td>-0.367941</td>\n",
              "      <td>-0.574298</td>\n",
              "      <td>-0.120269</td>\n",
              "      <td>-0.473561</td>\n",
              "      <td>-0.483913</td>\n",
              "      <td>-0.130041</td>\n",
              "      <td>-0.414808</td>\n",
              "      <td>-0.443855</td>\n",
              "      <td>-0.380967</td>\n",
              "      <td>-0.747256</td>\n",
              "      <td>-0.529592</td>\n",
              "      <td>-0.385239</td>\n",
              "      <td>-0.073212</td>\n",
              "      <td>-0.035597</td>\n",
              "      <td>0.163274</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.451175</td>\n",
              "      <td>-0.332583</td>\n",
              "      <td>-0.302667</td>\n",
              "      <td>-0.468331</td>\n",
              "      <td>0.570134</td>\n",
              "      <td>-0.393841</td>\n",
              "      <td>-0.462748</td>\n",
              "      <td>-0.392345</td>\n",
              "      <td>-0.045927</td>\n",
              "      <td>-0.193832</td>\n",
              "      <td>-0.012167</td>\n",
              "      <td>-0.280946</td>\n",
              "      <td>-0.246912</td>\n",
              "      <td>-0.408938</td>\n",
              "      <td>-0.269130</td>\n",
              "      <td>0.723776</td>\n",
              "      <td>-0.391182</td>\n",
              "      <td>-0.666195</td>\n",
              "      <td>-0.065135</td>\n",
              "      <td>-0.074972</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.271829</td>\n",
              "      <td>-0.449730</td>\n",
              "      <td>-0.258513</td>\n",
              "      <td>-0.110432</td>\n",
              "      <td>-0.568636</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.590548</td>\n",
              "      <td>-0.419089</td>\n",
              "      <td>-0.413579</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.502677</td>\n",
              "      <td>-0.487174</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.634039</td>\n",
              "      <td>-0.480022</td>\n",
              "      <td>-0.721641</td>\n",
              "      <td>-0.065662</td>\n",
              "      <td>-0.077885</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.271550</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.092833</td>\n",
              "      <td>-0.516481</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.327557</td>\n",
              "      <td>-0.668604</td>\n",
              "      <td>-0.035824</td>\n",
              "      <td>-0.623865</td>\n",
              "      <td>-0.451253</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.524100</td>\n",
              "      <td>-0.515796</td>\n",
              "      <td>-0.340255</td>\n",
              "      <td>-0.769522</td>\n",
              "      <td>-0.731959</td>\n",
              "      <td>-0.082648</td>\n",
              "      <td>1.811375</td>\n",
              "      <td>-0.075882</td>\n",
              "      <td>-0.354718</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>1.186270</td>\n",
              "      <td>-0.240973</td>\n",
              "      <td>0.659702</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.362634</td>\n",
              "      <td>-0.349700</td>\n",
              "      <td>-0.342885</td>\n",
              "      <td>-0.354256</td>\n",
              "      <td>-0.335367</td>\n",
              "      <td>0.760939</td>\n",
              "      <td>-0.351770</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.390574</td>\n",
              "      <td>0.117367</td>\n",
              "      <td>0.179593</td>\n",
              "      <td>-0.366666</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.372921</td>\n",
              "      <td>-0.448385</td>\n",
              "      <td>3.169109</td>\n",
              "      <td>-0.064782</td>\n",
              "      <td>0.183005</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>-0.266673</td>\n",
              "      <td>-0.068724</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.266186</td>\n",
              "      <td>-0.573705</td>\n",
              "      <td>-0.410653</td>\n",
              "      <td>-0.346419</td>\n",
              "      <td>-0.672950</td>\n",
              "      <td>-0.061576</td>\n",
              "      <td>-0.192505</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.441825</td>\n",
              "      <td>-0.186979</td>\n",
              "      <td>-0.149111</td>\n",
              "      <td>-0.367658</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.620984</td>\n",
              "      <td>-0.284457</td>\n",
              "      <td>-0.688607</td>\n",
              "      <td>-0.071552</td>\n",
              "      <td>0.017497</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>0.176585</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.220929</td>\n",
              "      <td>-0.286091</td>\n",
              "      <td>-0.391529</td>\n",
              "      <td>-0.345344</td>\n",
              "      <td>-0.633839</td>\n",
              "      <td>-0.640032</td>\n",
              "      <td>0.277967</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.348377</td>\n",
              "      <td>-0.189213</td>\n",
              "      <td>-0.152089</td>\n",
              "      <td>-0.388677</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.431673</td>\n",
              "      <td>0.565601</td>\n",
              "      <td>0.204993</td>\n",
              "      <td>-0.071150</td>\n",
              "      <td>-0.007911</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.456569</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.154085</td>\n",
              "      <td>-0.519123</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.386989</td>\n",
              "      <td>-0.645756</td>\n",
              "      <td>-0.418857</td>\n",
              "      <td>-0.337516</td>\n",
              "      <td>-0.146831</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.493490</td>\n",
              "      <td>-0.478839</td>\n",
              "      <td>-0.401621</td>\n",
              "      <td>-0.201482</td>\n",
              "      <td>-0.464313</td>\n",
              "      <td>-0.254440</td>\n",
              "      <td>0.746064</td>\n",
              "      <td>-0.068090</td>\n",
              "      <td>-0.009812</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.194056</td>\n",
              "      <td>0.045046</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.145413</td>\n",
              "      <td>0.047939</td>\n",
              "      <td>-0.469017</td>\n",
              "      <td>-0.051444</td>\n",
              "      <td>-0.635968</td>\n",
              "      <td>0.433456</td>\n",
              "      <td>-0.127313</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.457953</td>\n",
              "      <td>-0.104525</td>\n",
              "      <td>-0.076074</td>\n",
              "      <td>-0.061633</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.305987</td>\n",
              "      <td>1.199177</td>\n",
              "      <td>3.245778</td>\n",
              "      <td>-0.064383</td>\n",
              "      <td>-0.051896</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>-0.726079</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.177215</td>\n",
              "      <td>-0.022373</td>\n",
              "      <td>-0.171416</td>\n",
              "      <td>-0.268537</td>\n",
              "      <td>-0.355034</td>\n",
              "      <td>-0.614447</td>\n",
              "      <td>-0.573125</td>\n",
              "      <td>-0.112130</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.251523</td>\n",
              "      <td>-0.492659</td>\n",
              "      <td>-0.480162</td>\n",
              "      <td>-0.343325</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.372921</td>\n",
              "      <td>0.607937</td>\n",
              "      <td>0.323982</td>\n",
              "      <td>-0.067850</td>\n",
              "      <td>-0.023960</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.171818</td>\n",
              "      <td>0.762689</td>\n",
              "      <td>1.343518</td>\n",
              "      <td>1.251549</td>\n",
              "      <td>2.185145</td>\n",
              "      <td>-0.093313</td>\n",
              "      <td>-0.087100</td>\n",
              "      <td>1.206764</td>\n",
              "      <td>-0.286436</td>\n",
              "      <td>0.637504</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.215969</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>0.022079</td>\n",
              "      <td>-0.083471</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.261147</td>\n",
              "      <td>-0.189332</td>\n",
              "      <td>-0.027353</td>\n",
              "      <td>-0.067785</td>\n",
              "      <td>-0.062610</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>1.298761</td>\n",
              "      <td>-0.269223</td>\n",
              "      <td>-0.586850</td>\n",
              "      <td>-0.332183</td>\n",
              "      <td>-0.392844</td>\n",
              "      <td>-0.583465</td>\n",
              "      <td>-0.469020</td>\n",
              "      <td>-0.394451</td>\n",
              "      <td>-0.659188</td>\n",
              "      <td>-0.555066</td>\n",
              "      <td>-0.389205</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.457504</td>\n",
              "      <td>-0.554224</td>\n",
              "      <td>-0.543009</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.790710</td>\n",
              "      <td>-0.491644</td>\n",
              "      <td>0.671541</td>\n",
              "      <td>-0.070974</td>\n",
              "      <td>0.022550</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.269223</td>\n",
              "      <td>-0.586850</td>\n",
              "      <td>-0.332183</td>\n",
              "      <td>-0.392844</td>\n",
              "      <td>-0.583465</td>\n",
              "      <td>-0.469020</td>\n",
              "      <td>-0.394451</td>\n",
              "      <td>-0.659188</td>\n",
              "      <td>-0.555066</td>\n",
              "      <td>-0.389205</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.457504</td>\n",
              "      <td>-0.554224</td>\n",
              "      <td>-0.543009</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.790710</td>\n",
              "      <td>-0.491644</td>\n",
              "      <td>0.671541</td>\n",
              "      <td>-0.070974</td>\n",
              "      <td>0.022550</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.588628</td>\n",
              "      <td>-0.148009</td>\n",
              "      <td>0.090268</td>\n",
              "      <td>-0.345768</td>\n",
              "      <td>-0.470629</td>\n",
              "      <td>-0.379581</td>\n",
              "      <td>-0.635662</td>\n",
              "      <td>-0.482751</td>\n",
              "      <td>-0.231433</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.463098</td>\n",
              "      <td>-0.465899</td>\n",
              "      <td>-0.461648</td>\n",
              "      <td>-0.199289</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.229306</td>\n",
              "      <td>0.229764</td>\n",
              "      <td>1.239398</td>\n",
              "      <td>-0.066841</td>\n",
              "      <td>0.015293</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.321157</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.062763</td>\n",
              "      <td>-0.181765</td>\n",
              "      <td>-0.446360</td>\n",
              "      <td>-0.204867</td>\n",
              "      <td>-0.626003</td>\n",
              "      <td>-0.267350</td>\n",
              "      <td>-0.261358</td>\n",
              "      <td>-0.022733</td>\n",
              "      <td>-0.434222</td>\n",
              "      <td>-0.471991</td>\n",
              "      <td>-0.464372</td>\n",
              "      <td>-0.231304</td>\n",
              "      <td>-0.020937</td>\n",
              "      <td>-0.242362</td>\n",
              "      <td>0.914368</td>\n",
              "      <td>-0.141090</td>\n",
              "      <td>-0.066560</td>\n",
              "      <td>-0.025625</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.253871</td>\n",
              "      <td>-0.413258</td>\n",
              "      <td>-0.245055</td>\n",
              "      <td>-0.082478</td>\n",
              "      <td>-0.479785</td>\n",
              "      <td>-0.464758</td>\n",
              "      <td>-0.388645</td>\n",
              "      <td>-0.677236</td>\n",
              "      <td>-0.367983</td>\n",
              "      <td>-0.321402</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.443626</td>\n",
              "      <td>-0.465503</td>\n",
              "      <td>-0.508237</td>\n",
              "      <td>-0.400237</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.470841</td>\n",
              "      <td>1.167820</td>\n",
              "      <td>-0.820172</td>\n",
              "      <td>-0.068374</td>\n",
              "      <td>0.067055</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.235899</td>\n",
              "      <td>-0.489946</td>\n",
              "      <td>-0.303527</td>\n",
              "      <td>-0.186730</td>\n",
              "      <td>-0.494270</td>\n",
              "      <td>-0.372275</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.658514</td>\n",
              "      <td>-0.494007</td>\n",
              "      <td>-0.261741</td>\n",
              "      <td>-0.134266</td>\n",
              "      <td>-0.340664</td>\n",
              "      <td>-0.486542</td>\n",
              "      <td>-0.469601</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.209188</td>\n",
              "      <td>-0.640567</td>\n",
              "      <td>0.259361</td>\n",
              "      <td>-0.746780</td>\n",
              "      <td>-0.070594</td>\n",
              "      <td>0.015831</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>-0.163623</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.408790</td>\n",
              "      <td>-0.182111</td>\n",
              "      <td>-0.250364</td>\n",
              "      <td>-0.418621</td>\n",
              "      <td>-0.014175</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.661885</td>\n",
              "      <td>-0.314403</td>\n",
              "      <td>-0.280742</td>\n",
              "      <td>0.148926</td>\n",
              "      <td>-0.287849</td>\n",
              "      <td>-0.445573</td>\n",
              "      <td>-0.425165</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>0.331557</td>\n",
              "      <td>-0.444729</td>\n",
              "      <td>1.988421</td>\n",
              "      <td>-1.308120</td>\n",
              "      <td>-0.066318</td>\n",
              "      <td>-0.059042</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>-0.726079</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.436487</td>\n",
              "      <td>0.234882</td>\n",
              "      <td>0.358233</td>\n",
              "      <td>-0.264328</td>\n",
              "      <td>-0.457337</td>\n",
              "      <td>-0.378679</td>\n",
              "      <td>-0.695282</td>\n",
              "      <td>-0.349474</td>\n",
              "      <td>-0.222681</td>\n",
              "      <td>-0.043745</td>\n",
              "      <td>-0.447660</td>\n",
              "      <td>-0.467051</td>\n",
              "      <td>-0.452497</td>\n",
              "      <td>-0.392747</td>\n",
              "      <td>-0.151634</td>\n",
              "      <td>-0.542648</td>\n",
              "      <td>8.746198</td>\n",
              "      <td>2.968160</td>\n",
              "      <td>-0.069649</td>\n",
              "      <td>-0.022298</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.164168</td>\n",
              "      <td>-0.451016</td>\n",
              "      <td>-0.307794</td>\n",
              "      <td>-0.082266</td>\n",
              "      <td>-0.130523</td>\n",
              "      <td>-0.476327</td>\n",
              "      <td>-0.350835</td>\n",
              "      <td>-0.627999</td>\n",
              "      <td>-0.431889</td>\n",
              "      <td>-0.182303</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.455587</td>\n",
              "      <td>-0.492528</td>\n",
              "      <td>-0.481220</td>\n",
              "      <td>-0.323051</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.483896</td>\n",
              "      <td>0.875320</td>\n",
              "      <td>0.627362</td>\n",
              "      <td>-0.069031</td>\n",
              "      <td>0.016191</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>-0.234930</td>\n",
              "      <td>-0.312821</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.136769</td>\n",
              "      <td>-0.357308</td>\n",
              "      <td>-0.475392</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.608724</td>\n",
              "      <td>-0.383753</td>\n",
              "      <td>-0.237215</td>\n",
              "      <td>-0.016779</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.426792</td>\n",
              "      <td>-0.406670</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>0.158504</td>\n",
              "      <td>-0.320697</td>\n",
              "      <td>0.344403</td>\n",
              "      <td>-0.787415</td>\n",
              "      <td>-0.067604</td>\n",
              "      <td>0.072405</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>-0.501097</td>\n",
              "      <td>2.058272</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>10.989161</td>\n",
              "      <td>0.618244</td>\n",
              "      <td>5.157264</td>\n",
              "      <td>5.501729</td>\n",
              "      <td>0.932294</td>\n",
              "      <td>0.280420</td>\n",
              "      <td>0.275625</td>\n",
              "      <td>9.664131</td>\n",
              "      <td>14.691887</td>\n",
              "      <td>6.168130</td>\n",
              "      <td>5.083747</td>\n",
              "      <td>2.917045</td>\n",
              "      <td>0.871140</td>\n",
              "      <td>13.161333</td>\n",
              "      <td>1.885752</td>\n",
              "      <td>0.885388</td>\n",
              "      <td>0.185527</td>\n",
              "      <td>-0.069541</td>\n",
              "      <td>-0.300773</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.273920</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.567163</td>\n",
              "      <td>-0.595753</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.393597</td>\n",
              "      <td>-0.650872</td>\n",
              "      <td>-0.639755</td>\n",
              "      <td>-0.171405</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.405560</td>\n",
              "      <td>-0.399858</td>\n",
              "      <td>-0.103885</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.869046</td>\n",
              "      <td>-0.557094</td>\n",
              "      <td>-0.108802</td>\n",
              "      <td>-0.072921</td>\n",
              "      <td>-0.017819</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.265450</td>\n",
              "      <td>-0.354038</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.470823</td>\n",
              "      <td>-0.553206</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.392581</td>\n",
              "      <td>-0.635033</td>\n",
              "      <td>-0.452197</td>\n",
              "      <td>-0.231602</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.324948</td>\n",
              "      <td>-0.387163</td>\n",
              "      <td>-0.361938</td>\n",
              "      <td>-0.407593</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.686263</td>\n",
              "      <td>-0.322553</td>\n",
              "      <td>-0.849232</td>\n",
              "      <td>-0.071454</td>\n",
              "      <td>-0.161037</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.250569</td>\n",
              "      <td>-0.508628</td>\n",
              "      <td>-0.326285</td>\n",
              "      <td>-0.222780</td>\n",
              "      <td>-0.470407</td>\n",
              "      <td>-0.478159</td>\n",
              "      <td>-0.392335</td>\n",
              "      <td>-0.673716</td>\n",
              "      <td>-0.473699</td>\n",
              "      <td>-0.382755</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.465990</td>\n",
              "      <td>-0.577582</td>\n",
              "      <td>-0.570526</td>\n",
              "      <td>-0.407330</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.594872</td>\n",
              "      <td>0.443992</td>\n",
              "      <td>0.775344</td>\n",
              "      <td>-0.068392</td>\n",
              "      <td>0.019435</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>-0.276115</td>\n",
              "      <td>-0.020888</td>\n",
              "      <td>-0.439644</td>\n",
              "      <td>0.137802</td>\n",
              "      <td>0.498933</td>\n",
              "      <td>0.122827</td>\n",
              "      <td>0.737199</td>\n",
              "      <td>2.800829</td>\n",
              "      <td>0.513492</td>\n",
              "      <td>-0.223282</td>\n",
              "      <td>0.081976</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.758792</td>\n",
              "      <td>0.103247</td>\n",
              "      <td>0.014177</td>\n",
              "      <td>2.220617</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.108925</td>\n",
              "      <td>-0.417984</td>\n",
              "      <td>0.024669</td>\n",
              "      <td>-0.057730</td>\n",
              "      <td>-0.036356</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.432724</td>\n",
              "      <td>0.026048</td>\n",
              "      <td>-0.296428</td>\n",
              "      <td>-0.261534</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>0.097503</td>\n",
              "      <td>-0.530794</td>\n",
              "      <td>-0.086744</td>\n",
              "      <td>-0.230727</td>\n",
              "      <td>-0.023388</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.311867</td>\n",
              "      <td>-0.388599</td>\n",
              "      <td>0.115768</td>\n",
              "      <td>0.029894</td>\n",
              "      <td>0.364738</td>\n",
              "      <td>0.062879</td>\n",
              "      <td>-0.609591</td>\n",
              "      <td>-0.064281</td>\n",
              "      <td>-0.008333</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.250153</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.215289</td>\n",
              "      <td>0.338598</td>\n",
              "      <td>-0.408967</td>\n",
              "      <td>-0.393487</td>\n",
              "      <td>-0.608790</td>\n",
              "      <td>-0.485645</td>\n",
              "      <td>-0.180947</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.384881</td>\n",
              "      <td>-0.567027</td>\n",
              "      <td>-0.561800</td>\n",
              "      <td>-0.408561</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.490424</td>\n",
              "      <td>1.318598</td>\n",
              "      <td>2.541730</td>\n",
              "      <td>-0.069351</td>\n",
              "      <td>-0.171182</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.064817</td>\n",
              "      <td>-0.594783</td>\n",
              "      <td>0.706498</td>\n",
              "      <td>-0.229812</td>\n",
              "      <td>-0.242646</td>\n",
              "      <td>-0.129453</td>\n",
              "      <td>-0.161435</td>\n",
              "      <td>-0.149655</td>\n",
              "      <td>-0.419546</td>\n",
              "      <td>0.110046</td>\n",
              "      <td>0.284099</td>\n",
              "      <td>-0.069451</td>\n",
              "      <td>-0.054373</td>\n",
              "      <td>-0.021299</td>\n",
              "      <td>-0.067450</td>\n",
              "      <td>0.557026</td>\n",
              "      <td>0.423490</td>\n",
              "      <td>-0.393465</td>\n",
              "      <td>-0.208861</td>\n",
              "      <td>-0.065049</td>\n",
              "      <td>0.001650</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>1.186270</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.116930</td>\n",
              "      <td>-0.062827</td>\n",
              "      <td>0.022937</td>\n",
              "      <td>-0.309586</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.356170</td>\n",
              "      <td>-0.597927</td>\n",
              "      <td>-0.035295</td>\n",
              "      <td>-0.111465</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.151863</td>\n",
              "      <td>-0.128895</td>\n",
              "      <td>-0.368709</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.031812</td>\n",
              "      <td>-0.036542</td>\n",
              "      <td>2.180510</td>\n",
              "      <td>-0.067641</td>\n",
              "      <td>0.025322</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>-0.163623</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.658085</td>\n",
              "      <td>-0.257732</td>\n",
              "      <td>-0.138405</td>\n",
              "      <td>-0.394639</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.128841</td>\n",
              "      <td>-0.687759</td>\n",
              "      <td>-0.640893</td>\n",
              "      <td>-0.351908</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.659914</td>\n",
              "      <td>-0.674428</td>\n",
              "      <td>-0.125945</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.823350</td>\n",
              "      <td>3.751035</td>\n",
              "      <td>-0.434930</td>\n",
              "      <td>-0.071687</td>\n",
              "      <td>-0.092374</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>1.411252</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.658085</td>\n",
              "      <td>-0.257732</td>\n",
              "      <td>-0.138405</td>\n",
              "      <td>-0.394639</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.128841</td>\n",
              "      <td>-0.687759</td>\n",
              "      <td>-0.640893</td>\n",
              "      <td>-0.351908</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.659914</td>\n",
              "      <td>-0.674428</td>\n",
              "      <td>-0.125945</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.502887</td>\n",
              "      <td>3.751035</td>\n",
              "      <td>-0.434930</td>\n",
              "      <td>-0.071687</td>\n",
              "      <td>-0.092374</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>-1.401026</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>2.697792</td>\n",
              "      <td>1.921241</td>\n",
              "      <td>2.032747</td>\n",
              "      <td>0.373840</td>\n",
              "      <td>1.661810</td>\n",
              "      <td>-0.513394</td>\n",
              "      <td>0.557116</td>\n",
              "      <td>1.426372</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.231002</td>\n",
              "      <td>0.392348</td>\n",
              "      <td>0.380649</td>\n",
              "      <td>1.331485</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.092219</td>\n",
              "      <td>2.943313</td>\n",
              "      <td>0.131261</td>\n",
              "      <td>-0.069749</td>\n",
              "      <td>-0.011078</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>0.770496</td>\n",
              "      <td>-0.400564</td>\n",
              "      <td>2.193357</td>\n",
              "      <td>1.907546</td>\n",
              "      <td>0.807715</td>\n",
              "      <td>0.283231</td>\n",
              "      <td>-0.233919</td>\n",
              "      <td>0.188829</td>\n",
              "      <td>-0.360669</td>\n",
              "      <td>0.377393</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.225732</td>\n",
              "      <td>-0.085321</td>\n",
              "      <td>-0.095473</td>\n",
              "      <td>-0.257605</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.606273</td>\n",
              "      <td>-0.177946</td>\n",
              "      <td>0.175389</td>\n",
              "      <td>-0.063641</td>\n",
              "      <td>-0.056812</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>-0.838570</td>\n",
              "      <td>3.928141</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>3.350724</td>\n",
              "      <td>3.239462</td>\n",
              "      <td>5.446834</td>\n",
              "      <td>7.010583</td>\n",
              "      <td>3.286945</td>\n",
              "      <td>2.324962</td>\n",
              "      <td>0.348737</td>\n",
              "      <td>1.584814</td>\n",
              "      <td>-0.084540</td>\n",
              "      <td>6.272286</td>\n",
              "      <td>0.095657</td>\n",
              "      <td>-0.483326</td>\n",
              "      <td>3.241897</td>\n",
              "      <td>-0.121218</td>\n",
              "      <td>1.944504</td>\n",
              "      <td>-0.028332</td>\n",
              "      <td>-0.159250</td>\n",
              "      <td>-0.061809</td>\n",
              "      <td>-0.039382</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>-0.154178</td>\n",
              "      <td>-0.661018</td>\n",
              "      <td>-0.321718</td>\n",
              "      <td>-0.215168</td>\n",
              "      <td>-0.397501</td>\n",
              "      <td>-0.272109</td>\n",
              "      <td>-0.345344</td>\n",
              "      <td>-0.611423</td>\n",
              "      <td>-0.623097</td>\n",
              "      <td>-0.279481</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.214898</td>\n",
              "      <td>-0.563465</td>\n",
              "      <td>-0.564033</td>\n",
              "      <td>-0.357149</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.581816</td>\n",
              "      <td>0.046027</td>\n",
              "      <td>-0.228823</td>\n",
              "      <td>-0.067120</td>\n",
              "      <td>-0.025226</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>-0.726079</td>\n",
              "      <td>4.925319</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>0.295678</td>\n",
              "      <td>-4.358549</td>\n",
              "      <td>3.301342</td>\n",
              "      <td>3.741463</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>2.299245</td>\n",
              "      <td>5.137371</td>\n",
              "      <td>7.836260</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>3.361953</td>\n",
              "      <td>8.694915</td>\n",
              "      <td>9.426533</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>6.938392</td>\n",
              "      <td>-0.246101</td>\n",
              "      <td>0.155948</td>\n",
              "      <td>-0.069005</td>\n",
              "      <td>0.037529</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.331427</td>\n",
              "      <td>-0.297069</td>\n",
              "      <td>-0.060811</td>\n",
              "      <td>-0.400540</td>\n",
              "      <td>-0.473247</td>\n",
              "      <td>0.074075</td>\n",
              "      <td>-0.635625</td>\n",
              "      <td>-0.267382</td>\n",
              "      <td>-0.413637</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.459589</td>\n",
              "      <td>-0.501911</td>\n",
              "      <td>-0.514883</td>\n",
              "      <td>0.073970</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.614456</td>\n",
              "      <td>0.371310</td>\n",
              "      <td>-0.564675</td>\n",
              "      <td>-0.059238</td>\n",
              "      <td>-0.024978</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.293378</td>\n",
              "      <td>-0.332433</td>\n",
              "      <td>-0.181647</td>\n",
              "      <td>-0.464470</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.342195</td>\n",
              "      <td>-0.594441</td>\n",
              "      <td>-0.213285</td>\n",
              "      <td>0.058277</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.070663</td>\n",
              "      <td>-0.022663</td>\n",
              "      <td>-0.353786</td>\n",
              "      <td>1.250708</td>\n",
              "      <td>-0.418617</td>\n",
              "      <td>-0.226424</td>\n",
              "      <td>-0.213890</td>\n",
              "      <td>-0.070892</td>\n",
              "      <td>-0.012058</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>-2.413446</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.596399</td>\n",
              "      <td>0.631641</td>\n",
              "      <td>1.576874</td>\n",
              "      <td>4.553706</td>\n",
              "      <td>2.765551</td>\n",
              "      <td>3.716207</td>\n",
              "      <td>1.440509</td>\n",
              "      <td>-0.083383</td>\n",
              "      <td>2.627605</td>\n",
              "      <td>-0.148699</td>\n",
              "      <td>3.160511</td>\n",
              "      <td>0.639351</td>\n",
              "      <td>0.727646</td>\n",
              "      <td>-0.005549</td>\n",
              "      <td>-0.204775</td>\n",
              "      <td>2.858418</td>\n",
              "      <td>0.072536</td>\n",
              "      <td>-0.106745</td>\n",
              "      <td>-0.067964</td>\n",
              "      <td>-0.022433</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>-2.525937</td>\n",
              "      <td>-0.189491</td>\n",
              "      <td>-0.662182</td>\n",
              "      <td>-0.286359</td>\n",
              "      <td>-0.276053</td>\n",
              "      <td>-0.446690</td>\n",
              "      <td>-0.138750</td>\n",
              "      <td>-0.371386</td>\n",
              "      <td>-0.645640</td>\n",
              "      <td>-0.472348</td>\n",
              "      <td>0.245377</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.218433</td>\n",
              "      <td>-0.072881</td>\n",
              "      <td>-0.025014</td>\n",
              "      <td>-0.384959</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.607928</td>\n",
              "      <td>0.113556</td>\n",
              "      <td>0.099509</td>\n",
              "      <td>-0.071537</td>\n",
              "      <td>-0.007715</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.412223</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.215800</td>\n",
              "      <td>-0.470560</td>\n",
              "      <td>-0.304966</td>\n",
              "      <td>-0.384610</td>\n",
              "      <td>-0.528988</td>\n",
              "      <td>-0.480866</td>\n",
              "      <td>-0.229357</td>\n",
              "      <td>-0.041191</td>\n",
              "      <td>-0.283354</td>\n",
              "      <td>-0.402763</td>\n",
              "      <td>-0.379362</td>\n",
              "      <td>-0.399081</td>\n",
              "      <td>-0.019423</td>\n",
              "      <td>-0.620984</td>\n",
              "      <td>-0.242531</td>\n",
              "      <td>-1.095498</td>\n",
              "      <td>-0.070941</td>\n",
              "      <td>0.126579</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.586067</td>\n",
              "      <td>-0.327613</td>\n",
              "      <td>-0.192337</td>\n",
              "      <td>-0.587938</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.365026</td>\n",
              "      <td>-0.697368</td>\n",
              "      <td>-0.583334</td>\n",
              "      <td>-0.447741</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.636280</td>\n",
              "      <td>-0.633874</td>\n",
              "      <td>-0.378167</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.836406</td>\n",
              "      <td>3.370865</td>\n",
              "      <td>-1.687754</td>\n",
              "      <td>-0.061480</td>\n",
              "      <td>-1.689950</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>-0.257616</td>\n",
              "      <td>-0.390439</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.214358</td>\n",
              "      <td>-0.473537</td>\n",
              "      <td>-0.436673</td>\n",
              "      <td>-0.370734</td>\n",
              "      <td>-0.650583</td>\n",
              "      <td>-0.404415</td>\n",
              "      <td>-0.319031</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.447677</td>\n",
              "      <td>-0.486593</td>\n",
              "      <td>-0.475162</td>\n",
              "      <td>-0.370079</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.568760</td>\n",
              "      <td>0.154214</td>\n",
              "      <td>-0.426327</td>\n",
              "      <td>-0.068224</td>\n",
              "      <td>0.001079</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>-0.245282</td>\n",
              "      <td>-0.421009</td>\n",
              "      <td>-0.263036</td>\n",
              "      <td>-0.180164</td>\n",
              "      <td>-0.056392</td>\n",
              "      <td>-0.408970</td>\n",
              "      <td>-0.197289</td>\n",
              "      <td>-0.648770</td>\n",
              "      <td>-0.086322</td>\n",
              "      <td>-0.381404</td>\n",
              "      <td>-0.096955</td>\n",
              "      <td>-0.418054</td>\n",
              "      <td>-0.524624</td>\n",
              "      <td>-0.523231</td>\n",
              "      <td>-0.201353</td>\n",
              "      <td>-0.120551</td>\n",
              "      <td>0.260291</td>\n",
              "      <td>3.439268</td>\n",
              "      <td>-0.946728</td>\n",
              "      <td>-0.054344</td>\n",
              "      <td>-0.009654</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>-0.276115</td>\n",
              "      <td>-0.128763</td>\n",
              "      <td>-0.485131</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.137376</td>\n",
              "      <td>-0.475004</td>\n",
              "      <td>0.012481</td>\n",
              "      <td>-0.295450</td>\n",
              "      <td>-0.648553</td>\n",
              "      <td>-0.457059</td>\n",
              "      <td>-0.414655</td>\n",
              "      <td>-0.120921</td>\n",
              "      <td>-0.269955</td>\n",
              "      <td>-0.584760</td>\n",
              "      <td>-0.582715</td>\n",
              "      <td>-0.303867</td>\n",
              "      <td>-0.181939</td>\n",
              "      <td>-0.075977</td>\n",
              "      <td>-0.063677</td>\n",
              "      <td>0.896990</td>\n",
              "      <td>-0.063724</td>\n",
              "      <td>-0.046656</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.576899</td>\n",
              "      <td>1.113488</td>\n",
              "      <td>0.713283</td>\n",
              "      <td>0.493486</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.392588</td>\n",
              "      <td>-0.501945</td>\n",
              "      <td>-0.448050</td>\n",
              "      <td>0.410680</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.250169</td>\n",
              "      <td>-0.213604</td>\n",
              "      <td>-0.403347</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.103620</td>\n",
              "      <td>0.957877</td>\n",
              "      <td>-0.252940</td>\n",
              "      <td>-0.069541</td>\n",
              "      <td>-0.049216</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>-1.401026</td>\n",
              "      <td>1.052459</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>4.182753</td>\n",
              "      <td>3.248100</td>\n",
              "      <td>2.907835</td>\n",
              "      <td>1.269136</td>\n",
              "      <td>1.658121</td>\n",
              "      <td>-0.139948</td>\n",
              "      <td>2.803441</td>\n",
              "      <td>2.200916</td>\n",
              "      <td>3.240786</td>\n",
              "      <td>1.340838</td>\n",
              "      <td>1.837419</td>\n",
              "      <td>1.011658</td>\n",
              "      <td>1.782386</td>\n",
              "      <td>4.994478</td>\n",
              "      <td>1.539771</td>\n",
              "      <td>0.995481</td>\n",
              "      <td>0.142082</td>\n",
              "      <td>-0.068094</td>\n",
              "      <td>-0.006632</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>-1.850991</td>\n",
              "      <td>-0.218487</td>\n",
              "      <td>0.510269</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-1.120981</td>\n",
              "      <td>-0.419754</td>\n",
              "      <td>-0.267619</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.617623</td>\n",
              "      <td>5.485736</td>\n",
              "      <td>-0.586651</td>\n",
              "      <td>-0.145570</td>\n",
              "      <td>-0.296446</td>\n",
              "      <td>2.007380</td>\n",
              "      <td>2.235102</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.199260</td>\n",
              "      <td>-0.699319</td>\n",
              "      <td>1.064861</td>\n",
              "      <td>-1.476203</td>\n",
              "      <td>-0.073931</td>\n",
              "      <td>0.721041</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.176314</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.559975</td>\n",
              "      <td>-0.542773</td>\n",
              "      <td>-0.429523</td>\n",
              "      <td>-0.369455</td>\n",
              "      <td>-0.343006</td>\n",
              "      <td>-0.344687</td>\n",
              "      <td>-0.470491</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.364113</td>\n",
              "      <td>-0.425220</td>\n",
              "      <td>-0.404459</td>\n",
              "      <td>-0.386575</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.333753</td>\n",
              "      <td>-0.521589</td>\n",
              "      <td>-0.600067</td>\n",
              "      <td>-0.045166</td>\n",
              "      <td>0.176241</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.299028</td>\n",
              "      <td>-0.318586</td>\n",
              "      <td>-0.162979</td>\n",
              "      <td>-0.320065</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.656221</td>\n",
              "      <td>-0.374227</td>\n",
              "      <td>-0.280531</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.489609</td>\n",
              "      <td>-0.479047</td>\n",
              "      <td>-0.398436</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.523064</td>\n",
              "      <td>2.689198</td>\n",
              "      <td>-1.262565</td>\n",
              "      <td>-0.069370</td>\n",
              "      <td>0.013249</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>-0.268453</td>\n",
              "      <td>-0.624333</td>\n",
              "      <td>-0.321363</td>\n",
              "      <td>-0.181425</td>\n",
              "      <td>-0.473486</td>\n",
              "      <td>-0.409776</td>\n",
              "      <td>-0.288412</td>\n",
              "      <td>-0.592813</td>\n",
              "      <td>-0.461741</td>\n",
              "      <td>-0.395665</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.470851</td>\n",
              "      <td>-0.545114</td>\n",
              "      <td>-0.534238</td>\n",
              "      <td>-0.390934</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.594872</td>\n",
              "      <td>-0.122700</td>\n",
              "      <td>-0.942236</td>\n",
              "      <td>-0.067309</td>\n",
              "      <td>-0.011152</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>1.298761</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>1.750047</td>\n",
              "      <td>3.342037</td>\n",
              "      <td>4.957269</td>\n",
              "      <td>2.576934</td>\n",
              "      <td>0.730973</td>\n",
              "      <td>0.203145</td>\n",
              "      <td>2.103118</td>\n",
              "      <td>2.246944</td>\n",
              "      <td>1.278456</td>\n",
              "      <td>0.360947</td>\n",
              "      <td>0.493706</td>\n",
              "      <td>1.686537</td>\n",
              "      <td>1.727151</td>\n",
              "      <td>0.100540</td>\n",
              "      <td>0.693487</td>\n",
              "      <td>1.951032</td>\n",
              "      <td>-0.225342</td>\n",
              "      <td>-0.491792</td>\n",
              "      <td>-0.065118</td>\n",
              "      <td>-0.043097</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>1.073779</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>1.750047</td>\n",
              "      <td>3.342037</td>\n",
              "      <td>4.957269</td>\n",
              "      <td>2.576934</td>\n",
              "      <td>0.730973</td>\n",
              "      <td>0.203145</td>\n",
              "      <td>2.103118</td>\n",
              "      <td>2.246944</td>\n",
              "      <td>1.278456</td>\n",
              "      <td>0.360947</td>\n",
              "      <td>0.493706</td>\n",
              "      <td>1.686537</td>\n",
              "      <td>1.727151</td>\n",
              "      <td>0.100540</td>\n",
              "      <td>0.693487</td>\n",
              "      <td>1.951032</td>\n",
              "      <td>-0.225342</td>\n",
              "      <td>-0.491792</td>\n",
              "      <td>-0.065118</td>\n",
              "      <td>-0.043097</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>1.186270</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.419623</td>\n",
              "      <td>-0.309213</td>\n",
              "      <td>-0.043948</td>\n",
              "      <td>-0.408216</td>\n",
              "      <td>-0.011969</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.578941</td>\n",
              "      <td>-0.460492</td>\n",
              "      <td>-0.391604</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.258870</td>\n",
              "      <td>-0.565662</td>\n",
              "      <td>-0.555416</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.627512</td>\n",
              "      <td>-0.027630</td>\n",
              "      <td>-0.775932</td>\n",
              "      <td>-0.067982</td>\n",
              "      <td>-0.018548</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>0.173850</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.274827</td>\n",
              "      <td>-0.231397</td>\n",
              "      <td>-0.543060</td>\n",
              "      <td>-0.463111</td>\n",
              "      <td>-0.359294</td>\n",
              "      <td>-0.500703</td>\n",
              "      <td>-0.095122</td>\n",
              "      <td>-0.800930</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.447377</td>\n",
              "      <td>-0.620219</td>\n",
              "      <td>-0.616816</td>\n",
              "      <td>-0.372045</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.581816</td>\n",
              "      <td>-0.476268</td>\n",
              "      <td>-0.745085</td>\n",
              "      <td>-0.074851</td>\n",
              "      <td>0.005523</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>5.196472</td>\n",
              "      <td>-0.466662</td>\n",
              "      <td>-0.109509</td>\n",
              "      <td>-0.320028</td>\n",
              "      <td>-0.623505</td>\n",
              "      <td>-0.218111</td>\n",
              "      <td>-0.061418</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.287614</td>\n",
              "      <td>-0.214733</td>\n",
              "      <td>-0.179681</td>\n",
              "      <td>-0.351132</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.346809</td>\n",
              "      <td>-0.241468</td>\n",
              "      <td>1.936501</td>\n",
              "      <td>-0.069503</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>-0.248149</td>\n",
              "      <td>-0.417138</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.109348</td>\n",
              "      <td>-0.489400</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.365731</td>\n",
              "      <td>-0.441410</td>\n",
              "      <td>-0.604626</td>\n",
              "      <td>-0.394086</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.287090</td>\n",
              "      <td>-0.530844</td>\n",
              "      <td>-0.520093</td>\n",
              "      <td>-0.368411</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.405561</td>\n",
              "      <td>-0.434739</td>\n",
              "      <td>-0.636616</td>\n",
              "      <td>-0.064245</td>\n",
              "      <td>0.145468</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>-1.063553</td>\n",
              "      <td>-0.244845</td>\n",
              "      <td>-0.270963</td>\n",
              "      <td>-0.332861</td>\n",
              "      <td>-0.586632</td>\n",
              "      <td>-0.424019</td>\n",
              "      <td>-0.427346</td>\n",
              "      <td>-0.392515</td>\n",
              "      <td>-0.366353</td>\n",
              "      <td>-0.317212</td>\n",
              "      <td>-0.419851</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.364707</td>\n",
              "      <td>-0.426090</td>\n",
              "      <td>-0.404163</td>\n",
              "      <td>-0.407523</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.503480</td>\n",
              "      <td>-0.397677</td>\n",
              "      <td>-0.683731</td>\n",
              "      <td>-0.062139</td>\n",
              "      <td>0.062924</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.265386</td>\n",
              "      <td>-0.270803</td>\n",
              "      <td>-0.112863</td>\n",
              "      <td>0.015877</td>\n",
              "      <td>-0.408655</td>\n",
              "      <td>-0.475285</td>\n",
              "      <td>-0.377525</td>\n",
              "      <td>-0.691559</td>\n",
              "      <td>-0.192935</td>\n",
              "      <td>-0.204594</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.462520</td>\n",
              "      <td>-0.345180</td>\n",
              "      <td>-0.318589</td>\n",
              "      <td>-0.391514</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.562232</td>\n",
              "      <td>5.180161</td>\n",
              "      <td>-0.538214</td>\n",
              "      <td>-0.070120</td>\n",
              "      <td>-0.036792</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.261832</td>\n",
              "      <td>-0.553698</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.222512</td>\n",
              "      <td>-0.480145</td>\n",
              "      <td>-0.477915</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.672381</td>\n",
              "      <td>-0.521158</td>\n",
              "      <td>-0.383395</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.454018</td>\n",
              "      <td>-0.588460</td>\n",
              "      <td>-0.581255</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.784183</td>\n",
              "      <td>0.364460</td>\n",
              "      <td>0.507802</td>\n",
              "      <td>-0.069750</td>\n",
              "      <td>0.003322</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>0.511323</td>\n",
              "      <td>-0.223136</td>\n",
              "      <td>-0.407947</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.219749</td>\n",
              "      <td>-0.570495</td>\n",
              "      <td>-0.473930</td>\n",
              "      <td>-0.374350</td>\n",
              "      <td>-0.548668</td>\n",
              "      <td>-0.428927</td>\n",
              "      <td>-0.391089</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.440514</td>\n",
              "      <td>-0.532212</td>\n",
              "      <td>-0.522652</td>\n",
              "      <td>-0.388124</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.660151</td>\n",
              "      <td>-0.503720</td>\n",
              "      <td>-0.862016</td>\n",
              "      <td>-0.061212</td>\n",
              "      <td>-1.222295</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>-1.850991</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.570503</td>\n",
              "      <td>-0.244432</td>\n",
              "      <td>-0.142689</td>\n",
              "      <td>-0.047746</td>\n",
              "      <td>0.116655</td>\n",
              "      <td>-0.178593</td>\n",
              "      <td>-0.444182</td>\n",
              "      <td>-0.257463</td>\n",
              "      <td>-0.132728</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.214740</td>\n",
              "      <td>-0.371800</td>\n",
              "      <td>-0.409119</td>\n",
              "      <td>-0.344533</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.075977</td>\n",
              "      <td>0.021466</td>\n",
              "      <td>-0.223969</td>\n",
              "      <td>-0.066651</td>\n",
              "      <td>0.015553</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.072101</td>\n",
              "      <td>0.187164</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-1.317488</td>\n",
              "      <td>-0.391010</td>\n",
              "      <td>0.152841</td>\n",
              "      <td>-0.338357</td>\n",
              "      <td>-0.531771</td>\n",
              "      <td>-0.006188</td>\n",
              "      <td>0.096145</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.083501</td>\n",
              "      <td>0.057283</td>\n",
              "      <td>0.116688</td>\n",
              "      <td>-0.349687</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.235834</td>\n",
              "      <td>-0.195802</td>\n",
              "      <td>-0.509256</td>\n",
              "      <td>-0.069210</td>\n",
              "      <td>-0.101352</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>-0.163623</td>\n",
              "      <td>-0.159858</td>\n",
              "      <td>0.208986</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.673799</td>\n",
              "      <td>-0.302996</td>\n",
              "      <td>-0.244908</td>\n",
              "      <td>-0.057491</td>\n",
              "      <td>0.268203</td>\n",
              "      <td>-0.055426</td>\n",
              "      <td>-0.271723</td>\n",
              "      <td>-0.202645</td>\n",
              "      <td>-0.196420</td>\n",
              "      <td>0.156052</td>\n",
              "      <td>0.088857</td>\n",
              "      <td>-0.368919</td>\n",
              "      <td>0.139917</td>\n",
              "      <td>0.279875</td>\n",
              "      <td>-0.470513</td>\n",
              "      <td>-0.666217</td>\n",
              "      <td>-0.057628</td>\n",
              "      <td>0.210694</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>-0.276115</td>\n",
              "      <td>2.986397</td>\n",
              "      <td>-0.103229</td>\n",
              "      <td>2.798688</td>\n",
              "      <td>6.329799</td>\n",
              "      <td>3.960875</td>\n",
              "      <td>2.624268</td>\n",
              "      <td>-0.394371</td>\n",
              "      <td>1.201830</td>\n",
              "      <td>5.931218</td>\n",
              "      <td>2.467333</td>\n",
              "      <td>-0.106336</td>\n",
              "      <td>3.072000</td>\n",
              "      <td>1.678778</td>\n",
              "      <td>1.794618</td>\n",
              "      <td>-0.409504</td>\n",
              "      <td>-0.130109</td>\n",
              "      <td>4.372905</td>\n",
              "      <td>0.371772</td>\n",
              "      <td>-0.950464</td>\n",
              "      <td>-0.064066</td>\n",
              "      <td>-0.064651</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>-0.246525</td>\n",
              "      <td>-0.366468</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.065045</td>\n",
              "      <td>-0.530431</td>\n",
              "      <td>-0.463760</td>\n",
              "      <td>-0.340703</td>\n",
              "      <td>-0.270630</td>\n",
              "      <td>-0.397613</td>\n",
              "      <td>-0.281039</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.431463</td>\n",
              "      <td>-0.278492</td>\n",
              "      <td>-0.252290</td>\n",
              "      <td>-0.365013</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.405561</td>\n",
              "      <td>-0.539233</td>\n",
              "      <td>1.525804</td>\n",
              "      <td>-0.067994</td>\n",
              "      <td>-0.022091</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.210959</td>\n",
              "      <td>0.029199</td>\n",
              "      <td>1.468136</td>\n",
              "      <td>0.186066</td>\n",
              "      <td>-0.083141</td>\n",
              "      <td>-0.276255</td>\n",
              "      <td>-0.290702</td>\n",
              "      <td>0.326127</td>\n",
              "      <td>-0.501174</td>\n",
              "      <td>0.198979</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.355015</td>\n",
              "      <td>0.036353</td>\n",
              "      <td>0.092412</td>\n",
              "      <td>-0.344485</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.279875</td>\n",
              "      <td>-0.087410</td>\n",
              "      <td>-1.560450</td>\n",
              "      <td>-0.068782</td>\n",
              "      <td>0.011950</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.480441</td>\n",
              "      <td>-0.129625</td>\n",
              "      <td>-0.018631</td>\n",
              "      <td>-0.023323</td>\n",
              "      <td>-0.432161</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>-0.559698</td>\n",
              "      <td>-0.220341</td>\n",
              "      <td>-0.163708</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.410469</td>\n",
              "      <td>-0.402593</td>\n",
              "      <td>-0.378740</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.490424</td>\n",
              "      <td>0.561579</td>\n",
              "      <td>-0.253529</td>\n",
              "      <td>-0.068755</td>\n",
              "      <td>-0.038265</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>-0.225206</td>\n",
              "      <td>-0.023005</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.287556</td>\n",
              "      <td>-0.174512</td>\n",
              "      <td>-0.461534</td>\n",
              "      <td>-0.276598</td>\n",
              "      <td>-0.476251</td>\n",
              "      <td>0.218592</td>\n",
              "      <td>-0.062051</td>\n",
              "      <td>-0.004969</td>\n",
              "      <td>-0.443701</td>\n",
              "      <td>-0.003260</td>\n",
              "      <td>0.045367</td>\n",
              "      <td>-0.328924</td>\n",
              "      <td>-0.067567</td>\n",
              "      <td>0.364738</td>\n",
              "      <td>-0.181250</td>\n",
              "      <td>1.334954</td>\n",
              "      <td>-0.068572</td>\n",
              "      <td>-0.023043</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>-0.176455</td>\n",
              "      <td>-0.411866</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.274337</td>\n",
              "      <td>-0.503588</td>\n",
              "      <td>-0.332624</td>\n",
              "      <td>-0.309001</td>\n",
              "      <td>-0.228747</td>\n",
              "      <td>-0.374440</td>\n",
              "      <td>-0.097382</td>\n",
              "      <td>0.534097</td>\n",
              "      <td>-0.309497</td>\n",
              "      <td>-0.110133</td>\n",
              "      <td>-0.069247</td>\n",
              "      <td>-0.318337</td>\n",
              "      <td>0.998666</td>\n",
              "      <td>-0.797238</td>\n",
              "      <td>-0.523936</td>\n",
              "      <td>0.430761</td>\n",
              "      <td>-0.072082</td>\n",
              "      <td>-0.055591</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>0.275351</td>\n",
              "      <td>-0.192384</td>\n",
              "      <td>0.243797</td>\n",
              "      <td>0.519144</td>\n",
              "      <td>0.519931</td>\n",
              "      <td>0.847836</td>\n",
              "      <td>-0.194838</td>\n",
              "      <td>-0.229713</td>\n",
              "      <td>0.388180</td>\n",
              "      <td>-0.259216</td>\n",
              "      <td>0.228915</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.248964</td>\n",
              "      <td>-0.097192</td>\n",
              "      <td>-0.068707</td>\n",
              "      <td>-0.235767</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.367938</td>\n",
              "      <td>-0.332107</td>\n",
              "      <td>-0.263765</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>-0.492892</td>\n",
              "      <td>-0.224112</td>\n",
              "      <td>0.932381</td>\n",
              "      <td>-0.277933</td>\n",
              "      <td>1.192187</td>\n",
              "      <td>-0.200995</td>\n",
              "      <td>-0.427767</td>\n",
              "      <td>-0.353795</td>\n",
              "      <td>-0.325056</td>\n",
              "      <td>-0.094966</td>\n",
              "      <td>0.139853</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.418251</td>\n",
              "      <td>-0.071972</td>\n",
              "      <td>-0.027105</td>\n",
              "      <td>-0.336954</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.093742</td>\n",
              "      <td>-0.269265</td>\n",
              "      <td>0.096115</td>\n",
              "      <td>-0.021253</td>\n",
              "      <td>-7.494783</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>0.574913</td>\n",
              "      <td>-0.273772</td>\n",
              "      <td>-0.356982</td>\n",
              "      <td>-0.302943</td>\n",
              "      <td>-0.239795</td>\n",
              "      <td>-0.327756</td>\n",
              "      <td>-0.212627</td>\n",
              "      <td>-0.394514</td>\n",
              "      <td>-0.510884</td>\n",
              "      <td>-0.413459</td>\n",
              "      <td>-0.280469</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.420745</td>\n",
              "      <td>-0.457494</td>\n",
              "      <td>-0.438659</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.337904</td>\n",
              "      <td>0.318380</td>\n",
              "      <td>-0.727056</td>\n",
              "      <td>-0.069267</td>\n",
              "      <td>0.012690</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>0.898485</td>\n",
              "      <td>-0.154753</td>\n",
              "      <td>-0.140580</td>\n",
              "      <td>-0.248879</td>\n",
              "      <td>0.208266</td>\n",
              "      <td>-0.420582</td>\n",
              "      <td>-0.304368</td>\n",
              "      <td>-0.362794</td>\n",
              "      <td>-0.452545</td>\n",
              "      <td>-0.105322</td>\n",
              "      <td>0.155316</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.296589</td>\n",
              "      <td>0.048679</td>\n",
              "      <td>0.102890</td>\n",
              "      <td>-0.292335</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.013096</td>\n",
              "      <td>-0.339893</td>\n",
              "      <td>-0.539031</td>\n",
              "      <td>-0.070363</td>\n",
              "      <td>0.041829</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>0.375277</td>\n",
              "      <td>-0.048023</td>\n",
              "      <td>0.127303</td>\n",
              "      <td>-0.109373</td>\n",
              "      <td>1.004651</td>\n",
              "      <td>-0.235214</td>\n",
              "      <td>-0.314357</td>\n",
              "      <td>-0.339055</td>\n",
              "      <td>-0.455999</td>\n",
              "      <td>0.123755</td>\n",
              "      <td>0.782714</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.321701</td>\n",
              "      <td>0.503919</td>\n",
              "      <td>0.583997</td>\n",
              "      <td>-0.111448</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.830973</td>\n",
              "      <td>-0.088059</td>\n",
              "      <td>-0.599041</td>\n",
              "      <td>-0.070273</td>\n",
              "      <td>-0.037015</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>-0.146566</td>\n",
              "      <td>-0.068615</td>\n",
              "      <td>0.114486</td>\n",
              "      <td>-0.092713</td>\n",
              "      <td>1.025361</td>\n",
              "      <td>-0.177553</td>\n",
              "      <td>-0.363755</td>\n",
              "      <td>-0.186474</td>\n",
              "      <td>-0.405431</td>\n",
              "      <td>-0.016344</td>\n",
              "      <td>0.709668</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.369566</td>\n",
              "      <td>0.381259</td>\n",
              "      <td>0.439183</td>\n",
              "      <td>0.004670</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.820575</td>\n",
              "      <td>-0.074587</td>\n",
              "      <td>-0.433743</td>\n",
              "      <td>-0.069574</td>\n",
              "      <td>-0.045768</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>-0.125340</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.369180</td>\n",
              "      <td>-0.307455</td>\n",
              "      <td>0.434394</td>\n",
              "      <td>-0.197188</td>\n",
              "      <td>-0.241281</td>\n",
              "      <td>-0.378793</td>\n",
              "      <td>-0.366081</td>\n",
              "      <td>-0.258801</td>\n",
              "      <td>-0.155604</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.416185</td>\n",
              "      <td>-0.335355</td>\n",
              "      <td>-0.307274</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.122229</td>\n",
              "      <td>0.270666</td>\n",
              "      <td>-0.716580</td>\n",
              "      <td>-0.041888</td>\n",
              "      <td>-4.162989</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>-0.155691</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.412913</td>\n",
              "      <td>-0.309035</td>\n",
              "      <td>0.477571</td>\n",
              "      <td>-0.198467</td>\n",
              "      <td>-0.254328</td>\n",
              "      <td>-0.377631</td>\n",
              "      <td>-0.361730</td>\n",
              "      <td>-0.253438</td>\n",
              "      <td>-0.151035</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.417416</td>\n",
              "      <td>-0.330476</td>\n",
              "      <td>-0.302037</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.125638</td>\n",
              "      <td>0.232604</td>\n",
              "      <td>-0.656162</td>\n",
              "      <td>-0.039943</td>\n",
              "      <td>-4.470419</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>0.705595</td>\n",
              "      <td>-0.165492</td>\n",
              "      <td>-0.268685</td>\n",
              "      <td>-0.220715</td>\n",
              "      <td>0.289147</td>\n",
              "      <td>-0.416371</td>\n",
              "      <td>-0.414954</td>\n",
              "      <td>-0.369126</td>\n",
              "      <td>-0.586049</td>\n",
              "      <td>-0.258217</td>\n",
              "      <td>0.173879</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.413596</td>\n",
              "      <td>-0.070080</td>\n",
              "      <td>-0.028178</td>\n",
              "      <td>-0.261764</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.016785</td>\n",
              "      <td>-0.289924</td>\n",
              "      <td>0.196378</td>\n",
              "      <td>-0.070699</td>\n",
              "      <td>-0.016315</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>0.314931</td>\n",
              "      <td>-0.055427</td>\n",
              "      <td>0.068905</td>\n",
              "      <td>-0.102440</td>\n",
              "      <td>1.012776</td>\n",
              "      <td>-0.239075</td>\n",
              "      <td>-0.357586</td>\n",
              "      <td>-0.342254</td>\n",
              "      <td>-0.508444</td>\n",
              "      <td>0.056755</td>\n",
              "      <td>0.771341</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.367008</td>\n",
              "      <td>0.443622</td>\n",
              "      <td>0.518085</td>\n",
              "      <td>-0.104801</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.804393</td>\n",
              "      <td>-0.075888</td>\n",
              "      <td>-0.307795</td>\n",
              "      <td>-0.070408</td>\n",
              "      <td>-0.057554</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>0.381741</td>\n",
              "      <td>-0.264908</td>\n",
              "      <td>-0.459079</td>\n",
              "      <td>-0.295728</td>\n",
              "      <td>-0.232285</td>\n",
              "      <td>-0.466326</td>\n",
              "      <td>-0.465832</td>\n",
              "      <td>-0.181717</td>\n",
              "      <td>-0.506622</td>\n",
              "      <td>-0.589792</td>\n",
              "      <td>-0.352580</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.451651</td>\n",
              "      <td>-0.535163</td>\n",
              "      <td>-0.539496</td>\n",
              "      <td>-0.232760</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.605976</td>\n",
              "      <td>-0.447377</td>\n",
              "      <td>0.403671</td>\n",
              "      <td>-0.069806</td>\n",
              "      <td>0.030045</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>0.281592</td>\n",
              "      <td>-0.270230</td>\n",
              "      <td>-0.213206</td>\n",
              "      <td>-0.280533</td>\n",
              "      <td>-0.116780</td>\n",
              "      <td>-0.178661</td>\n",
              "      <td>-0.069720</td>\n",
              "      <td>-0.393078</td>\n",
              "      <td>-0.425793</td>\n",
              "      <td>-0.319035</td>\n",
              "      <td>-0.188512</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.398158</td>\n",
              "      <td>-0.376664</td>\n",
              "      <td>-0.351894</td>\n",
              "      <td>-0.401109</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.046144</td>\n",
              "      <td>0.771662</td>\n",
              "      <td>-1.517062</td>\n",
              "      <td>-0.068320</td>\n",
              "      <td>0.005059</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>-0.175555</td>\n",
              "      <td>-0.267317</td>\n",
              "      <td>-0.250342</td>\n",
              "      <td>-0.264096</td>\n",
              "      <td>-0.074324</td>\n",
              "      <td>-0.240410</td>\n",
              "      <td>-0.268044</td>\n",
              "      <td>-0.128272</td>\n",
              "      <td>-0.354316</td>\n",
              "      <td>-0.489727</td>\n",
              "      <td>-0.259829</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.421938</td>\n",
              "      <td>-0.456053</td>\n",
              "      <td>-0.458441</td>\n",
              "      <td>-0.188269</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.211633</td>\n",
              "      <td>0.186096</td>\n",
              "      <td>-0.738228</td>\n",
              "      <td>-0.068200</td>\n",
              "      <td>0.024354</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>0.978712</td>\n",
              "      <td>-0.242439</td>\n",
              "      <td>-0.504698</td>\n",
              "      <td>-0.303401</td>\n",
              "      <td>-0.216749</td>\n",
              "      <td>-0.540320</td>\n",
              "      <td>-0.455060</td>\n",
              "      <td>-0.387912</td>\n",
              "      <td>-0.640303</td>\n",
              "      <td>-0.478418</td>\n",
              "      <td>-0.243813</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.446167</td>\n",
              "      <td>-0.429215</td>\n",
              "      <td>-0.410077</td>\n",
              "      <td>-0.371498</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.590878</td>\n",
              "      <td>-0.439559</td>\n",
              "      <td>0.548851</td>\n",
              "      <td>-0.070903</td>\n",
              "      <td>0.012515</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>0.119395</td>\n",
              "      <td>-0.214130</td>\n",
              "      <td>0.176457</td>\n",
              "      <td>0.615603</td>\n",
              "      <td>0.537234</td>\n",
              "      <td>0.982476</td>\n",
              "      <td>-0.256518</td>\n",
              "      <td>-0.220612</td>\n",
              "      <td>0.396205</td>\n",
              "      <td>-0.403127</td>\n",
              "      <td>0.191507</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.320890</td>\n",
              "      <td>-0.238028</td>\n",
              "      <td>-0.223392</td>\n",
              "      <td>-0.225181</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.491187</td>\n",
              "      <td>-0.320655</td>\n",
              "      <td>0.276243</td>\n",
              "      <td>-0.069170</td>\n",
              "      <td>-0.025617</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>-0.298542</td>\n",
              "      <td>-0.264898</td>\n",
              "      <td>-0.257352</td>\n",
              "      <td>-0.258256</td>\n",
              "      <td>-0.053838</td>\n",
              "      <td>-0.256538</td>\n",
              "      <td>-0.322290</td>\n",
              "      <td>-0.057371</td>\n",
              "      <td>-0.335470</td>\n",
              "      <td>-0.531845</td>\n",
              "      <td>-0.271081</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.427917</td>\n",
              "      <td>-0.470567</td>\n",
              "      <td>-0.479780</td>\n",
              "      <td>-0.129311</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.247754</td>\n",
              "      <td>0.026110</td>\n",
              "      <td>-0.525786</td>\n",
              "      <td>-0.068180</td>\n",
              "      <td>0.028979</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>0.787043</td>\n",
              "      <td>-0.132020</td>\n",
              "      <td>-0.083522</td>\n",
              "      <td>-0.219165</td>\n",
              "      <td>0.377894</td>\n",
              "      <td>-0.381099</td>\n",
              "      <td>-0.306496</td>\n",
              "      <td>-0.357738</td>\n",
              "      <td>-0.453281</td>\n",
              "      <td>-0.056529</td>\n",
              "      <td>0.288950</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.301938</td>\n",
              "      <td>0.145644</td>\n",
              "      <td>0.205364</td>\n",
              "      <td>-0.253806</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.166688</td>\n",
              "      <td>-0.286253</td>\n",
              "      <td>-0.551813</td>\n",
              "      <td>-0.070344</td>\n",
              "      <td>0.025035</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>0.547043</td>\n",
              "      <td>-0.274026</td>\n",
              "      <td>-0.344140</td>\n",
              "      <td>-0.301309</td>\n",
              "      <td>-0.231245</td>\n",
              "      <td>-0.313470</td>\n",
              "      <td>-0.198304</td>\n",
              "      <td>-0.394517</td>\n",
              "      <td>-0.502599</td>\n",
              "      <td>-0.405548</td>\n",
              "      <td>-0.274395</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.418691</td>\n",
              "      <td>-0.452091</td>\n",
              "      <td>-0.432830</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.312608</td>\n",
              "      <td>0.363633</td>\n",
              "      <td>-0.805190</td>\n",
              "      <td>-0.069171</td>\n",
              "      <td>0.012139</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.261105</td>\n",
              "      <td>-0.092207</td>\n",
              "      <td>-0.255464</td>\n",
              "      <td>0.152176</td>\n",
              "      <td>-0.286972</td>\n",
              "      <td>-0.459877</td>\n",
              "      <td>0.058459</td>\n",
              "      <td>-0.288401</td>\n",
              "      <td>-0.576274</td>\n",
              "      <td>-0.270320</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.442250</td>\n",
              "      <td>-0.473669</td>\n",
              "      <td>-0.492144</td>\n",
              "      <td>-0.036883</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.318114</td>\n",
              "      <td>-0.377994</td>\n",
              "      <td>0.050589</td>\n",
              "      <td>-0.060311</td>\n",
              "      <td>-1.201952</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>0.113146</td>\n",
              "      <td>-0.051296</td>\n",
              "      <td>0.170164</td>\n",
              "      <td>-0.085601</td>\n",
              "      <td>1.106409</td>\n",
              "      <td>-0.136850</td>\n",
              "      <td>-0.261415</td>\n",
              "      <td>-0.340865</td>\n",
              "      <td>-0.450481</td>\n",
              "      <td>0.124757</td>\n",
              "      <td>0.842244</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.351204</td>\n",
              "      <td>0.505761</td>\n",
              "      <td>0.584704</td>\n",
              "      <td>-0.096553</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.012065</td>\n",
              "      <td>0.229670</td>\n",
              "      <td>-0.842450</td>\n",
              "      <td>-0.069772</td>\n",
              "      <td>-0.063303</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>-0.275570</td>\n",
              "      <td>-0.077751</td>\n",
              "      <td>0.524976</td>\n",
              "      <td>0.657819</td>\n",
              "      <td>1.367196</td>\n",
              "      <td>1.028712</td>\n",
              "      <td>-0.207015</td>\n",
              "      <td>-0.206148</td>\n",
              "      <td>0.373003</td>\n",
              "      <td>-0.012177</td>\n",
              "      <td>0.898745</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.276466</td>\n",
              "      <td>0.391559</td>\n",
              "      <td>0.448053</td>\n",
              "      <td>-0.043130</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.539418</td>\n",
              "      <td>-0.062943</td>\n",
              "      <td>-0.331689</td>\n",
              "      <td>-0.069004</td>\n",
              "      <td>-0.073483</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.221515</td>\n",
              "      <td>0.168949</td>\n",
              "      <td>0.435853</td>\n",
              "      <td>0.534880</td>\n",
              "      <td>0.764861</td>\n",
              "      <td>-0.303773</td>\n",
              "      <td>0.033793</td>\n",
              "      <td>0.352214</td>\n",
              "      <td>-0.488525</td>\n",
              "      <td>0.101303</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.345965</td>\n",
              "      <td>-0.288373</td>\n",
              "      <td>-0.296254</td>\n",
              "      <td>-0.023303</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.306412</td>\n",
              "      <td>-0.300517</td>\n",
              "      <td>-0.001179</td>\n",
              "      <td>-0.067980</td>\n",
              "      <td>-0.003445</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>0.078056</td>\n",
              "      <td>-0.252197</td>\n",
              "      <td>0.004355</td>\n",
              "      <td>0.091124</td>\n",
              "      <td>0.173412</td>\n",
              "      <td>0.367526</td>\n",
              "      <td>-0.071027</td>\n",
              "      <td>-0.323392</td>\n",
              "      <td>-0.047409</td>\n",
              "      <td>-0.321106</td>\n",
              "      <td>-0.019667</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.357008</td>\n",
              "      <td>-0.307492</td>\n",
              "      <td>-0.285632</td>\n",
              "      <td>-0.334192</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.118834</td>\n",
              "      <td>0.560987</td>\n",
              "      <td>-1.186892</td>\n",
              "      <td>-0.068165</td>\n",
              "      <td>-0.009122</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>1.147030</td>\n",
              "      <td>-0.216204</td>\n",
              "      <td>-0.285139</td>\n",
              "      <td>-0.323887</td>\n",
              "      <td>-0.226354</td>\n",
              "      <td>-0.507598</td>\n",
              "      <td>-0.288646</td>\n",
              "      <td>-0.376667</td>\n",
              "      <td>-0.449531</td>\n",
              "      <td>-0.235694</td>\n",
              "      <td>-0.190989</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.287865</td>\n",
              "      <td>-0.210434</td>\n",
              "      <td>-0.171181</td>\n",
              "      <td>-0.392608</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.459866</td>\n",
              "      <td>-0.423238</td>\n",
              "      <td>-0.550938</td>\n",
              "      <td>-0.070319</td>\n",
              "      <td>0.081786</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>0.057681</td>\n",
              "      <td>0.014421</td>\n",
              "      <td>0.292305</td>\n",
              "      <td>-0.027345</td>\n",
              "      <td>1.480545</td>\n",
              "      <td>-0.124506</td>\n",
              "      <td>-0.321063</td>\n",
              "      <td>-0.325004</td>\n",
              "      <td>-0.457189</td>\n",
              "      <td>0.258834</td>\n",
              "      <td>1.152379</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.337309</td>\n",
              "      <td>0.771423</td>\n",
              "      <td>0.866684</td>\n",
              "      <td>-0.005136</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.329151</td>\n",
              "      <td>0.060833</td>\n",
              "      <td>-0.630193</td>\n",
              "      <td>-0.069893</td>\n",
              "      <td>-0.133498</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>0.179381</td>\n",
              "      <td>-0.263646</td>\n",
              "      <td>-0.421717</td>\n",
              "      <td>-0.285069</td>\n",
              "      <td>-0.185335</td>\n",
              "      <td>-0.432074</td>\n",
              "      <td>-0.464900</td>\n",
              "      <td>-0.119511</td>\n",
              "      <td>-0.462010</td>\n",
              "      <td>-0.599946</td>\n",
              "      <td>-0.341870</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.449940</td>\n",
              "      <td>-0.529589</td>\n",
              "      <td>-0.538469</td>\n",
              "      <td>-0.181023</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.551958</td>\n",
              "      <td>-0.434433</td>\n",
              "      <td>0.325343</td>\n",
              "      <td>-0.069464</td>\n",
              "      <td>0.032237</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>0.034757</td>\n",
              "      <td>-0.271454</td>\n",
              "      <td>-0.238354</td>\n",
              "      <td>-0.274082</td>\n",
              "      <td>-0.109356</td>\n",
              "      <td>-0.212831</td>\n",
              "      <td>-0.175282</td>\n",
              "      <td>-0.249514</td>\n",
              "      <td>-0.386544</td>\n",
              "      <td>-0.417704</td>\n",
              "      <td>-0.240588</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.411713</td>\n",
              "      <td>-0.431232</td>\n",
              "      <td>-0.421950</td>\n",
              "      <td>-0.289089</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.149866</td>\n",
              "      <td>0.459674</td>\n",
              "      <td>-1.101508</td>\n",
              "      <td>-0.068236</td>\n",
              "      <td>0.016446</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>1.055211</td>\n",
              "      <td>-0.186724</td>\n",
              "      <td>-0.220824</td>\n",
              "      <td>-0.290668</td>\n",
              "      <td>-0.030290</td>\n",
              "      <td>-0.476108</td>\n",
              "      <td>-0.301376</td>\n",
              "      <td>-0.369905</td>\n",
              "      <td>-0.451511</td>\n",
              "      <td>-0.173942</td>\n",
              "      <td>-0.032620</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.289067</td>\n",
              "      <td>-0.087688</td>\n",
              "      <td>-0.041225</td>\n",
              "      <td>-0.346519</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.265937</td>\n",
              "      <td>-0.415330</td>\n",
              "      <td>-0.521055</td>\n",
              "      <td>-0.070390</td>\n",
              "      <td>0.065447</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>0.118118</td>\n",
              "      <td>-0.057757</td>\n",
              "      <td>0.158850</td>\n",
              "      <td>-0.091354</td>\n",
              "      <td>1.070310</td>\n",
              "      <td>-0.138084</td>\n",
              "      <td>-0.255757</td>\n",
              "      <td>-0.342406</td>\n",
              "      <td>-0.449753</td>\n",
              "      <td>0.111660</td>\n",
              "      <td>0.811824</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.352590</td>\n",
              "      <td>0.479719</td>\n",
              "      <td>0.557063</td>\n",
              "      <td>-0.105541</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.980835</td>\n",
              "      <td>0.245666</td>\n",
              "      <td>-0.862360</td>\n",
              "      <td>-0.069729</td>\n",
              "      <td>-0.061286</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>1.079058</td>\n",
              "      <td>-0.266607</td>\n",
              "      <td>-0.572822</td>\n",
              "      <td>-0.331875</td>\n",
              "      <td>-0.385191</td>\n",
              "      <td>-0.580606</td>\n",
              "      <td>-0.461035</td>\n",
              "      <td>-0.393578</td>\n",
              "      <td>-0.649402</td>\n",
              "      <td>-0.539873</td>\n",
              "      <td>-0.379846</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.449304</td>\n",
              "      <td>-0.537681</td>\n",
              "      <td>-0.525117</td>\n",
              "      <td>-0.408857</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.776006</td>\n",
              "      <td>-0.491023</td>\n",
              "      <td>0.616280</td>\n",
              "      <td>-0.070948</td>\n",
              "      <td>0.025490</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>-0.390349</td>\n",
              "      <td>-0.167839</td>\n",
              "      <td>-0.088986</td>\n",
              "      <td>-0.171317</td>\n",
              "      <td>0.489418</td>\n",
              "      <td>-0.240348</td>\n",
              "      <td>-0.414645</td>\n",
              "      <td>-0.024277</td>\n",
              "      <td>-0.343711</td>\n",
              "      <td>-0.341752</td>\n",
              "      <td>0.182677</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.408020</td>\n",
              "      <td>-0.082799</td>\n",
              "      <td>-0.069045</td>\n",
              "      <td>0.013569</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.214614</td>\n",
              "      <td>-0.235948</td>\n",
              "      <td>-0.197723</td>\n",
              "      <td>-0.068818</td>\n",
              "      <td>-0.000567</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>-0.311728</td>\n",
              "      <td>-0.087813</td>\n",
              "      <td>0.550404</td>\n",
              "      <td>0.731169</td>\n",
              "      <td>1.354825</td>\n",
              "      <td>1.152417</td>\n",
              "      <td>-0.194852</td>\n",
              "      <td>-0.193413</td>\n",
              "      <td>0.462191</td>\n",
              "      <td>-0.041515</td>\n",
              "      <td>0.870799</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.269994</td>\n",
              "      <td>0.350188</td>\n",
              "      <td>0.402486</td>\n",
              "      <td>-0.047445</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.453781</td>\n",
              "      <td>-0.076463</td>\n",
              "      <td>-0.299134</td>\n",
              "      <td>-0.068873</td>\n",
              "      <td>-0.072320</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>0.352079</td>\n",
              "      <td>-0.272294</td>\n",
              "      <td>0.122951</td>\n",
              "      <td>-0.332476</td>\n",
              "      <td>0.258049</td>\n",
              "      <td>-0.427051</td>\n",
              "      <td>-0.461381</td>\n",
              "      <td>-0.379761</td>\n",
              "      <td>-0.503888</td>\n",
              "      <td>-0.391451</td>\n",
              "      <td>-0.257859</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.448304</td>\n",
              "      <td>-0.427039</td>\n",
              "      <td>-0.406156</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.528257</td>\n",
              "      <td>-0.427481</td>\n",
              "      <td>0.493527</td>\n",
              "      <td>-0.045147</td>\n",
              "      <td>-3.882875</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>0.065784</td>\n",
              "      <td>0.010263</td>\n",
              "      <td>0.277958</td>\n",
              "      <td>-0.030792</td>\n",
              "      <td>1.450338</td>\n",
              "      <td>-0.125094</td>\n",
              "      <td>-0.315314</td>\n",
              "      <td>-0.326184</td>\n",
              "      <td>-0.457423</td>\n",
              "      <td>0.249540</td>\n",
              "      <td>1.132066</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.338002</td>\n",
              "      <td>0.753876</td>\n",
              "      <td>0.848051</td>\n",
              "      <td>-0.010920</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.309605</td>\n",
              "      <td>0.077277</td>\n",
              "      <td>-0.652767</td>\n",
              "      <td>-0.070180</td>\n",
              "      <td>-0.082525</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>-0.562733</td>\n",
              "      <td>-0.157666</td>\n",
              "      <td>0.726925</td>\n",
              "      <td>1.240355</td>\n",
              "      <td>1.268948</td>\n",
              "      <td>2.011160</td>\n",
              "      <td>-0.110419</td>\n",
              "      <td>-0.105011</td>\n",
              "      <td>1.081325</td>\n",
              "      <td>-0.245173</td>\n",
              "      <td>0.676808</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.225070</td>\n",
              "      <td>0.063000</td>\n",
              "      <td>0.086167</td>\n",
              "      <td>-0.077402</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.140703</td>\n",
              "      <td>-0.170317</td>\n",
              "      <td>-0.073141</td>\n",
              "      <td>-0.067968</td>\n",
              "      <td>-0.064246</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>-0.338482</td>\n",
              "      <td>-0.264112</td>\n",
              "      <td>-0.259629</td>\n",
              "      <td>-0.256360</td>\n",
              "      <td>-0.047185</td>\n",
              "      <td>-0.261775</td>\n",
              "      <td>-0.339906</td>\n",
              "      <td>-0.034346</td>\n",
              "      <td>-0.329350</td>\n",
              "      <td>-0.545523</td>\n",
              "      <td>-0.274735</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.429859</td>\n",
              "      <td>-0.475281</td>\n",
              "      <td>-0.486710</td>\n",
              "      <td>-0.110164</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.259484</td>\n",
              "      <td>-0.025845</td>\n",
              "      <td>-0.456796</td>\n",
              "      <td>-0.068173</td>\n",
              "      <td>0.030481</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>0.842657</td>\n",
              "      <td>-0.205510</td>\n",
              "      <td>-0.087352</td>\n",
              "      <td>-0.006958</td>\n",
              "      <td>0.053024</td>\n",
              "      <td>-0.005614</td>\n",
              "      <td>-0.259630</td>\n",
              "      <td>-0.320726</td>\n",
              "      <td>-0.134228</td>\n",
              "      <td>-0.241845</td>\n",
              "      <td>-0.031840</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.270022</td>\n",
              "      <td>-0.162291</td>\n",
              "      <td>-0.126646</td>\n",
              "      <td>-0.332960</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.436090</td>\n",
              "      <td>-0.423224</td>\n",
              "      <td>-0.414640</td>\n",
              "      <td>-0.069911</td>\n",
              "      <td>0.056978</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>-0.371980</td>\n",
              "      <td>-0.199896</td>\n",
              "      <td>0.497784</td>\n",
              "      <td>0.906026</td>\n",
              "      <td>0.874929</td>\n",
              "      <td>1.550206</td>\n",
              "      <td>-0.085528</td>\n",
              "      <td>-0.169642</td>\n",
              "      <td>0.768650</td>\n",
              "      <td>-0.298547</td>\n",
              "      <td>0.407938</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.265237</td>\n",
              "      <td>-0.104282</td>\n",
              "      <td>-0.085412</td>\n",
              "      <td>-0.171054</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.211434</td>\n",
              "      <td>0.072773</td>\n",
              "      <td>-0.432409</td>\n",
              "      <td>-0.067918</td>\n",
              "      <td>-0.043925</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>-0.150376</td>\n",
              "      <td>-0.267812</td>\n",
              "      <td>-0.248907</td>\n",
              "      <td>-0.265292</td>\n",
              "      <td>-0.078518</td>\n",
              "      <td>-0.237108</td>\n",
              "      <td>-0.256938</td>\n",
              "      <td>-0.142787</td>\n",
              "      <td>-0.358175</td>\n",
              "      <td>-0.481105</td>\n",
              "      <td>-0.257525</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.420714</td>\n",
              "      <td>-0.453081</td>\n",
              "      <td>-0.454072</td>\n",
              "      <td>-0.200339</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.204238</td>\n",
              "      <td>0.218848</td>\n",
              "      <td>-0.781720</td>\n",
              "      <td>-0.068205</td>\n",
              "      <td>0.023408</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>0.579809</td>\n",
              "      <td>-0.273727</td>\n",
              "      <td>-0.359238</td>\n",
              "      <td>-0.303230</td>\n",
              "      <td>-0.241297</td>\n",
              "      <td>-0.330266</td>\n",
              "      <td>-0.215144</td>\n",
              "      <td>-0.394513</td>\n",
              "      <td>-0.512339</td>\n",
              "      <td>-0.414849</td>\n",
              "      <td>-0.281537</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.421106</td>\n",
              "      <td>-0.458444</td>\n",
              "      <td>-0.439684</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.342349</td>\n",
              "      <td>0.310430</td>\n",
              "      <td>-0.713329</td>\n",
              "      <td>-0.069283</td>\n",
              "      <td>0.012787</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>0.407790</td>\n",
              "      <td>-0.230778</td>\n",
              "      <td>-0.054199</td>\n",
              "      <td>0.329202</td>\n",
              "      <td>0.256184</td>\n",
              "      <td>0.509282</td>\n",
              "      <td>-0.320732</td>\n",
              "      <td>-0.273142</td>\n",
              "      <td>0.077287</td>\n",
              "      <td>-0.449040</td>\n",
              "      <td>0.016028</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.362172</td>\n",
              "      <td>-0.333576</td>\n",
              "      <td>-0.319974</td>\n",
              "      <td>-0.280937</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.581696</td>\n",
              "      <td>-0.372324</td>\n",
              "      <td>0.395694</td>\n",
              "      <td>-0.069715</td>\n",
              "      <td>-0.011062</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>0.810909</td>\n",
              "      <td>-0.270342</td>\n",
              "      <td>-0.328315</td>\n",
              "      <td>-0.332290</td>\n",
              "      <td>-0.155764</td>\n",
              "      <td>-0.526494</td>\n",
              "      <td>-0.466238</td>\n",
              "      <td>-0.389101</td>\n",
              "      <td>-0.602622</td>\n",
              "      <td>-0.495471</td>\n",
              "      <td>-0.341364</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.454153</td>\n",
              "      <td>-0.507898</td>\n",
              "      <td>-0.493162</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.695115</td>\n",
              "      <td>-0.468273</td>\n",
              "      <td>0.606702</td>\n",
              "      <td>-0.061567</td>\n",
              "      <td>-1.399950</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>-0.487628</td>\n",
              "      <td>-0.255534</td>\n",
              "      <td>-0.276193</td>\n",
              "      <td>-0.249058</td>\n",
              "      <td>-0.017345</td>\n",
              "      <td>-0.313575</td>\n",
              "      <td>-0.449884</td>\n",
              "      <td>0.089251</td>\n",
              "      <td>-0.298632</td>\n",
              "      <td>-0.611154</td>\n",
              "      <td>-0.292197</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.432004</td>\n",
              "      <td>-0.486331</td>\n",
              "      <td>-0.508360</td>\n",
              "      <td>-0.007224</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.349875</td>\n",
              "      <td>-0.390337</td>\n",
              "      <td>-0.018295</td>\n",
              "      <td>-0.068286</td>\n",
              "      <td>0.043930</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>-0.566061</td>\n",
              "      <td>-0.239356</td>\n",
              "      <td>-0.235642</td>\n",
              "      <td>-0.227973</td>\n",
              "      <td>0.103126</td>\n",
              "      <td>-0.285609</td>\n",
              "      <td>-0.451326</td>\n",
              "      <td>0.092629</td>\n",
              "      <td>-0.299226</td>\n",
              "      <td>-0.576296</td>\n",
              "      <td>-0.197163</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.435737</td>\n",
              "      <td>-0.417277</td>\n",
              "      <td>-0.435361</td>\n",
              "      <td>0.019982</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.222145</td>\n",
              "      <td>-0.352252</td>\n",
              "      <td>-0.027607</td>\n",
              "      <td>-0.068273</td>\n",
              "      <td>0.032013</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>-0.425576</td>\n",
              "      <td>-0.262399</td>\n",
              "      <td>-0.264593</td>\n",
              "      <td>-0.252225</td>\n",
              "      <td>-0.032677</td>\n",
              "      <td>-0.273196</td>\n",
              "      <td>-0.378321</td>\n",
              "      <td>0.015863</td>\n",
              "      <td>-0.316004</td>\n",
              "      <td>-0.575349</td>\n",
              "      <td>-0.282703</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.434093</td>\n",
              "      <td>-0.485560</td>\n",
              "      <td>-0.501821</td>\n",
              "      <td>-0.068413</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.285063</td>\n",
              "      <td>-0.139139</td>\n",
              "      <td>-0.306355</td>\n",
              "      <td>-0.068158</td>\n",
              "      <td>0.033756</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>0.890323</td>\n",
              "      <td>-0.234159</td>\n",
              "      <td>-0.266908</td>\n",
              "      <td>-0.312596</td>\n",
              "      <td>-0.203957</td>\n",
              "      <td>-0.409842</td>\n",
              "      <td>-0.221738</td>\n",
              "      <td>-0.382001</td>\n",
              "      <td>-0.442243</td>\n",
              "      <td>-0.264283</td>\n",
              "      <td>-0.198917</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.321156</td>\n",
              "      <td>-0.267433</td>\n",
              "      <td>-0.232955</td>\n",
              "      <td>-0.397705</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.345368</td>\n",
              "      <td>-0.062292</td>\n",
              "      <td>-0.844764</td>\n",
              "      <td>-0.069711</td>\n",
              "      <td>0.059476</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>-0.371521</td>\n",
              "      <td>-0.160175</td>\n",
              "      <td>-0.073271</td>\n",
              "      <td>-0.165246</td>\n",
              "      <td>0.530811</td>\n",
              "      <td>-0.235498</td>\n",
              "      <td>-0.410715</td>\n",
              "      <td>-0.036805</td>\n",
              "      <td>-0.348478</td>\n",
              "      <td>-0.316619</td>\n",
              "      <td>0.223379</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.405050</td>\n",
              "      <td>-0.046957</td>\n",
              "      <td>-0.029792</td>\n",
              "      <td>0.012881</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.261415</td>\n",
              "      <td>-0.223485</td>\n",
              "      <td>-0.215952</td>\n",
              "      <td>-0.068876</td>\n",
              "      <td>-0.004058</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>-0.156129</td>\n",
              "      <td>-0.182402</td>\n",
              "      <td>0.495661</td>\n",
              "      <td>0.919285</td>\n",
              "      <td>0.875050</td>\n",
              "      <td>1.496949</td>\n",
              "      <td>-0.145559</td>\n",
              "      <td>-0.160490</td>\n",
              "      <td>0.785511</td>\n",
              "      <td>-0.272428</td>\n",
              "      <td>0.427239</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.232949</td>\n",
              "      <td>-0.047679</td>\n",
              "      <td>-0.024641</td>\n",
              "      <td>-0.161844</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.316103</td>\n",
              "      <td>-0.262806</td>\n",
              "      <td>-0.149014</td>\n",
              "      <td>-0.068453</td>\n",
              "      <td>-0.025043</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>1.153053</td>\n",
              "      <td>-0.229926</td>\n",
              "      <td>-0.376195</td>\n",
              "      <td>-0.327553</td>\n",
              "      <td>-0.277929</td>\n",
              "      <td>-0.540532</td>\n",
              "      <td>-0.349116</td>\n",
              "      <td>-0.381344</td>\n",
              "      <td>-0.512226</td>\n",
              "      <td>-0.326922</td>\n",
              "      <td>-0.248667</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.334372</td>\n",
              "      <td>-0.305812</td>\n",
              "      <td>-0.274324</td>\n",
              "      <td>-0.397105</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.569895</td>\n",
              "      <td>-0.482320</td>\n",
              "      <td>-0.158300</td>\n",
              "      <td>-0.070578</td>\n",
              "      <td>0.066697</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>-0.395883</td>\n",
              "      <td>-0.111233</td>\n",
              "      <td>0.609587</td>\n",
              "      <td>0.901885</td>\n",
              "      <td>1.326033</td>\n",
              "      <td>1.440330</td>\n",
              "      <td>-0.166544</td>\n",
              "      <td>-0.163774</td>\n",
              "      <td>0.669770</td>\n",
              "      <td>-0.109796</td>\n",
              "      <td>0.805759</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.254933</td>\n",
              "      <td>0.253902</td>\n",
              "      <td>0.296433</td>\n",
              "      <td>-0.057489</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.254467</td>\n",
              "      <td>-0.107930</td>\n",
              "      <td>-0.223365</td>\n",
              "      <td>-0.068570</td>\n",
              "      <td>-0.069613</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>0.214569</td>\n",
              "      <td>-0.027151</td>\n",
              "      <td>0.155633</td>\n",
              "      <td>-0.072055</td>\n",
              "      <td>1.198679</td>\n",
              "      <td>-0.193527</td>\n",
              "      <td>-0.342848</td>\n",
              "      <td>-0.335351</td>\n",
              "      <td>-0.488507</td>\n",
              "      <td>0.137673</td>\n",
              "      <td>0.924832</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.355039</td>\n",
              "      <td>0.575594</td>\n",
              "      <td>0.658422</td>\n",
              "      <td>-0.064477</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.015356</td>\n",
              "      <td>-0.020902</td>\n",
              "      <td>-0.437319</td>\n",
              "      <td>-0.070333</td>\n",
              "      <td>-0.068148</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>0.163392</td>\n",
              "      <td>-0.262114</td>\n",
              "      <td>-0.089209</td>\n",
              "      <td>-0.063397</td>\n",
              "      <td>0.040391</td>\n",
              "      <td>0.143268</td>\n",
              "      <td>-0.068277</td>\n",
              "      <td>-0.352545</td>\n",
              "      <td>-0.202150</td>\n",
              "      <td>-0.325383</td>\n",
              "      <td>-0.100749</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.374410</td>\n",
              "      <td>-0.346024</td>\n",
              "      <td>-0.323598</td>\n",
              "      <td>-0.365126</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.101275</td>\n",
              "      <td>0.653562</td>\n",
              "      <td>-1.329956</td>\n",
              "      <td>-0.068212</td>\n",
              "      <td>-0.002522</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>0.582605</td>\n",
              "      <td>-0.271313</td>\n",
              "      <td>-0.103774</td>\n",
              "      <td>-0.332382</td>\n",
              "      <td>0.050141</td>\n",
              "      <td>-0.477013</td>\n",
              "      <td>-0.463821</td>\n",
              "      <td>-0.384454</td>\n",
              "      <td>-0.553494</td>\n",
              "      <td>-0.443713</td>\n",
              "      <td>-0.299814</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.451243</td>\n",
              "      <td>-0.467664</td>\n",
              "      <td>-0.449870</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.612090</td>\n",
              "      <td>-0.447976</td>\n",
              "      <td>0.550388</td>\n",
              "      <td>-0.053397</td>\n",
              "      <td>-2.635402</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>1.135966</td>\n",
              "      <td>-0.238396</td>\n",
              "      <td>-0.421599</td>\n",
              "      <td>-0.328551</td>\n",
              "      <td>-0.302697</td>\n",
              "      <td>-0.549785</td>\n",
              "      <td>-0.374960</td>\n",
              "      <td>-0.384169</td>\n",
              "      <td>-0.543902</td>\n",
              "      <td>-0.376095</td>\n",
              "      <td>-0.278958</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.360911</td>\n",
              "      <td>-0.359353</td>\n",
              "      <td>-0.332235</td>\n",
              "      <td>-0.399819</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.617489</td>\n",
              "      <td>-0.484329</td>\n",
              "      <td>0.020560</td>\n",
              "      <td>-0.070663</td>\n",
              "      <td>0.057182</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>-0.413459</td>\n",
              "      <td>-0.189698</td>\n",
              "      <td>0.840036</td>\n",
              "      <td>-0.241780</td>\n",
              "      <td>1.233789</td>\n",
              "      <td>-0.189959</td>\n",
              "      <td>-0.412373</td>\n",
              "      <td>-0.349641</td>\n",
              "      <td>-0.344119</td>\n",
              "      <td>-0.043922</td>\n",
              "      <td>0.285933</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.406573</td>\n",
              "      <td>0.049707</td>\n",
              "      <td>0.101845</td>\n",
              "      <td>-0.289082</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.271978</td>\n",
              "      <td>-0.221640</td>\n",
              "      <td>-0.008672</td>\n",
              "      <td>-0.028270</td>\n",
              "      <td>-6.432749</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>0.616892</td>\n",
              "      <td>-0.273389</td>\n",
              "      <td>-0.376326</td>\n",
              "      <td>-0.305403</td>\n",
              "      <td>-0.252674</td>\n",
              "      <td>-0.349274</td>\n",
              "      <td>-0.234203</td>\n",
              "      <td>-0.394508</td>\n",
              "      <td>-0.523364</td>\n",
              "      <td>-0.425376</td>\n",
              "      <td>-0.289620</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.423838</td>\n",
              "      <td>-0.465634</td>\n",
              "      <td>-0.447441</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.376008</td>\n",
              "      <td>0.250216</td>\n",
              "      <td>-0.609363</td>\n",
              "      <td>-0.069410</td>\n",
              "      <td>0.013519</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>-0.181273</td>\n",
              "      <td>-0.274563</td>\n",
              "      <td>0.647510</td>\n",
              "      <td>-0.332692</td>\n",
              "      <td>0.739074</td>\n",
              "      <td>-0.311458</td>\n",
              "      <td>-0.455735</td>\n",
              "      <td>-0.368905</td>\n",
              "      <td>-0.389118</td>\n",
              "      <td>-0.270535</td>\n",
              "      <td>-0.160791</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.441505</td>\n",
              "      <td>-0.333047</td>\n",
              "      <td>-0.305018</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.334298</td>\n",
              "      <td>-0.380063</td>\n",
              "      <td>0.361970</td>\n",
              "      <td>-0.026060</td>\n",
              "      <td>-6.769072</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>-0.351380</td>\n",
              "      <td>-0.151978</td>\n",
              "      <td>-0.056461</td>\n",
              "      <td>-0.158752</td>\n",
              "      <td>0.575088</td>\n",
              "      <td>-0.230310</td>\n",
              "      <td>-0.406511</td>\n",
              "      <td>-0.050205</td>\n",
              "      <td>-0.353577</td>\n",
              "      <td>-0.289735</td>\n",
              "      <td>0.266916</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.401873</td>\n",
              "      <td>-0.008619</td>\n",
              "      <td>0.012195</td>\n",
              "      <td>0.012146</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.311476</td>\n",
              "      <td>-0.210154</td>\n",
              "      <td>-0.235451</td>\n",
              "      <td>-0.068939</td>\n",
              "      <td>-0.007792</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>0.153145</td>\n",
              "      <td>-0.002710</td>\n",
              "      <td>0.241034</td>\n",
              "      <td>-0.050144</td>\n",
              "      <td>1.342761</td>\n",
              "      <td>-0.156514</td>\n",
              "      <td>-0.318598</td>\n",
              "      <td>-0.328977</td>\n",
              "      <td>-0.457466</td>\n",
              "      <td>0.221011</td>\n",
              "      <td>1.049079</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.332363</td>\n",
              "      <td>0.697194</td>\n",
              "      <td>0.788254</td>\n",
              "      <td>-0.034652</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.189329</td>\n",
              "      <td>0.018859</td>\n",
              "      <td>-0.624518</td>\n",
              "      <td>-0.070234</td>\n",
              "      <td>-0.070489</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>1.181604</td>\n",
              "      <td>-0.213786</td>\n",
              "      <td>-0.287594</td>\n",
              "      <td>-0.325408</td>\n",
              "      <td>-0.229370</td>\n",
              "      <td>-0.520765</td>\n",
              "      <td>-0.297657</td>\n",
              "      <td>-0.375949</td>\n",
              "      <td>-0.450513</td>\n",
              "      <td>-0.231843</td>\n",
              "      <td>-0.189922</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.283381</td>\n",
              "      <td>-0.202757</td>\n",
              "      <td>-0.162861</td>\n",
              "      <td>-0.391922</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.475287</td>\n",
              "      <td>-0.471852</td>\n",
              "      <td>-0.511364</td>\n",
              "      <td>-0.070401</td>\n",
              "      <td>0.084791</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>1.115638</td>\n",
              "      <td>-0.248473</td>\n",
              "      <td>-0.475619</td>\n",
              "      <td>-0.329739</td>\n",
              "      <td>-0.332166</td>\n",
              "      <td>-0.560795</td>\n",
              "      <td>-0.405708</td>\n",
              "      <td>-0.387530</td>\n",
              "      <td>-0.581588</td>\n",
              "      <td>-0.434600</td>\n",
              "      <td>-0.314997</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.392487</td>\n",
              "      <td>-0.423055</td>\n",
              "      <td>-0.401136</td>\n",
              "      <td>-0.403047</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.674114</td>\n",
              "      <td>-0.486720</td>\n",
              "      <td>0.233363</td>\n",
              "      <td>-0.070765</td>\n",
              "      <td>0.045861</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>0.258046</td>\n",
              "      <td>-0.039400</td>\n",
              "      <td>0.118062</td>\n",
              "      <td>-0.085218</td>\n",
              "      <td>1.118146</td>\n",
              "      <td>-0.213258</td>\n",
              "      <td>-0.349233</td>\n",
              "      <td>-0.338341</td>\n",
              "      <td>-0.497143</td>\n",
              "      <td>0.102619</td>\n",
              "      <td>0.858340</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.360224</td>\n",
              "      <td>0.518424</td>\n",
              "      <td>0.597628</td>\n",
              "      <td>-0.081945</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.923967</td>\n",
              "      <td>-0.044722</td>\n",
              "      <td>-0.381210</td>\n",
              "      <td>-0.070365</td>\n",
              "      <td>-0.063559</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>-0.565707</td>\n",
              "      <td>-0.158493</td>\n",
              "      <td>0.729016</td>\n",
              "      <td>1.246387</td>\n",
              "      <td>1.267931</td>\n",
              "      <td>2.021332</td>\n",
              "      <td>-0.109419</td>\n",
              "      <td>-0.103963</td>\n",
              "      <td>1.088659</td>\n",
              "      <td>-0.247586</td>\n",
              "      <td>0.674510</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.224538</td>\n",
              "      <td>0.059598</td>\n",
              "      <td>0.082419</td>\n",
              "      <td>-0.077757</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.147745</td>\n",
              "      <td>-0.171429</td>\n",
              "      <td>-0.070463</td>\n",
              "      <td>-0.067958</td>\n",
              "      <td>-0.064151</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>-0.206449</td>\n",
              "      <td>-0.058515</td>\n",
              "      <td>0.476366</td>\n",
              "      <td>0.517601</td>\n",
              "      <td>1.390845</td>\n",
              "      <td>0.792234</td>\n",
              "      <td>-0.230265</td>\n",
              "      <td>-0.230492</td>\n",
              "      <td>0.202508</td>\n",
              "      <td>0.043906</td>\n",
              "      <td>0.952165</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.288837</td>\n",
              "      <td>0.470644</td>\n",
              "      <td>0.535160</td>\n",
              "      <td>-0.034881</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.703125</td>\n",
              "      <td>-0.037098</td>\n",
              "      <td>-0.393923</td>\n",
              "      <td>-0.069253</td>\n",
              "      <td>-0.075706</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>-0.522212</td>\n",
              "      <td>-0.173932</td>\n",
              "      <td>0.709351</td>\n",
              "      <td>1.258779</td>\n",
              "      <td>1.176344</td>\n",
              "      <td>2.047680</td>\n",
              "      <td>-0.103749</td>\n",
              "      <td>-0.101759</td>\n",
              "      <td>1.122619</td>\n",
              "      <td>-0.283638</td>\n",
              "      <td>0.595504</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.219360</td>\n",
              "      <td>-0.005671</td>\n",
              "      <td>0.012747</td>\n",
              "      <td>-0.099126</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.272125</td>\n",
              "      <td>-0.204008</td>\n",
              "      <td>-0.051655</td>\n",
              "      <td>-0.067919</td>\n",
              "      <td>-0.055106</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>-0.015169</td>\n",
              "      <td>-0.017141</td>\n",
              "      <td>0.376997</td>\n",
              "      <td>-0.060501</td>\n",
              "      <td>1.442390</td>\n",
              "      <td>-0.134627</td>\n",
              "      <td>-0.335182</td>\n",
              "      <td>-0.328814</td>\n",
              "      <td>-0.439706</td>\n",
              "      <td>0.212021</td>\n",
              "      <td>1.018406</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.348019</td>\n",
              "      <td>0.659828</td>\n",
              "      <td>0.748422</td>\n",
              "      <td>-0.049040</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.165687</td>\n",
              "      <td>0.017156</td>\n",
              "      <td>-0.534092</td>\n",
              "      <td>-0.063457</td>\n",
              "      <td>-1.107510</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>-0.563882</td>\n",
              "      <td>-0.257451</td>\n",
              "      <td>-0.275659</td>\n",
              "      <td>-0.245571</td>\n",
              "      <td>-0.007668</td>\n",
              "      <td>-0.304056</td>\n",
              "      <td>-0.456763</td>\n",
              "      <td>0.110439</td>\n",
              "      <td>-0.291707</td>\n",
              "      <td>-0.628457</td>\n",
              "      <td>-0.296862</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.438802</td>\n",
              "      <td>-0.499297</td>\n",
              "      <td>-0.524150</td>\n",
              "      <td>0.010296</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.344067</td>\n",
              "      <td>-0.386325</td>\n",
              "      <td>0.003923</td>\n",
              "      <td>-0.068189</td>\n",
              "      <td>0.042051</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>-0.329344</td>\n",
              "      <td>-0.092715</td>\n",
              "      <td>0.562793</td>\n",
              "      <td>0.766905</td>\n",
              "      <td>1.348798</td>\n",
              "      <td>1.212685</td>\n",
              "      <td>-0.188926</td>\n",
              "      <td>-0.187209</td>\n",
              "      <td>0.505643</td>\n",
              "      <td>-0.055808</td>\n",
              "      <td>0.857185</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.266842</td>\n",
              "      <td>0.330033</td>\n",
              "      <td>0.380286</td>\n",
              "      <td>-0.049548</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.412059</td>\n",
              "      <td>-0.083050</td>\n",
              "      <td>-0.283274</td>\n",
              "      <td>-0.068810</td>\n",
              "      <td>-0.071753</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>-0.108255</td>\n",
              "      <td>-0.268641</td>\n",
              "      <td>-0.246506</td>\n",
              "      <td>-0.267292</td>\n",
              "      <td>-0.085534</td>\n",
              "      <td>-0.231585</td>\n",
              "      <td>-0.238360</td>\n",
              "      <td>-0.167069</td>\n",
              "      <td>-0.364629</td>\n",
              "      <td>-0.466679</td>\n",
              "      <td>-0.253672</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.418666</td>\n",
              "      <td>-0.448110</td>\n",
              "      <td>-0.446763</td>\n",
              "      <td>-0.220532</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.191867</td>\n",
              "      <td>0.273641</td>\n",
              "      <td>-0.854478</td>\n",
              "      <td>-0.068212</td>\n",
              "      <td>0.021824</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>0.135147</td>\n",
              "      <td>-0.079891</td>\n",
              "      <td>0.120092</td>\n",
              "      <td>-0.111061</td>\n",
              "      <td>0.946649</td>\n",
              "      <td>-0.142311</td>\n",
              "      <td>-0.236378</td>\n",
              "      <td>-0.347684</td>\n",
              "      <td>-0.447257</td>\n",
              "      <td>0.066793</td>\n",
              "      <td>0.707617</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.357337</td>\n",
              "      <td>0.390508</td>\n",
              "      <td>0.462375</td>\n",
              "      <td>-0.136331</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.873852</td>\n",
              "      <td>0.300460</td>\n",
              "      <td>-0.930561</td>\n",
              "      <td>-0.069582</td>\n",
              "      <td>-0.054374</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>0.393587</td>\n",
              "      <td>-0.077587</td>\n",
              "      <td>0.000935</td>\n",
              "      <td>-0.126254</td>\n",
              "      <td>0.867081</td>\n",
              "      <td>-0.274772</td>\n",
              "      <td>-0.369137</td>\n",
              "      <td>-0.347664</td>\n",
              "      <td>-0.524069</td>\n",
              "      <td>-0.006661</td>\n",
              "      <td>0.651049</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.376388</td>\n",
              "      <td>0.340194</td>\n",
              "      <td>0.408101</td>\n",
              "      <td>-0.136404</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.639058</td>\n",
              "      <td>-0.118982</td>\n",
              "      <td>-0.206286</td>\n",
              "      <td>-0.070466</td>\n",
              "      <td>-0.049251</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>-0.334535</td>\n",
              "      <td>-0.251686</td>\n",
              "      <td>-0.277266</td>\n",
              "      <td>-0.256060</td>\n",
              "      <td>-0.036774</td>\n",
              "      <td>-0.332687</td>\n",
              "      <td>-0.436072</td>\n",
              "      <td>0.046713</td>\n",
              "      <td>-0.312535</td>\n",
              "      <td>-0.576415</td>\n",
              "      <td>-0.282829</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.418356</td>\n",
              "      <td>-0.460301</td>\n",
              "      <td>-0.476658</td>\n",
              "      <td>-0.042400</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.361536</td>\n",
              "      <td>-0.398393</td>\n",
              "      <td>-0.062902</td>\n",
              "      <td>-0.068481</td>\n",
              "      <td>0.047705</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>1.059480</td>\n",
              "      <td>-0.269354</td>\n",
              "      <td>-0.580262</td>\n",
              "      <td>-0.331345</td>\n",
              "      <td>-0.388457</td>\n",
              "      <td>-0.576136</td>\n",
              "      <td>-0.461671</td>\n",
              "      <td>-0.394453</td>\n",
              "      <td>-0.654937</td>\n",
              "      <td>-0.551007</td>\n",
              "      <td>-0.386088</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.456450</td>\n",
              "      <td>-0.551451</td>\n",
              "      <td>-0.540018</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.777732</td>\n",
              "      <td>-0.468426</td>\n",
              "      <td>0.631453</td>\n",
              "      <td>-0.070925</td>\n",
              "      <td>0.022267</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>0.910712</td>\n",
              "      <td>-0.223281</td>\n",
              "      <td>-0.445937</td>\n",
              "      <td>-0.282814</td>\n",
              "      <td>-0.090793</td>\n",
              "      <td>-0.509460</td>\n",
              "      <td>-0.445074</td>\n",
              "      <td>-0.383235</td>\n",
              "      <td>-0.626795</td>\n",
              "      <td>-0.423593</td>\n",
              "      <td>-0.139818</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.438057</td>\n",
              "      <td>-0.339799</td>\n",
              "      <td>-0.314993</td>\n",
              "      <td>-0.344177</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.447943</td>\n",
              "      <td>-0.402303</td>\n",
              "      <td>0.461094</td>\n",
              "      <td>-0.070852</td>\n",
              "      <td>0.005337</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>0.160715</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.042999</td>\n",
              "      <td>-0.292567</td>\n",
              "      <td>0.027452</td>\n",
              "      <td>-0.185134</td>\n",
              "      <td>-0.118317</td>\n",
              "      <td>-0.389741</td>\n",
              "      <td>-0.407088</td>\n",
              "      <td>-0.309348</td>\n",
              "      <td>-0.198661</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.404579</td>\n",
              "      <td>-0.381344</td>\n",
              "      <td>-0.356625</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.090091</td>\n",
              "      <td>0.629399</td>\n",
              "      <td>-1.286007</td>\n",
              "      <td>-0.060226</td>\n",
              "      <td>-1.265496</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>0.735853</td>\n",
              "      <td>-0.224781</td>\n",
              "      <td>-0.284769</td>\n",
              "      <td>-0.305014</td>\n",
              "      <td>-0.172615</td>\n",
              "      <td>-0.466312</td>\n",
              "      <td>-0.339508</td>\n",
              "      <td>-0.250701</td>\n",
              "      <td>-0.409741</td>\n",
              "      <td>-0.333530</td>\n",
              "      <td>-0.217337</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.322931</td>\n",
              "      <td>-0.278305</td>\n",
              "      <td>-0.255008</td>\n",
              "      <td>-0.288339</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.443062</td>\n",
              "      <td>-0.454713</td>\n",
              "      <td>-0.374784</td>\n",
              "      <td>-0.069840</td>\n",
              "      <td>0.074092</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>0.316921</td>\n",
              "      <td>-0.276124</td>\n",
              "      <td>-0.238104</td>\n",
              "      <td>-0.287821</td>\n",
              "      <td>-0.160645</td>\n",
              "      <td>-0.195514</td>\n",
              "      <td>-0.080032</td>\n",
              "      <td>-0.394546</td>\n",
              "      <td>-0.434187</td>\n",
              "      <td>-0.340227</td>\n",
              "      <td>-0.224236</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.401735</td>\n",
              "      <td>-0.407471</td>\n",
              "      <td>-0.384694</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.103733</td>\n",
              "      <td>0.737288</td>\n",
              "      <td>-1.450348</td>\n",
              "      <td>-0.068384</td>\n",
              "      <td>0.007591</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>0.562059</td>\n",
              "      <td>-0.239684</td>\n",
              "      <td>-0.177582</td>\n",
              "      <td>0.175999</td>\n",
              "      <td>0.105844</td>\n",
              "      <td>0.256159</td>\n",
              "      <td>-0.355081</td>\n",
              "      <td>-0.301242</td>\n",
              "      <td>-0.093309</td>\n",
              "      <td>-0.473600</td>\n",
              "      <td>-0.077840</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.384254</td>\n",
              "      <td>-0.384687</td>\n",
              "      <td>-0.371638</td>\n",
              "      <td>-0.310763</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.630112</td>\n",
              "      <td>-0.399963</td>\n",
              "      <td>0.459591</td>\n",
              "      <td>-0.070007</td>\n",
              "      <td>-0.003276</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>0.167524</td>\n",
              "      <td>-0.216909</td>\n",
              "      <td>0.137964</td>\n",
              "      <td>0.567807</td>\n",
              "      <td>0.490331</td>\n",
              "      <td>0.903507</td>\n",
              "      <td>-0.267234</td>\n",
              "      <td>-0.229378</td>\n",
              "      <td>0.342982</td>\n",
              "      <td>-0.410789</td>\n",
              "      <td>0.162222</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.327779</td>\n",
              "      <td>-0.253974</td>\n",
              "      <td>-0.239510</td>\n",
              "      <td>-0.234486</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.506291</td>\n",
              "      <td>-0.329277</td>\n",
              "      <td>0.296177</td>\n",
              "      <td>-0.069261</td>\n",
              "      <td>-0.023188</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>0.816561</td>\n",
              "      <td>-0.271569</td>\n",
              "      <td>-0.468329</td>\n",
              "      <td>-0.317107</td>\n",
              "      <td>-0.313931</td>\n",
              "      <td>-0.451620</td>\n",
              "      <td>-0.336823</td>\n",
              "      <td>-0.394483</td>\n",
              "      <td>-0.582722</td>\n",
              "      <td>-0.482053</td>\n",
              "      <td>-0.333141</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.438551</td>\n",
              "      <td>-0.504350</td>\n",
              "      <td>-0.489206</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.557242</td>\n",
              "      <td>-0.073992</td>\n",
              "      <td>-0.049581</td>\n",
              "      <td>-0.070094</td>\n",
              "      <td>0.017466</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.186773</td>\n",
              "      <td>0.584017</td>\n",
              "      <td>1.070378</td>\n",
              "      <td>1.035885</td>\n",
              "      <td>1.757744</td>\n",
              "      <td>-0.156646</td>\n",
              "      <td>-0.050720</td>\n",
              "      <td>0.949607</td>\n",
              "      <td>-0.347250</td>\n",
              "      <td>0.476147</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.255088</td>\n",
              "      <td>-0.083413</td>\n",
              "      <td>-0.073716</td>\n",
              "      <td>-0.065365</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.274769</td>\n",
              "      <td>-0.222790</td>\n",
              "      <td>-0.019477</td>\n",
              "      <td>-0.067844</td>\n",
              "      <td>-0.044806</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>-0.369219</td>\n",
              "      <td>-0.200217</td>\n",
              "      <td>0.494757</td>\n",
              "      <td>0.901026</td>\n",
              "      <td>0.870625</td>\n",
              "      <td>1.542950</td>\n",
              "      <td>-0.085439</td>\n",
              "      <td>-0.170586</td>\n",
              "      <td>0.763644</td>\n",
              "      <td>-0.298685</td>\n",
              "      <td>0.405315</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.265800</td>\n",
              "      <td>-0.105528</td>\n",
              "      <td>-0.086641</td>\n",
              "      <td>-0.172055</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.210866</td>\n",
              "      <td>0.075768</td>\n",
              "      <td>-0.437037</td>\n",
              "      <td>-0.067919</td>\n",
              "      <td>-0.043712</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>-0.582317</td>\n",
              "      <td>-0.259316</td>\n",
              "      <td>-0.273528</td>\n",
              "      <td>-0.244782</td>\n",
              "      <td>-0.006569</td>\n",
              "      <td>-0.293750</td>\n",
              "      <td>-0.447454</td>\n",
              "      <td>0.106223</td>\n",
              "      <td>-0.291985</td>\n",
              "      <td>-0.629027</td>\n",
              "      <td>-0.297043</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.441713</td>\n",
              "      <td>-0.504058</td>\n",
              "      <td>-0.529017</td>\n",
              "      <td>0.006726</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.331097</td>\n",
              "      <td>-0.343032</td>\n",
              "      <td>-0.035610</td>\n",
              "      <td>-0.068131</td>\n",
              "      <td>0.039649</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>0.183465</td>\n",
              "      <td>-0.142691</td>\n",
              "      <td>0.010125</td>\n",
              "      <td>-0.166976</td>\n",
              "      <td>0.595787</td>\n",
              "      <td>-0.154304</td>\n",
              "      <td>-0.181391</td>\n",
              "      <td>-0.362661</td>\n",
              "      <td>-0.440175</td>\n",
              "      <td>-0.060505</td>\n",
              "      <td>0.411953</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.370805</td>\n",
              "      <td>0.137391</td>\n",
              "      <td>0.193719</td>\n",
              "      <td>-0.223691</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.570313</td>\n",
              "      <td>0.455926</td>\n",
              "      <td>-1.124069</td>\n",
              "      <td>-0.069166</td>\n",
              "      <td>-0.034765</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>0.949382</td>\n",
              "      <td>-0.270358</td>\n",
              "      <td>-0.529531</td>\n",
              "      <td>-0.324892</td>\n",
              "      <td>-0.354679</td>\n",
              "      <td>-0.519702</td>\n",
              "      <td>-0.405086</td>\n",
              "      <td>-0.394467</td>\n",
              "      <td>-0.622207</td>\n",
              "      <td>-0.519755</td>\n",
              "      <td>-0.362091</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.448338</td>\n",
              "      <td>-0.530103</td>\n",
              "      <td>-0.516989</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.677799</td>\n",
              "      <td>-0.289657</td>\n",
              "      <td>0.322788</td>\n",
              "      <td>-0.070548</td>\n",
              "      <td>0.020091</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>327</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.272962</td>\n",
              "      <td>0.810672</td>\n",
              "      <td>-0.315456</td>\n",
              "      <td>0.909260</td>\n",
              "      <td>-0.233330</td>\n",
              "      <td>-0.453120</td>\n",
              "      <td>-0.265957</td>\n",
              "      <td>-0.294360</td>\n",
              "      <td>-0.263342</td>\n",
              "      <td>-0.124446</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.437402</td>\n",
              "      <td>-0.305628</td>\n",
              "      <td>-0.283570</td>\n",
              "      <td>-0.325837</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.208805</td>\n",
              "      <td>-0.349808</td>\n",
              "      <td>0.209281</td>\n",
              "      <td>-0.021774</td>\n",
              "      <td>-7.330065</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>0.657316</td>\n",
              "      <td>-0.151890</td>\n",
              "      <td>-0.226965</td>\n",
              "      <td>-0.206098</td>\n",
              "      <td>0.378574</td>\n",
              "      <td>-0.394460</td>\n",
              "      <td>-0.407864</td>\n",
              "      <td>-0.365805</td>\n",
              "      <td>-0.576458</td>\n",
              "      <td>-0.219293</td>\n",
              "      <td>0.247714</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.407839</td>\n",
              "      <td>-0.006596</td>\n",
              "      <td>0.039330</td>\n",
              "      <td>-0.242366</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.084698</td>\n",
              "      <td>-0.263473</td>\n",
              "      <td>0.134072</td>\n",
              "      <td>-0.070663</td>\n",
              "      <td>-0.021412</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329</th>\n",
              "      <td>0.522957</td>\n",
              "      <td>-0.274245</td>\n",
              "      <td>-0.333042</td>\n",
              "      <td>-0.299897</td>\n",
              "      <td>-0.223855</td>\n",
              "      <td>-0.301124</td>\n",
              "      <td>-0.185924</td>\n",
              "      <td>-0.394520</td>\n",
              "      <td>-0.495438</td>\n",
              "      <td>-0.398711</td>\n",
              "      <td>-0.269145</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.416917</td>\n",
              "      <td>-0.447420</td>\n",
              "      <td>-0.427792</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.290746</td>\n",
              "      <td>0.402742</td>\n",
              "      <td>-0.872717</td>\n",
              "      <td>-0.069089</td>\n",
              "      <td>0.011663</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>-0.032022</td>\n",
              "      <td>-0.021994</td>\n",
              "      <td>0.210089</td>\n",
              "      <td>-0.055780</td>\n",
              "      <td>1.277178</td>\n",
              "      <td>-0.148049</td>\n",
              "      <td>-0.339844</td>\n",
              "      <td>-0.262683</td>\n",
              "      <td>-0.434430</td>\n",
              "      <td>0.136551</td>\n",
              "      <td>0.957280</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.351498</td>\n",
              "      <td>0.599300</td>\n",
              "      <td>0.677979</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.105291</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>-0.544638</td>\n",
              "      <td>-0.069929</td>\n",
              "      <td>-0.067006</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331</th>\n",
              "      <td>0.195753</td>\n",
              "      <td>-0.021850</td>\n",
              "      <td>0.171892</td>\n",
              "      <td>-0.066359</td>\n",
              "      <td>1.233531</td>\n",
              "      <td>-0.184988</td>\n",
              "      <td>-0.340085</td>\n",
              "      <td>-0.334056</td>\n",
              "      <td>-0.484769</td>\n",
              "      <td>0.152843</td>\n",
              "      <td>0.953607</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.352795</td>\n",
              "      <td>0.600335</td>\n",
              "      <td>0.684732</td>\n",
              "      <td>-0.056917</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.054906</td>\n",
              "      <td>-0.010593</td>\n",
              "      <td>-0.461602</td>\n",
              "      <td>-0.070319</td>\n",
              "      <td>-0.070134</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>-0.562186</td>\n",
              "      <td>-0.174785</td>\n",
              "      <td>0.721578</td>\n",
              "      <td>1.292471</td>\n",
              "      <td>1.201456</td>\n",
              "      <td>2.100804</td>\n",
              "      <td>-0.104758</td>\n",
              "      <td>-0.096463</td>\n",
              "      <td>1.149921</td>\n",
              "      <td>-0.294619</td>\n",
              "      <td>0.606227</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.223326</td>\n",
              "      <td>-0.012215</td>\n",
              "      <td>0.004864</td>\n",
              "      <td>-0.093409</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.277280</td>\n",
              "      <td>-0.198541</td>\n",
              "      <td>-0.006063</td>\n",
              "      <td>-0.067882</td>\n",
              "      <td>-0.060016</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>0.222130</td>\n",
              "      <td>-0.192945</td>\n",
              "      <td>-0.077874</td>\n",
              "      <td>-0.211721</td>\n",
              "      <td>0.315013</td>\n",
              "      <td>-0.163901</td>\n",
              "      <td>-0.137389</td>\n",
              "      <td>-0.374646</td>\n",
              "      <td>-0.434508</td>\n",
              "      <td>-0.162374</td>\n",
              "      <td>0.175350</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.381583</td>\n",
              "      <td>-0.065164</td>\n",
              "      <td>-0.021270</td>\n",
              "      <td>-0.293599</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.327409</td>\n",
              "      <td>0.580336</td>\n",
              "      <td>-1.278921</td>\n",
              "      <td>-0.068832</td>\n",
              "      <td>-0.019073</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>334</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.175386</td>\n",
              "      <td>0.720061</td>\n",
              "      <td>1.278351</td>\n",
              "      <td>1.200095</td>\n",
              "      <td>2.083174</td>\n",
              "      <td>-0.108423</td>\n",
              "      <td>-0.078420</td>\n",
              "      <td>1.145411</td>\n",
              "      <td>-0.300945</td>\n",
              "      <td>0.599007</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.225302</td>\n",
              "      <td>-0.016235</td>\n",
              "      <td>-0.000776</td>\n",
              "      <td>-0.079151</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.264397</td>\n",
              "      <td>-0.197315</td>\n",
              "      <td>-0.025474</td>\n",
              "      <td>-0.067799</td>\n",
              "      <td>-0.058363</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>0.400822</td>\n",
              "      <td>-0.230376</td>\n",
              "      <td>-0.048626</td>\n",
              "      <td>0.336122</td>\n",
              "      <td>0.262975</td>\n",
              "      <td>0.520714</td>\n",
              "      <td>-0.319180</td>\n",
              "      <td>-0.271873</td>\n",
              "      <td>0.084992</td>\n",
              "      <td>-0.447931</td>\n",
              "      <td>0.020268</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.361174</td>\n",
              "      <td>-0.331267</td>\n",
              "      <td>-0.317640</td>\n",
              "      <td>-0.279590</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.579510</td>\n",
              "      <td>-0.371076</td>\n",
              "      <td>0.392807</td>\n",
              "      <td>-0.069702</td>\n",
              "      <td>-0.011414</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>0.166688</td>\n",
              "      <td>-0.239087</td>\n",
              "      <td>-0.280779</td>\n",
              "      <td>-0.278983</td>\n",
              "      <td>-0.100383</td>\n",
              "      <td>-0.395259</td>\n",
              "      <td>-0.390855</td>\n",
              "      <td>-0.092555</td>\n",
              "      <td>-0.358053</td>\n",
              "      <td>-0.462681</td>\n",
              "      <td>-0.252162</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.373672</td>\n",
              "      <td>-0.375079</td>\n",
              "      <td>-0.372868</td>\n",
              "      <td>-0.157564</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.399711</td>\n",
              "      <td>-0.424765</td>\n",
              "      <td>-0.208945</td>\n",
              "      <td>-0.069117</td>\n",
              "      <td>0.060061</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>337</th>\n",
              "      <td>-0.181694</td>\n",
              "      <td>-0.196750</td>\n",
              "      <td>0.417265</td>\n",
              "      <td>0.914610</td>\n",
              "      <td>0.830655</td>\n",
              "      <td>1.476499</td>\n",
              "      <td>-0.189478</td>\n",
              "      <td>-0.165769</td>\n",
              "      <td>0.729160</td>\n",
              "      <td>-0.355194</td>\n",
              "      <td>0.374710</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.277791</td>\n",
              "      <td>-0.138275</td>\n",
              "      <td>-0.122560</td>\n",
              "      <td>-0.166970</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.396693</td>\n",
              "      <td>-0.266711</td>\n",
              "      <td>0.151534</td>\n",
              "      <td>-0.068601</td>\n",
              "      <td>-0.040813</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>-0.190294</td>\n",
              "      <td>-0.274602</td>\n",
              "      <td>0.656383</td>\n",
              "      <td>-0.332695</td>\n",
              "      <td>0.747210</td>\n",
              "      <td>-0.309503</td>\n",
              "      <td>-0.455640</td>\n",
              "      <td>-0.368722</td>\n",
              "      <td>-0.387177</td>\n",
              "      <td>-0.268490</td>\n",
              "      <td>-0.159149</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.441390</td>\n",
              "      <td>-0.331457</td>\n",
              "      <td>-0.303308</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.331017</td>\n",
              "      <td>-0.379261</td>\n",
              "      <td>0.359745</td>\n",
              "      <td>-0.025738</td>\n",
              "      <td>-6.817891</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>-0.302549</td>\n",
              "      <td>-0.260640</td>\n",
              "      <td>-0.332737</td>\n",
              "      <td>-0.259682</td>\n",
              "      <td>-0.073523</td>\n",
              "      <td>-0.350499</td>\n",
              "      <td>-0.462680</td>\n",
              "      <td>0.028636</td>\n",
              "      <td>-0.355764</td>\n",
              "      <td>-0.624128</td>\n",
              "      <td>-0.316365</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.445864</td>\n",
              "      <td>-0.516315</td>\n",
              "      <td>-0.536022</td>\n",
              "      <td>-0.057807</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.423311</td>\n",
              "      <td>-0.403606</td>\n",
              "      <td>0.138801</td>\n",
              "      <td>-0.068651</td>\n",
              "      <td>0.037456</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>-0.468028</td>\n",
              "      <td>-0.255042</td>\n",
              "      <td>-0.276330</td>\n",
              "      <td>-0.249954</td>\n",
              "      <td>-0.019832</td>\n",
              "      <td>-0.316022</td>\n",
              "      <td>-0.448115</td>\n",
              "      <td>0.083805</td>\n",
              "      <td>-0.300412</td>\n",
              "      <td>-0.606706</td>\n",
              "      <td>-0.290997</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.430257</td>\n",
              "      <td>-0.482999</td>\n",
              "      <td>-0.504301</td>\n",
              "      <td>-0.011728</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.351368</td>\n",
              "      <td>-0.391369</td>\n",
              "      <td>-0.024006</td>\n",
              "      <td>-0.068311</td>\n",
              "      <td>0.044414</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.221487</td>\n",
              "      <td>0.169280</td>\n",
              "      <td>0.436359</td>\n",
              "      <td>0.535279</td>\n",
              "      <td>0.765652</td>\n",
              "      <td>-0.303655</td>\n",
              "      <td>0.033726</td>\n",
              "      <td>0.352690</td>\n",
              "      <td>-0.488412</td>\n",
              "      <td>0.101602</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.345893</td>\n",
              "      <td>-0.288210</td>\n",
              "      <td>-0.296077</td>\n",
              "      <td>-0.023336</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.306387</td>\n",
              "      <td>-0.300455</td>\n",
              "      <td>-0.001193</td>\n",
              "      <td>-0.067980</td>\n",
              "      <td>-0.003478</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>-0.148887</td>\n",
              "      <td>-0.042496</td>\n",
              "      <td>0.435885</td>\n",
              "      <td>0.400832</td>\n",
              "      <td>1.410539</td>\n",
              "      <td>0.595302</td>\n",
              "      <td>-0.249628</td>\n",
              "      <td>-0.250765</td>\n",
              "      <td>0.060525</td>\n",
              "      <td>0.090610</td>\n",
              "      <td>0.996653</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.299139</td>\n",
              "      <td>0.536503</td>\n",
              "      <td>0.607700</td>\n",
              "      <td>-0.028011</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.839455</td>\n",
              "      <td>-0.015575</td>\n",
              "      <td>-0.445749</td>\n",
              "      <td>-0.069460</td>\n",
              "      <td>-0.077558</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>-0.515394</td>\n",
              "      <td>-0.275985</td>\n",
              "      <td>0.976123</td>\n",
              "      <td>-0.332827</td>\n",
              "      <td>1.040414</td>\n",
              "      <td>-0.239044</td>\n",
              "      <td>-0.452198</td>\n",
              "      <td>-0.362105</td>\n",
              "      <td>-0.317219</td>\n",
              "      <td>-0.194787</td>\n",
              "      <td>-0.099983</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.437246</td>\n",
              "      <td>-0.274165</td>\n",
              "      <td>-0.241660</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.212792</td>\n",
              "      <td>-0.350358</td>\n",
              "      <td>0.279556</td>\n",
              "      <td>-0.014104</td>\n",
              "      <td>-8.577145</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.251334</td>\n",
              "      <td>-0.187302</td>\n",
              "      <td>-0.108757</td>\n",
              "      <td>0.104870</td>\n",
              "      <td>-0.087327</td>\n",
              "      <td>-0.430051</td>\n",
              "      <td>0.106330</td>\n",
              "      <td>-0.160526</td>\n",
              "      <td>-0.609781</td>\n",
              "      <td>-0.220424</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.423965</td>\n",
              "      <td>-0.464290</td>\n",
              "      <td>-0.487258</td>\n",
              "      <td>0.012799</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.333572</td>\n",
              "      <td>-0.367229</td>\n",
              "      <td>0.014526</td>\n",
              "      <td>-0.068097</td>\n",
              "      <td>0.032055</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>0.186820</td>\n",
              "      <td>-0.147051</td>\n",
              "      <td>0.002489</td>\n",
              "      <td>-0.170859</td>\n",
              "      <td>0.571423</td>\n",
              "      <td>-0.155137</td>\n",
              "      <td>-0.177573</td>\n",
              "      <td>-0.363701</td>\n",
              "      <td>-0.439683</td>\n",
              "      <td>-0.069345</td>\n",
              "      <td>0.391422</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.371740</td>\n",
              "      <td>0.119814</td>\n",
              "      <td>0.175064</td>\n",
              "      <td>-0.229757</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.549235</td>\n",
              "      <td>0.466722</td>\n",
              "      <td>-1.137506</td>\n",
              "      <td>-0.069137</td>\n",
              "      <td>-0.033404</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>-0.451744</td>\n",
              "      <td>-0.206285</td>\n",
              "      <td>0.884544</td>\n",
              "      <td>-0.259205</td>\n",
              "      <td>1.213738</td>\n",
              "      <td>-0.195278</td>\n",
              "      <td>-0.419793</td>\n",
              "      <td>-0.351643</td>\n",
              "      <td>-0.334931</td>\n",
              "      <td>-0.068524</td>\n",
              "      <td>0.215525</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.412201</td>\n",
              "      <td>-0.008940</td>\n",
              "      <td>0.039694</td>\n",
              "      <td>-0.312155</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.186072</td>\n",
              "      <td>-0.244594</td>\n",
              "      <td>0.041833</td>\n",
              "      <td>-0.024888</td>\n",
              "      <td>-6.944627</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>-0.550229</td>\n",
              "      <td>-0.248952</td>\n",
              "      <td>0.999039</td>\n",
              "      <td>-0.304029</td>\n",
              "      <td>1.162158</td>\n",
              "      <td>-0.208960</td>\n",
              "      <td>-0.438880</td>\n",
              "      <td>-0.356793</td>\n",
              "      <td>-0.311295</td>\n",
              "      <td>-0.131811</td>\n",
              "      <td>0.034409</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.426680</td>\n",
              "      <td>-0.159803</td>\n",
              "      <td>-0.120183</td>\n",
              "      <td>-0.371510</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.034913</td>\n",
              "      <td>-0.303641</td>\n",
              "      <td>0.171752</td>\n",
              "      <td>-0.016187</td>\n",
              "      <td>-8.261383</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>-0.490163</td>\n",
              "      <td>-0.137470</td>\n",
              "      <td>0.675890</td>\n",
              "      <td>1.093140</td>\n",
              "      <td>1.293777</td>\n",
              "      <td>1.762882</td>\n",
              "      <td>-0.134830</td>\n",
              "      <td>-0.130569</td>\n",
              "      <td>0.902322</td>\n",
              "      <td>-0.186292</td>\n",
              "      <td>0.732894</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.238059</td>\n",
              "      <td>0.146031</td>\n",
              "      <td>0.177620</td>\n",
              "      <td>-0.068741</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.031173</td>\n",
              "      <td>-0.143182</td>\n",
              "      <td>-0.138479</td>\n",
              "      <td>-0.068230</td>\n",
              "      <td>-0.066580</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>1.068585</td>\n",
              "      <td>-0.217575</td>\n",
              "      <td>-0.198960</td>\n",
              "      <td>-0.326088</td>\n",
              "      <td>-0.140934</td>\n",
              "      <td>-0.502613</td>\n",
              "      <td>-0.308831</td>\n",
              "      <td>-0.374822</td>\n",
              "      <td>-0.440540</td>\n",
              "      <td>-0.227479</td>\n",
              "      <td>-0.182738</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.292795</td>\n",
              "      <td>-0.205326</td>\n",
              "      <td>-0.165747</td>\n",
              "      <td>-0.392997</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.457734</td>\n",
              "      <td>-0.469469</td>\n",
              "      <td>-0.456241</td>\n",
              "      <td>-0.066501</td>\n",
              "      <td>-0.515944</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>-0.003218</td>\n",
              "      <td>-0.262507</td>\n",
              "      <td>-0.388003</td>\n",
              "      <td>-0.275450</td>\n",
              "      <td>-0.142971</td>\n",
              "      <td>-0.401166</td>\n",
              "      <td>-0.464059</td>\n",
              "      <td>-0.063380</td>\n",
              "      <td>-0.421755</td>\n",
              "      <td>-0.609108</td>\n",
              "      <td>-0.332207</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.448396</td>\n",
              "      <td>-0.524560</td>\n",
              "      <td>-0.537542</td>\n",
              "      <td>-0.134337</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.503215</td>\n",
              "      <td>-0.422753</td>\n",
              "      <td>0.254664</td>\n",
              "      <td>-0.069156</td>\n",
              "      <td>0.034214</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>0.201923</td>\n",
              "      <td>-0.263786</td>\n",
              "      <td>-0.425879</td>\n",
              "      <td>-0.286256</td>\n",
              "      <td>-0.190565</td>\n",
              "      <td>-0.435889</td>\n",
              "      <td>-0.465004</td>\n",
              "      <td>-0.126440</td>\n",
              "      <td>-0.466980</td>\n",
              "      <td>-0.598815</td>\n",
              "      <td>-0.343063</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.450130</td>\n",
              "      <td>-0.530210</td>\n",
              "      <td>-0.538583</td>\n",
              "      <td>-0.186786</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.557975</td>\n",
              "      <td>-0.435875</td>\n",
              "      <td>0.334069</td>\n",
              "      <td>-0.069502</td>\n",
              "      <td>0.031993</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>-0.170073</td>\n",
              "      <td>-0.078182</td>\n",
              "      <td>0.094866</td>\n",
              "      <td>-0.100292</td>\n",
              "      <td>0.973682</td>\n",
              "      <td>-0.183608</td>\n",
              "      <td>-0.368662</td>\n",
              "      <td>-0.170834</td>\n",
              "      <td>-0.399479</td>\n",
              "      <td>-0.047722</td>\n",
              "      <td>0.658853</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.373274</td>\n",
              "      <td>0.336512</td>\n",
              "      <td>0.390177</td>\n",
              "      <td>0.005528</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.762145</td>\n",
              "      <td>-0.090146</td>\n",
              "      <td>-0.410984</td>\n",
              "      <td>-0.069501</td>\n",
              "      <td>-0.041410</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>0.121552</td>\n",
              "      <td>-0.214255</td>\n",
              "      <td>0.174731</td>\n",
              "      <td>0.613460</td>\n",
              "      <td>0.535132</td>\n",
              "      <td>0.978936</td>\n",
              "      <td>-0.256998</td>\n",
              "      <td>-0.221005</td>\n",
              "      <td>0.393819</td>\n",
              "      <td>-0.403471</td>\n",
              "      <td>0.190195</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.321199</td>\n",
              "      <td>-0.238743</td>\n",
              "      <td>-0.224115</td>\n",
              "      <td>-0.225598</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.491864</td>\n",
              "      <td>-0.321041</td>\n",
              "      <td>0.277136</td>\n",
              "      <td>-0.069174</td>\n",
              "      <td>-0.025508</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>-0.431379</td>\n",
              "      <td>-0.184538</td>\n",
              "      <td>-0.123231</td>\n",
              "      <td>-0.184547</td>\n",
              "      <td>0.399216</td>\n",
              "      <td>-0.250917</td>\n",
              "      <td>-0.423210</td>\n",
              "      <td>0.003021</td>\n",
              "      <td>-0.333323</td>\n",
              "      <td>-0.396519</td>\n",
              "      <td>0.093982</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.414492</td>\n",
              "      <td>-0.160901</td>\n",
              "      <td>-0.154582</td>\n",
              "      <td>0.015066</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.112628</td>\n",
              "      <td>-0.263105</td>\n",
              "      <td>-0.158000</td>\n",
              "      <td>-0.068691</td>\n",
              "      <td>0.007041</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>-0.581382</td>\n",
              "      <td>-0.262450</td>\n",
              "      <td>1.035257</td>\n",
              "      <td>-0.318209</td>\n",
              "      <td>1.145841</td>\n",
              "      <td>-0.213288</td>\n",
              "      <td>-0.444917</td>\n",
              "      <td>-0.358422</td>\n",
              "      <td>-0.303819</td>\n",
              "      <td>-0.151830</td>\n",
              "      <td>-0.022884</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.431260</td>\n",
              "      <td>-0.207526</td>\n",
              "      <td>-0.170758</td>\n",
              "      <td>-0.390285</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.104817</td>\n",
              "      <td>-0.322319</td>\n",
              "      <td>0.212850</td>\n",
              "      <td>-0.013435</td>\n",
              "      <td>-8.677915</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>0.405199</td>\n",
              "      <td>-0.195389</td>\n",
              "      <td>0.168002</td>\n",
              "      <td>0.398727</td>\n",
              "      <td>0.413063</td>\n",
              "      <td>0.652494</td>\n",
              "      <td>-0.209668</td>\n",
              "      <td>-0.250544</td>\n",
              "      <td>0.268608</td>\n",
              "      <td>-0.255240</td>\n",
              "      <td>0.169232</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.253784</td>\n",
              "      <td>-0.112092</td>\n",
              "      <td>-0.081969</td>\n",
              "      <td>-0.258013</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.383537</td>\n",
              "      <td>-0.352962</td>\n",
              "      <td>-0.298298</td>\n",
              "      <td>-0.069272</td>\n",
              "      <td>0.021054</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>-0.328047</td>\n",
              "      <td>-0.266417</td>\n",
              "      <td>0.856840</td>\n",
              "      <td>-0.331716</td>\n",
              "      <td>0.913413</td>\n",
              "      <td>-0.266114</td>\n",
              "      <td>-0.427000</td>\n",
              "      <td>-0.362604</td>\n",
              "      <td>-0.320609</td>\n",
              "      <td>-0.181854</td>\n",
              "      <td>-0.099193</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.411687</td>\n",
              "      <td>-0.248112</td>\n",
              "      <td>-0.213315</td>\n",
              "      <td>-0.406861</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.224722</td>\n",
              "      <td>-0.363328</td>\n",
              "      <td>0.134548</td>\n",
              "      <td>-0.020080</td>\n",
              "      <td>-7.649965</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>0.871017</td>\n",
              "      <td>-0.267959</td>\n",
              "      <td>-0.549414</td>\n",
              "      <td>-0.321502</td>\n",
              "      <td>-0.345801</td>\n",
              "      <td>-0.549144</td>\n",
              "      <td>-0.468086</td>\n",
              "      <td>-0.332122</td>\n",
              "      <td>-0.614488</td>\n",
              "      <td>-0.565240</td>\n",
              "      <td>-0.378474</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.455789</td>\n",
              "      <td>-0.548639</td>\n",
              "      <td>-0.541980</td>\n",
              "      <td>-0.357855</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.736585</td>\n",
              "      <td>-0.478674</td>\n",
              "      <td>0.593057</td>\n",
              "      <td>-0.070632</td>\n",
              "      <td>0.024746</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>-0.021326</td>\n",
              "      <td>-0.270351</td>\n",
              "      <td>-0.241551</td>\n",
              "      <td>-0.271419</td>\n",
              "      <td>-0.100014</td>\n",
              "      <td>-0.220185</td>\n",
              "      <td>-0.200018</td>\n",
              "      <td>-0.217183</td>\n",
              "      <td>-0.377950</td>\n",
              "      <td>-0.436910</td>\n",
              "      <td>-0.245719</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.414440</td>\n",
              "      <td>-0.437851</td>\n",
              "      <td>-0.431681</td>\n",
              "      <td>-0.262204</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.166337</td>\n",
              "      <td>0.386720</td>\n",
              "      <td>-1.004633</td>\n",
              "      <td>-0.068227</td>\n",
              "      <td>0.018555</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>0.082012</td>\n",
              "      <td>-0.010830</td>\n",
              "      <td>0.241024</td>\n",
              "      <td>-0.049571</td>\n",
              "      <td>1.332496</td>\n",
              "      <td>-0.129122</td>\n",
              "      <td>-0.296846</td>\n",
              "      <td>-0.331214</td>\n",
              "      <td>-0.455045</td>\n",
              "      <td>0.206785</td>\n",
              "      <td>1.032762</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.342525</td>\n",
              "      <td>0.668863</td>\n",
              "      <td>0.757819</td>\n",
              "      <td>-0.040261</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.207657</td>\n",
              "      <td>0.129492</td>\n",
              "      <td>-0.717760</td>\n",
              "      <td>-0.070040</td>\n",
              "      <td>-0.075939</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>-0.528853</td>\n",
              "      <td>-0.181665</td>\n",
              "      <td>0.669784</td>\n",
              "      <td>1.190085</td>\n",
              "      <td>1.119464</td>\n",
              "      <td>1.962465</td>\n",
              "      <td>-0.090583</td>\n",
              "      <td>-0.116048</td>\n",
              "      <td>1.053113</td>\n",
              "      <td>-0.290683</td>\n",
              "      <td>0.556993</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.233248</td>\n",
              "      <td>-0.033447</td>\n",
              "      <td>-0.015619</td>\n",
              "      <td>-0.114187</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.243712</td>\n",
              "      <td>-0.097409</td>\n",
              "      <td>-0.169410</td>\n",
              "      <td>-0.067832</td>\n",
              "      <td>-0.056057</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>-0.333338</td>\n",
              "      <td>-0.264213</td>\n",
              "      <td>-0.259336</td>\n",
              "      <td>-0.256604</td>\n",
              "      <td>-0.048042</td>\n",
              "      <td>-0.261101</td>\n",
              "      <td>-0.337637</td>\n",
              "      <td>-0.037311</td>\n",
              "      <td>-0.330138</td>\n",
              "      <td>-0.543762</td>\n",
              "      <td>-0.274264</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.429609</td>\n",
              "      <td>-0.474674</td>\n",
              "      <td>-0.485817</td>\n",
              "      <td>-0.112630</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.257973</td>\n",
              "      <td>-0.019153</td>\n",
              "      <td>-0.465682</td>\n",
              "      <td>-0.068174</td>\n",
              "      <td>0.030287</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>1.167831</td>\n",
              "      <td>-0.214749</td>\n",
              "      <td>-0.286616</td>\n",
              "      <td>-0.324802</td>\n",
              "      <td>-0.228168</td>\n",
              "      <td>-0.515520</td>\n",
              "      <td>-0.294068</td>\n",
              "      <td>-0.376235</td>\n",
              "      <td>-0.450122</td>\n",
              "      <td>-0.233377</td>\n",
              "      <td>-0.190347</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.285167</td>\n",
              "      <td>-0.205815</td>\n",
              "      <td>-0.166176</td>\n",
              "      <td>-0.392195</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.469144</td>\n",
              "      <td>-0.452486</td>\n",
              "      <td>-0.527129</td>\n",
              "      <td>-0.070369</td>\n",
              "      <td>0.083594</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>-0.455803</td>\n",
              "      <td>-0.190155</td>\n",
              "      <td>0.589690</td>\n",
              "      <td>1.057809</td>\n",
              "      <td>1.005594</td>\n",
              "      <td>1.770491</td>\n",
              "      <td>-0.088229</td>\n",
              "      <td>-0.141005</td>\n",
              "      <td>0.920649</td>\n",
              "      <td>-0.294345</td>\n",
              "      <td>0.487584</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.248144</td>\n",
              "      <td>-0.066432</td>\n",
              "      <td>-0.048119</td>\n",
              "      <td>-0.140668</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.228681</td>\n",
              "      <td>-0.018162</td>\n",
              "      <td>-0.291879</td>\n",
              "      <td>-0.067872</td>\n",
              "      <td>-0.050408</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>-0.598144</td>\n",
              "      <td>-0.173613</td>\n",
              "      <td>0.745756</td>\n",
              "      <td>1.315553</td>\n",
              "      <td>1.227475</td>\n",
              "      <td>2.144559</td>\n",
              "      <td>-0.092815</td>\n",
              "      <td>-0.092376</td>\n",
              "      <td>1.178759</td>\n",
              "      <td>-0.287210</td>\n",
              "      <td>0.622830</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.219118</td>\n",
              "      <td>-0.002159</td>\n",
              "      <td>0.015208</td>\n",
              "      <td>-0.089069</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.257970</td>\n",
              "      <td>-0.172578</td>\n",
              "      <td>-0.053245</td>\n",
              "      <td>-0.067794</td>\n",
              "      <td>-0.061416</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>-0.187678</td>\n",
              "      <td>-0.181672</td>\n",
              "      <td>0.514076</td>\n",
              "      <td>0.948542</td>\n",
              "      <td>0.901015</td>\n",
              "      <td>1.544410</td>\n",
              "      <td>-0.141956</td>\n",
              "      <td>-0.155429</td>\n",
              "      <td>0.814562</td>\n",
              "      <td>-0.273394</td>\n",
              "      <td>0.441740</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.231778</td>\n",
              "      <td>-0.044059</td>\n",
              "      <td>-0.021419</td>\n",
              "      <td>-0.156439</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.312313</td>\n",
              "      <td>-0.257739</td>\n",
              "      <td>-0.140624</td>\n",
              "      <td>-0.068407</td>\n",
              "      <td>-0.027634</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>0.747970</td>\n",
              "      <td>-0.250416</td>\n",
              "      <td>-0.326272</td>\n",
              "      <td>-0.008627</td>\n",
              "      <td>-0.075333</td>\n",
              "      <td>-0.048883</td>\n",
              "      <td>-0.396476</td>\n",
              "      <td>-0.335106</td>\n",
              "      <td>-0.298897</td>\n",
              "      <td>-0.503197</td>\n",
              "      <td>-0.190961</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.410866</td>\n",
              "      <td>-0.446281</td>\n",
              "      <td>-0.433898</td>\n",
              "      <td>-0.346705</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.688459</td>\n",
              "      <td>-0.433271</td>\n",
              "      <td>0.536594</td>\n",
              "      <td>-0.070358</td>\n",
              "      <td>0.006107</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>-0.301939</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.623641</td>\n",
              "      <td>-0.316646</td>\n",
              "      <td>0.685622</td>\n",
              "      <td>-0.204630</td>\n",
              "      <td>-0.317194</td>\n",
              "      <td>-0.372034</td>\n",
              "      <td>-0.340765</td>\n",
              "      <td>-0.227595</td>\n",
              "      <td>-0.129022</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.423350</td>\n",
              "      <td>-0.306964</td>\n",
              "      <td>-0.276806</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.142069</td>\n",
              "      <td>0.049201</td>\n",
              "      <td>-0.365040</td>\n",
              "      <td>-0.030568</td>\n",
              "      <td>-5.951777</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>-0.195415</td>\n",
              "      <td>-0.220416</td>\n",
              "      <td>0.304194</td>\n",
              "      <td>0.586311</td>\n",
              "      <td>0.599699</td>\n",
              "      <td>1.086198</td>\n",
              "      <td>-0.079839</td>\n",
              "      <td>-0.229964</td>\n",
              "      <td>0.448481</td>\n",
              "      <td>-0.307397</td>\n",
              "      <td>0.240173</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.301242</td>\n",
              "      <td>-0.184008</td>\n",
              "      <td>-0.163966</td>\n",
              "      <td>-0.235059</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.175103</td>\n",
              "      <td>0.264317</td>\n",
              "      <td>-0.728419</td>\n",
              "      <td>-0.068015</td>\n",
              "      <td>-0.030271</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370</th>\n",
              "      <td>-0.377204</td>\n",
              "      <td>-0.106034</td>\n",
              "      <td>0.596450</td>\n",
              "      <td>0.863992</td>\n",
              "      <td>1.332424</td>\n",
              "      <td>1.376423</td>\n",
              "      <td>-0.172827</td>\n",
              "      <td>-0.170353</td>\n",
              "      <td>0.623694</td>\n",
              "      <td>-0.094640</td>\n",
              "      <td>0.820196</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.258276</td>\n",
              "      <td>0.275274</td>\n",
              "      <td>0.319973</td>\n",
              "      <td>-0.055260</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.298707</td>\n",
              "      <td>-0.100945</td>\n",
              "      <td>-0.240183</td>\n",
              "      <td>-0.068637</td>\n",
              "      <td>-0.070214</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>1.035936</td>\n",
              "      <td>-0.269568</td>\n",
              "      <td>-0.569413</td>\n",
              "      <td>-0.329965</td>\n",
              "      <td>-0.381234</td>\n",
              "      <td>-0.564067</td>\n",
              "      <td>-0.449570</td>\n",
              "      <td>-0.394456</td>\n",
              "      <td>-0.647938</td>\n",
              "      <td>-0.544324</td>\n",
              "      <td>-0.380957</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.454715</td>\n",
              "      <td>-0.546886</td>\n",
              "      <td>-0.535093</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.756361</td>\n",
              "      <td>-0.430196</td>\n",
              "      <td>0.565445</td>\n",
              "      <td>-0.070844</td>\n",
              "      <td>0.021802</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>1.094740</td>\n",
              "      <td>-0.215760</td>\n",
              "      <td>-0.287284</td>\n",
              "      <td>-0.321427</td>\n",
              "      <td>-0.218161</td>\n",
              "      <td>-0.511115</td>\n",
              "      <td>-0.307131</td>\n",
              "      <td>-0.350419</td>\n",
              "      <td>-0.442333</td>\n",
              "      <td>-0.252093</td>\n",
              "      <td>-0.195378</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.290936</td>\n",
              "      <td>-0.217284</td>\n",
              "      <td>-0.180692</td>\n",
              "      <td>-0.370799</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.470397</td>\n",
              "      <td>-0.473596</td>\n",
              "      <td>-0.479354</td>\n",
              "      <td>-0.070296</td>\n",
              "      <td>0.082940</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.248855</td>\n",
              "      <td>-0.157684</td>\n",
              "      <td>-0.063480</td>\n",
              "      <td>0.140619</td>\n",
              "      <td>-0.016479</td>\n",
              "      <td>-0.419553</td>\n",
              "      <td>0.100300</td>\n",
              "      <td>-0.117898</td>\n",
              "      <td>-0.599700</td>\n",
              "      <td>-0.193677</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.417480</td>\n",
              "      <td>-0.449665</td>\n",
              "      <td>-0.471378</td>\n",
              "      <td>0.009797</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.331314</td>\n",
              "      <td>-0.361683</td>\n",
              "      <td>0.013221</td>\n",
              "      <td>-0.068087</td>\n",
              "      <td>0.029104</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>-0.144016</td>\n",
              "      <td>-0.259981</td>\n",
              "      <td>0.717720</td>\n",
              "      <td>-0.330975</td>\n",
              "      <td>0.774484</td>\n",
              "      <td>-0.297277</td>\n",
              "      <td>-0.411429</td>\n",
              "      <td>-0.364214</td>\n",
              "      <td>-0.336412</td>\n",
              "      <td>-0.187866</td>\n",
              "      <td>-0.110201</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.396021</td>\n",
              "      <td>-0.242475</td>\n",
              "      <td>-0.207047</td>\n",
              "      <td>-0.405034</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.377314</td>\n",
              "      <td>0.056701</td>\n",
              "      <td>-0.026197</td>\n",
              "      <td>-6.709934</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>-0.587764</td>\n",
              "      <td>-0.258862</td>\n",
              "      <td>-0.280078</td>\n",
              "      <td>-0.244658</td>\n",
              "      <td>-0.007351</td>\n",
              "      <td>-0.302222</td>\n",
              "      <td>-0.461366</td>\n",
              "      <td>0.116311</td>\n",
              "      <td>-0.292886</td>\n",
              "      <td>-0.638440</td>\n",
              "      <td>-0.301270</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.443452</td>\n",
              "      <td>-0.508459</td>\n",
              "      <td>-0.534574</td>\n",
              "      <td>0.015115</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.347175</td>\n",
              "      <td>-0.385362</td>\n",
              "      <td>0.028402</td>\n",
              "      <td>-0.068170</td>\n",
              "      <td>0.040545</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>1.126368</td>\n",
              "      <td>-0.217649</td>\n",
              "      <td>-0.283672</td>\n",
              "      <td>-0.322979</td>\n",
              "      <td>-0.224551</td>\n",
              "      <td>-0.499730</td>\n",
              "      <td>-0.283261</td>\n",
              "      <td>-0.377096</td>\n",
              "      <td>-0.448945</td>\n",
              "      <td>-0.237995</td>\n",
              "      <td>-0.191627</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.290545</td>\n",
              "      <td>-0.215022</td>\n",
              "      <td>-0.176153</td>\n",
              "      <td>-0.393019</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.450650</td>\n",
              "      <td>-0.394186</td>\n",
              "      <td>-0.574588</td>\n",
              "      <td>-0.070270</td>\n",
              "      <td>0.079990</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>0.680879</td>\n",
              "      <td>-0.272806</td>\n",
              "      <td>-0.405809</td>\n",
              "      <td>-0.309154</td>\n",
              "      <td>-0.272305</td>\n",
              "      <td>-0.382072</td>\n",
              "      <td>-0.267089</td>\n",
              "      <td>-0.394500</td>\n",
              "      <td>-0.542386</td>\n",
              "      <td>-0.443539</td>\n",
              "      <td>-0.303566</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.428553</td>\n",
              "      <td>-0.478041</td>\n",
              "      <td>-0.460825</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.434087</td>\n",
              "      <td>0.146319</td>\n",
              "      <td>-0.429974</td>\n",
              "      <td>-0.069629</td>\n",
              "      <td>0.014784</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.218680</td>\n",
              "      <td>0.202816</td>\n",
              "      <td>0.487626</td>\n",
              "      <td>0.575758</td>\n",
              "      <td>0.845873</td>\n",
              "      <td>-0.291768</td>\n",
              "      <td>0.026897</td>\n",
              "      <td>0.400957</td>\n",
              "      <td>-0.476998</td>\n",
              "      <td>0.131888</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.338550</td>\n",
              "      <td>-0.271650</td>\n",
              "      <td>-0.278097</td>\n",
              "      <td>-0.026735</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.303831</td>\n",
              "      <td>-0.294175</td>\n",
              "      <td>-0.002672</td>\n",
              "      <td>-0.067969</td>\n",
              "      <td>-0.006820</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>0.132461</td>\n",
              "      <td>-0.076400</td>\n",
              "      <td>0.126205</td>\n",
              "      <td>-0.107953</td>\n",
              "      <td>0.966152</td>\n",
              "      <td>-0.141644</td>\n",
              "      <td>-0.239434</td>\n",
              "      <td>-0.346852</td>\n",
              "      <td>-0.447650</td>\n",
              "      <td>0.073869</td>\n",
              "      <td>0.724052</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.356588</td>\n",
              "      <td>0.404577</td>\n",
              "      <td>0.477308</td>\n",
              "      <td>-0.131475</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.890724</td>\n",
              "      <td>0.291818</td>\n",
              "      <td>-0.919805</td>\n",
              "      <td>-0.069605</td>\n",
              "      <td>-0.055464</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>0.588834</td>\n",
              "      <td>-0.273645</td>\n",
              "      <td>-0.363397</td>\n",
              "      <td>-0.303759</td>\n",
              "      <td>-0.244066</td>\n",
              "      <td>-0.334892</td>\n",
              "      <td>-0.219782</td>\n",
              "      <td>-0.394512</td>\n",
              "      <td>-0.515023</td>\n",
              "      <td>-0.417411</td>\n",
              "      <td>-0.283504</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.421771</td>\n",
              "      <td>-0.460194</td>\n",
              "      <td>-0.441572</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.350541</td>\n",
              "      <td>0.295775</td>\n",
              "      <td>-0.688025</td>\n",
              "      <td>-0.069314</td>\n",
              "      <td>0.012965</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>1.016974</td>\n",
              "      <td>-0.253219</td>\n",
              "      <td>-0.537762</td>\n",
              "      <td>-0.314985</td>\n",
              "      <td>-0.287623</td>\n",
              "      <td>-0.557685</td>\n",
              "      <td>-0.460678</td>\n",
              "      <td>-0.390544</td>\n",
              "      <td>-0.647904</td>\n",
              "      <td>-0.509267</td>\n",
              "      <td>-0.302330</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.450729</td>\n",
              "      <td>-0.479528</td>\n",
              "      <td>-0.463579</td>\n",
              "      <td>-0.386872</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.671305</td>\n",
              "      <td>-0.460522</td>\n",
              "      <td>0.598231</td>\n",
              "      <td>-0.070931</td>\n",
              "      <td>0.016554</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>0.023732</td>\n",
              "      <td>-0.000288</td>\n",
              "      <td>0.331773</td>\n",
              "      <td>-0.042796</td>\n",
              "      <td>1.462764</td>\n",
              "      <td>-0.129223</td>\n",
              "      <td>-0.327643</td>\n",
              "      <td>-0.326780</td>\n",
              "      <td>-0.449042</td>\n",
              "      <td>0.237019</td>\n",
              "      <td>1.089945</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.342300</td>\n",
              "      <td>0.719418</td>\n",
              "      <td>0.811572</td>\n",
              "      <td>-0.025596</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.252974</td>\n",
              "      <td>0.040479</td>\n",
              "      <td>-0.585408</td>\n",
              "      <td>-0.066894</td>\n",
              "      <td>-0.587405</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>-0.389504</td>\n",
              "      <td>-0.260098</td>\n",
              "      <td>-0.316683</td>\n",
              "      <td>-0.255101</td>\n",
              "      <td>-0.053349</td>\n",
              "      <td>-0.335781</td>\n",
              "      <td>-0.462279</td>\n",
              "      <td>0.055366</td>\n",
              "      <td>-0.336594</td>\n",
              "      <td>-0.628492</td>\n",
              "      <td>-0.311763</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.445129</td>\n",
              "      <td>-0.513920</td>\n",
              "      <td>-0.535581</td>\n",
              "      <td>-0.035575</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.400099</td>\n",
              "      <td>-0.398044</td>\n",
              "      <td>0.105143</td>\n",
              "      <td>-0.068504</td>\n",
              "      <td>0.038398</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>-0.000067</td>\n",
              "      <td>-0.262527</td>\n",
              "      <td>-0.388585</td>\n",
              "      <td>-0.275616</td>\n",
              "      <td>-0.143702</td>\n",
              "      <td>-0.401699</td>\n",
              "      <td>-0.464073</td>\n",
              "      <td>-0.064348</td>\n",
              "      <td>-0.422449</td>\n",
              "      <td>-0.608950</td>\n",
              "      <td>-0.332373</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.448422</td>\n",
              "      <td>-0.524646</td>\n",
              "      <td>-0.537558</td>\n",
              "      <td>-0.135143</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.504056</td>\n",
              "      <td>-0.422954</td>\n",
              "      <td>0.255884</td>\n",
              "      <td>-0.069161</td>\n",
              "      <td>0.034180</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>0.872256</td>\n",
              "      <td>-0.212447</td>\n",
              "      <td>-0.412705</td>\n",
              "      <td>-0.271172</td>\n",
              "      <td>-0.019561</td>\n",
              "      <td>-0.492007</td>\n",
              "      <td>-0.439427</td>\n",
              "      <td>-0.380589</td>\n",
              "      <td>-0.619156</td>\n",
              "      <td>-0.392588</td>\n",
              "      <td>-0.081005</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.433471</td>\n",
              "      <td>-0.289231</td>\n",
              "      <td>-0.261220</td>\n",
              "      <td>-0.328726</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.367108</td>\n",
              "      <td>-0.381234</td>\n",
              "      <td>0.411464</td>\n",
              "      <td>-0.070823</td>\n",
              "      <td>0.001277</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.264415</td>\n",
              "      <td>0.159870</td>\n",
              "      <td>-0.272213</td>\n",
              "      <td>0.363548</td>\n",
              "      <td>-0.271996</td>\n",
              "      <td>-0.457990</td>\n",
              "      <td>-0.032116</td>\n",
              "      <td>-0.290065</td>\n",
              "      <td>-0.488906</td>\n",
              "      <td>-0.229594</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.440897</td>\n",
              "      <td>-0.426754</td>\n",
              "      <td>-0.433912</td>\n",
              "      <td>-0.117556</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.287596</td>\n",
              "      <td>-0.370125</td>\n",
              "      <td>0.094894</td>\n",
              "      <td>-0.049551</td>\n",
              "      <td>-2.912871</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>-0.097598</td>\n",
              "      <td>-0.268850</td>\n",
              "      <td>-0.245898</td>\n",
              "      <td>-0.267798</td>\n",
              "      <td>-0.087309</td>\n",
              "      <td>-0.230187</td>\n",
              "      <td>-0.233660</td>\n",
              "      <td>-0.173213</td>\n",
              "      <td>-0.366262</td>\n",
              "      <td>-0.463030</td>\n",
              "      <td>-0.252697</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.418148</td>\n",
              "      <td>-0.446853</td>\n",
              "      <td>-0.444915</td>\n",
              "      <td>-0.225640</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.188738</td>\n",
              "      <td>0.287503</td>\n",
              "      <td>-0.872885</td>\n",
              "      <td>-0.068214</td>\n",
              "      <td>0.021423</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>-0.396761</td>\n",
              "      <td>-0.260053</td>\n",
              "      <td>-0.315343</td>\n",
              "      <td>-0.254719</td>\n",
              "      <td>-0.051666</td>\n",
              "      <td>-0.334552</td>\n",
              "      <td>-0.462246</td>\n",
              "      <td>0.057597</td>\n",
              "      <td>-0.334994</td>\n",
              "      <td>-0.628856</td>\n",
              "      <td>-0.311379</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.445067</td>\n",
              "      <td>-0.513720</td>\n",
              "      <td>-0.535544</td>\n",
              "      <td>-0.033720</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.398162</td>\n",
              "      <td>-0.397579</td>\n",
              "      <td>0.102334</td>\n",
              "      <td>-0.068492</td>\n",
              "      <td>0.038477</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>-0.273782</td>\n",
              "      <td>-0.250159</td>\n",
              "      <td>-0.277692</td>\n",
              "      <td>-0.258838</td>\n",
              "      <td>-0.044484</td>\n",
              "      <td>-0.340272</td>\n",
              "      <td>-0.430592</td>\n",
              "      <td>0.029833</td>\n",
              "      <td>-0.318052</td>\n",
              "      <td>-0.562629</td>\n",
              "      <td>-0.279112</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.412940</td>\n",
              "      <td>-0.449972</td>\n",
              "      <td>-0.464078</td>\n",
              "      <td>-0.056359</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.366163</td>\n",
              "      <td>-0.401589</td>\n",
              "      <td>-0.080604</td>\n",
              "      <td>-0.068558</td>\n",
              "      <td>0.049202</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>0.124929</td>\n",
              "      <td>-0.273227</td>\n",
              "      <td>-0.233214</td>\n",
              "      <td>-0.278364</td>\n",
              "      <td>-0.124376</td>\n",
              "      <td>-0.201006</td>\n",
              "      <td>-0.135510</td>\n",
              "      <td>-0.301497</td>\n",
              "      <td>-0.400362</td>\n",
              "      <td>-0.386823</td>\n",
              "      <td>-0.232338</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.407329</td>\n",
              "      <td>-0.420591</td>\n",
              "      <td>-0.406304</td>\n",
              "      <td>-0.332317</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.123383</td>\n",
              "      <td>0.576973</td>\n",
              "      <td>-1.257267</td>\n",
              "      <td>-0.068252</td>\n",
              "      <td>0.013055</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>-0.050287</td>\n",
              "      <td>-0.269781</td>\n",
              "      <td>-0.243202</td>\n",
              "      <td>-0.270044</td>\n",
              "      <td>-0.095190</td>\n",
              "      <td>-0.223983</td>\n",
              "      <td>-0.212792</td>\n",
              "      <td>-0.200487</td>\n",
              "      <td>-0.373512</td>\n",
              "      <td>-0.446828</td>\n",
              "      <td>-0.248368</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.415848</td>\n",
              "      <td>-0.441269</td>\n",
              "      <td>-0.436706</td>\n",
              "      <td>-0.248321</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.174843</td>\n",
              "      <td>0.349047</td>\n",
              "      <td>-0.954609</td>\n",
              "      <td>-0.068222</td>\n",
              "      <td>0.019644</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>0.124738</td>\n",
              "      <td>-0.257622</td>\n",
              "      <td>-0.046828</td>\n",
              "      <td>0.006594</td>\n",
              "      <td>0.100644</td>\n",
              "      <td>0.244848</td>\n",
              "      <td>-0.069523</td>\n",
              "      <td>-0.339340</td>\n",
              "      <td>-0.132058</td>\n",
              "      <td>-0.323446</td>\n",
              "      <td>-0.064022</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.366528</td>\n",
              "      <td>-0.328571</td>\n",
              "      <td>-0.306401</td>\n",
              "      <td>-0.351114</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.109228</td>\n",
              "      <td>0.611629</td>\n",
              "      <td>-1.265154</td>\n",
              "      <td>-0.068190</td>\n",
              "      <td>-0.005512</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>-0.378359</td>\n",
              "      <td>-0.162958</td>\n",
              "      <td>-0.078978</td>\n",
              "      <td>-0.167451</td>\n",
              "      <td>0.515778</td>\n",
              "      <td>-0.237260</td>\n",
              "      <td>-0.412142</td>\n",
              "      <td>-0.032255</td>\n",
              "      <td>-0.346747</td>\n",
              "      <td>-0.325747</td>\n",
              "      <td>0.208597</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.406129</td>\n",
              "      <td>-0.059974</td>\n",
              "      <td>-0.044048</td>\n",
              "      <td>0.013131</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.244418</td>\n",
              "      <td>-0.228011</td>\n",
              "      <td>-0.209332</td>\n",
              "      <td>-0.068855</td>\n",
              "      <td>-0.002790</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>0.725027</td>\n",
              "      <td>-0.267049</td>\n",
              "      <td>-0.522460</td>\n",
              "      <td>-0.313812</td>\n",
              "      <td>-0.311930</td>\n",
              "      <td>-0.524433</td>\n",
              "      <td>-0.467413</td>\n",
              "      <td>-0.287244</td>\n",
              "      <td>-0.582303</td>\n",
              "      <td>-0.572566</td>\n",
              "      <td>-0.370748</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.454554</td>\n",
              "      <td>-0.544618</td>\n",
              "      <td>-0.541239</td>\n",
              "      <td>-0.320529</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.697614</td>\n",
              "      <td>-0.469336</td>\n",
              "      <td>0.536548</td>\n",
              "      <td>-0.070385</td>\n",
              "      <td>0.026327</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>0.858052</td>\n",
              "      <td>-0.271190</td>\n",
              "      <td>-0.487447</td>\n",
              "      <td>-0.319538</td>\n",
              "      <td>-0.326660</td>\n",
              "      <td>-0.472888</td>\n",
              "      <td>-0.358147</td>\n",
              "      <td>-0.394478</td>\n",
              "      <td>-0.595056</td>\n",
              "      <td>-0.493830</td>\n",
              "      <td>-0.342184</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.441608</td>\n",
              "      <td>-0.512395</td>\n",
              "      <td>-0.497885</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.594901</td>\n",
              "      <td>-0.141361</td>\n",
              "      <td>0.066739</td>\n",
              "      <td>-0.070236</td>\n",
              "      <td>0.018286</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>-0.227358</td>\n",
              "      <td>-0.248992</td>\n",
              "      <td>-0.278017</td>\n",
              "      <td>-0.260961</td>\n",
              "      <td>-0.050376</td>\n",
              "      <td>-0.346067</td>\n",
              "      <td>-0.426403</td>\n",
              "      <td>0.016934</td>\n",
              "      <td>-0.322268</td>\n",
              "      <td>-0.552095</td>\n",
              "      <td>-0.276272</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.408801</td>\n",
              "      <td>-0.442078</td>\n",
              "      <td>-0.454465</td>\n",
              "      <td>-0.067025</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.369699</td>\n",
              "      <td>-0.404032</td>\n",
              "      <td>-0.094131</td>\n",
              "      <td>-0.068617</td>\n",
              "      <td>0.050347</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>0.761887</td>\n",
              "      <td>-0.126888</td>\n",
              "      <td>-0.070642</td>\n",
              "      <td>-0.212457</td>\n",
              "      <td>0.416184</td>\n",
              "      <td>-0.372186</td>\n",
              "      <td>-0.306976</td>\n",
              "      <td>-0.356596</td>\n",
              "      <td>-0.453447</td>\n",
              "      <td>-0.045515</td>\n",
              "      <td>0.319115</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.303145</td>\n",
              "      <td>0.167532</td>\n",
              "      <td>0.228496</td>\n",
              "      <td>-0.245109</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.207271</td>\n",
              "      <td>-0.274145</td>\n",
              "      <td>-0.554698</td>\n",
              "      <td>-0.070339</td>\n",
              "      <td>0.021245</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>-0.578524</td>\n",
              "      <td>-0.259390</td>\n",
              "      <td>-0.273311</td>\n",
              "      <td>-0.244962</td>\n",
              "      <td>-0.007200</td>\n",
              "      <td>-0.293253</td>\n",
              "      <td>-0.445781</td>\n",
              "      <td>0.104036</td>\n",
              "      <td>-0.292566</td>\n",
              "      <td>-0.627728</td>\n",
              "      <td>-0.296696</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.441529</td>\n",
              "      <td>-0.503610</td>\n",
              "      <td>-0.528359</td>\n",
              "      <td>0.004908</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.329983</td>\n",
              "      <td>-0.338098</td>\n",
              "      <td>-0.042162</td>\n",
              "      <td>-0.068132</td>\n",
              "      <td>0.039507</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>-0.196300</td>\n",
              "      <td>-0.055690</td>\n",
              "      <td>0.469228</td>\n",
              "      <td>0.497012</td>\n",
              "      <td>1.394317</td>\n",
              "      <td>0.757510</td>\n",
              "      <td>-0.233680</td>\n",
              "      <td>-0.234066</td>\n",
              "      <td>0.177473</td>\n",
              "      <td>0.052141</td>\n",
              "      <td>0.960010</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.290654</td>\n",
              "      <td>0.482256</td>\n",
              "      <td>0.547951</td>\n",
              "      <td>-0.033669</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.727163</td>\n",
              "      <td>-0.033303</td>\n",
              "      <td>-0.403061</td>\n",
              "      <td>-0.069289</td>\n",
              "      <td>-0.076033</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>0.505889</td>\n",
              "      <td>-0.265682</td>\n",
              "      <td>-0.482000</td>\n",
              "      <td>-0.302268</td>\n",
              "      <td>-0.261088</td>\n",
              "      <td>-0.487341</td>\n",
              "      <td>-0.466404</td>\n",
              "      <td>-0.219880</td>\n",
              "      <td>-0.533992</td>\n",
              "      <td>-0.583562</td>\n",
              "      <td>-0.359150</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.452701</td>\n",
              "      <td>-0.538582</td>\n",
              "      <td>-0.540126</td>\n",
              "      <td>-0.264501</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.639117</td>\n",
              "      <td>-0.455318</td>\n",
              "      <td>0.451726</td>\n",
              "      <td>-0.070015</td>\n",
              "      <td>0.028700</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>0.876445</td>\n",
              "      <td>-0.235129</td>\n",
              "      <td>-0.265922</td>\n",
              "      <td>-0.311985</td>\n",
              "      <td>-0.202747</td>\n",
              "      <td>-0.404557</td>\n",
              "      <td>-0.218121</td>\n",
              "      <td>-0.382289</td>\n",
              "      <td>-0.441849</td>\n",
              "      <td>-0.265828</td>\n",
              "      <td>-0.199346</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.322955</td>\n",
              "      <td>-0.270515</td>\n",
              "      <td>-0.236295</td>\n",
              "      <td>-0.397980</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.339178</td>\n",
              "      <td>-0.042780</td>\n",
              "      <td>-0.860649</td>\n",
              "      <td>-0.069678</td>\n",
              "      <td>0.058270</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>0.423473</td>\n",
              "      <td>-0.265168</td>\n",
              "      <td>-0.466784</td>\n",
              "      <td>-0.297927</td>\n",
              "      <td>-0.241967</td>\n",
              "      <td>-0.473390</td>\n",
              "      <td>-0.466024</td>\n",
              "      <td>-0.194546</td>\n",
              "      <td>-0.515822</td>\n",
              "      <td>-0.587698</td>\n",
              "      <td>-0.354789</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.452004</td>\n",
              "      <td>-0.536312</td>\n",
              "      <td>-0.539708</td>\n",
              "      <td>-0.243430</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.617117</td>\n",
              "      <td>-0.450046</td>\n",
              "      <td>0.419825</td>\n",
              "      <td>-0.069876</td>\n",
              "      <td>0.029593</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>-0.153535</td>\n",
              "      <td>-0.071451</td>\n",
              "      <td>0.108669</td>\n",
              "      <td>-0.094960</td>\n",
              "      <td>1.010040</td>\n",
              "      <td>-0.179349</td>\n",
              "      <td>-0.365210</td>\n",
              "      <td>-0.181837</td>\n",
              "      <td>-0.403666</td>\n",
              "      <td>-0.025647</td>\n",
              "      <td>0.694603</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.370665</td>\n",
              "      <td>0.367992</td>\n",
              "      <td>0.424654</td>\n",
              "      <td>0.004925</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.803252</td>\n",
              "      <td>-0.079200</td>\n",
              "      <td>-0.426995</td>\n",
              "      <td>-0.069552</td>\n",
              "      <td>-0.044476</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>-0.307050</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.631006</td>\n",
              "      <td>-0.316913</td>\n",
              "      <td>0.692894</td>\n",
              "      <td>-0.204845</td>\n",
              "      <td>-0.319391</td>\n",
              "      <td>-0.371838</td>\n",
              "      <td>-0.340032</td>\n",
              "      <td>-0.226692</td>\n",
              "      <td>-0.128252</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.423557</td>\n",
              "      <td>-0.306142</td>\n",
              "      <td>-0.275925</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.142643</td>\n",
              "      <td>0.042791</td>\n",
              "      <td>-0.354865</td>\n",
              "      <td>-0.030240</td>\n",
              "      <td>-6.003551</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>405</th>\n",
              "      <td>0.717057</td>\n",
              "      <td>-0.225253</td>\n",
              "      <td>-0.284637</td>\n",
              "      <td>-0.304154</td>\n",
              "      <td>-0.170230</td>\n",
              "      <td>-0.463966</td>\n",
              "      <td>-0.341203</td>\n",
              "      <td>-0.245478</td>\n",
              "      <td>-0.408034</td>\n",
              "      <td>-0.337795</td>\n",
              "      <td>-0.218487</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.324607</td>\n",
              "      <td>-0.281501</td>\n",
              "      <td>-0.258900</td>\n",
              "      <td>-0.284020</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.441631</td>\n",
              "      <td>-0.453724</td>\n",
              "      <td>-0.369308</td>\n",
              "      <td>-0.069816</td>\n",
              "      <td>0.073629</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>0.563557</td>\n",
              "      <td>-0.125474</td>\n",
              "      <td>-0.145944</td>\n",
              "      <td>-0.177712</td>\n",
              "      <td>0.552245</td>\n",
              "      <td>-0.351910</td>\n",
              "      <td>-0.394096</td>\n",
              "      <td>-0.359356</td>\n",
              "      <td>-0.557833</td>\n",
              "      <td>-0.143700</td>\n",
              "      <td>0.391105</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.396658</td>\n",
              "      <td>0.116692</td>\n",
              "      <td>0.170432</td>\n",
              "      <td>-0.204695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.281779</td>\n",
              "      <td>-0.212105</td>\n",
              "      <td>0.013070</td>\n",
              "      <td>-0.070593</td>\n",
              "      <td>-0.031309</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.198500</td>\n",
              "      <td>0.443910</td>\n",
              "      <td>0.856192</td>\n",
              "      <td>0.866769</td>\n",
              "      <td>1.422593</td>\n",
              "      <td>-0.206309</td>\n",
              "      <td>-0.022192</td>\n",
              "      <td>0.747955</td>\n",
              "      <td>-0.394938</td>\n",
              "      <td>0.349617</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.285764</td>\n",
              "      <td>-0.152598</td>\n",
              "      <td>-0.148834</td>\n",
              "      <td>-0.051167</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.285450</td>\n",
              "      <td>-0.249027</td>\n",
              "      <td>-0.013300</td>\n",
              "      <td>-0.067890</td>\n",
              "      <td>-0.030844</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408</th>\n",
              "      <td>0.721116</td>\n",
              "      <td>-0.169865</td>\n",
              "      <td>-0.282098</td>\n",
              "      <td>-0.225414</td>\n",
              "      <td>0.260397</td>\n",
              "      <td>-0.423415</td>\n",
              "      <td>-0.417233</td>\n",
              "      <td>-0.370193</td>\n",
              "      <td>-0.589132</td>\n",
              "      <td>-0.270731</td>\n",
              "      <td>0.150141</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.415447</td>\n",
              "      <td>-0.090490</td>\n",
              "      <td>-0.049882</td>\n",
              "      <td>-0.268000</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.049410</td>\n",
              "      <td>-0.298428</td>\n",
              "      <td>0.216409</td>\n",
              "      <td>-0.070711</td>\n",
              "      <td>-0.014677</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409</th>\n",
              "      <td>1.129258</td>\n",
              "      <td>-0.217447</td>\n",
              "      <td>-0.283877</td>\n",
              "      <td>-0.323106</td>\n",
              "      <td>-0.224803</td>\n",
              "      <td>-0.500831</td>\n",
              "      <td>-0.284014</td>\n",
              "      <td>-0.377036</td>\n",
              "      <td>-0.449027</td>\n",
              "      <td>-0.237673</td>\n",
              "      <td>-0.191538</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.290170</td>\n",
              "      <td>-0.214380</td>\n",
              "      <td>-0.175458</td>\n",
              "      <td>-0.392961</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.451940</td>\n",
              "      <td>-0.398250</td>\n",
              "      <td>-0.571280</td>\n",
              "      <td>-0.070277</td>\n",
              "      <td>0.080242</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>0.070175</td>\n",
              "      <td>0.004555</td>\n",
              "      <td>0.267964</td>\n",
              "      <td>-0.035873</td>\n",
              "      <td>1.418451</td>\n",
              "      <td>-0.126184</td>\n",
              "      <td>-0.310317</td>\n",
              "      <td>-0.327545</td>\n",
              "      <td>-0.456780</td>\n",
              "      <td>0.237971</td>\n",
              "      <td>1.105195</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.339226</td>\n",
              "      <td>0.730872</td>\n",
              "      <td>0.823635</td>\n",
              "      <td>-0.018859</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.282019</td>\n",
              "      <td>0.091406</td>\n",
              "      <td>-0.670354</td>\n",
              "      <td>-0.070142</td>\n",
              "      <td>-0.080743</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>-0.237466</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.530743</td>\n",
              "      <td>-0.313291</td>\n",
              "      <td>0.593904</td>\n",
              "      <td>-0.201913</td>\n",
              "      <td>-0.289479</td>\n",
              "      <td>-0.374502</td>\n",
              "      <td>-0.350007</td>\n",
              "      <td>-0.238988</td>\n",
              "      <td>-0.138726</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.420734</td>\n",
              "      <td>-0.317329</td>\n",
              "      <td>-0.287929</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.134826</td>\n",
              "      <td>0.130053</td>\n",
              "      <td>-0.493380</td>\n",
              "      <td>-0.034701</td>\n",
              "      <td>-5.298727</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>0.315516</td>\n",
              "      <td>-0.274362</td>\n",
              "      <td>-0.226086</td>\n",
              "      <td>-0.287312</td>\n",
              "      <td>-0.153809</td>\n",
              "      <td>-0.190950</td>\n",
              "      <td>-0.071920</td>\n",
              "      <td>-0.393943</td>\n",
              "      <td>-0.425925</td>\n",
              "      <td>-0.328297</td>\n",
              "      <td>-0.216670</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.395698</td>\n",
              "      <td>-0.395063</td>\n",
              "      <td>-0.371277</td>\n",
              "      <td>-0.409116</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.088990</td>\n",
              "      <td>0.745920</td>\n",
              "      <td>-1.502687</td>\n",
              "      <td>-0.068348</td>\n",
              "      <td>0.009522</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>0.642285</td>\n",
              "      <td>-0.273157</td>\n",
              "      <td>-0.388026</td>\n",
              "      <td>-0.306892</td>\n",
              "      <td>-0.260464</td>\n",
              "      <td>-0.362290</td>\n",
              "      <td>-0.247253</td>\n",
              "      <td>-0.394505</td>\n",
              "      <td>-0.530913</td>\n",
              "      <td>-0.432584</td>\n",
              "      <td>-0.295154</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.425709</td>\n",
              "      <td>-0.470558</td>\n",
              "      <td>-0.452752</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.399056</td>\n",
              "      <td>0.208985</td>\n",
              "      <td>-0.538173</td>\n",
              "      <td>-0.069497</td>\n",
              "      <td>0.014021</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>0.191068</td>\n",
              "      <td>-0.152573</td>\n",
              "      <td>-0.007179</td>\n",
              "      <td>-0.175774</td>\n",
              "      <td>0.540576</td>\n",
              "      <td>-0.156191</td>\n",
              "      <td>-0.172739</td>\n",
              "      <td>-0.365018</td>\n",
              "      <td>-0.439061</td>\n",
              "      <td>-0.080536</td>\n",
              "      <td>0.365428</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.372924</td>\n",
              "      <td>0.097561</td>\n",
              "      <td>0.151444</td>\n",
              "      <td>-0.237437</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.522549</td>\n",
              "      <td>0.480390</td>\n",
              "      <td>-1.154518</td>\n",
              "      <td>-0.069100</td>\n",
              "      <td>-0.031680</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>0.797969</td>\n",
              "      <td>-0.240618</td>\n",
              "      <td>-0.260349</td>\n",
              "      <td>-0.308533</td>\n",
              "      <td>-0.195900</td>\n",
              "      <td>-0.374673</td>\n",
              "      <td>-0.197667</td>\n",
              "      <td>-0.383919</td>\n",
              "      <td>-0.439622</td>\n",
              "      <td>-0.274568</td>\n",
              "      <td>-0.201770</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.333132</td>\n",
              "      <td>-0.287939</td>\n",
              "      <td>-0.255179</td>\n",
              "      <td>-0.399538</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.304176</td>\n",
              "      <td>0.067562</td>\n",
              "      <td>-0.950472</td>\n",
              "      <td>-0.069492</td>\n",
              "      <td>0.051450</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>0.667979</td>\n",
              "      <td>-0.249710</td>\n",
              "      <td>-0.251117</td>\n",
              "      <td>-0.302815</td>\n",
              "      <td>-0.184559</td>\n",
              "      <td>-0.325171</td>\n",
              "      <td>-0.163786</td>\n",
              "      <td>-0.386620</td>\n",
              "      <td>-0.435931</td>\n",
              "      <td>-0.289044</td>\n",
              "      <td>-0.205784</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.349990</td>\n",
              "      <td>-0.316802</td>\n",
              "      <td>-0.286460</td>\n",
              "      <td>-0.402119</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.246197</td>\n",
              "      <td>0.250336</td>\n",
              "      <td>-1.099259</td>\n",
              "      <td>-0.069184</td>\n",
              "      <td>0.040153</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>0.703274</td>\n",
              "      <td>-0.202285</td>\n",
              "      <td>-0.005991</td>\n",
              "      <td>0.122302</td>\n",
              "      <td>0.167740</td>\n",
              "      <td>0.204072</td>\n",
              "      <td>-0.243711</td>\n",
              "      <td>-0.298364</td>\n",
              "      <td>-0.005876</td>\n",
              "      <td>-0.246113</td>\n",
              "      <td>0.032225</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.264848</td>\n",
              "      <td>-0.146297</td>\n",
              "      <td>-0.112411</td>\n",
              "      <td>-0.309081</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.419345</td>\n",
              "      <td>-0.400837</td>\n",
              "      <td>-0.377571</td>\n",
              "      <td>-0.069707</td>\n",
              "      <td>0.045532</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>-0.398129</td>\n",
              "      <td>-0.260044</td>\n",
              "      <td>-0.315091</td>\n",
              "      <td>-0.254647</td>\n",
              "      <td>-0.051348</td>\n",
              "      <td>-0.334321</td>\n",
              "      <td>-0.462240</td>\n",
              "      <td>0.058017</td>\n",
              "      <td>-0.334693</td>\n",
              "      <td>-0.628924</td>\n",
              "      <td>-0.311306</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.445056</td>\n",
              "      <td>-0.513683</td>\n",
              "      <td>-0.535537</td>\n",
              "      <td>-0.033370</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.397797</td>\n",
              "      <td>-0.397492</td>\n",
              "      <td>0.101805</td>\n",
              "      <td>-0.068490</td>\n",
              "      <td>0.038492</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>0.736276</td>\n",
              "      <td>-0.203048</td>\n",
              "      <td>-0.025255</td>\n",
              "      <td>0.091697</td>\n",
              "      <td>0.140579</td>\n",
              "      <td>0.154425</td>\n",
              "      <td>-0.247480</td>\n",
              "      <td>-0.303659</td>\n",
              "      <td>-0.036266</td>\n",
              "      <td>-0.245103</td>\n",
              "      <td>0.017057</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.266073</td>\n",
              "      <td>-0.150084</td>\n",
              "      <td>-0.115781</td>\n",
              "      <td>-0.314735</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.423310</td>\n",
              "      <td>-0.406138</td>\n",
              "      <td>-0.386348</td>\n",
              "      <td>-0.069756</td>\n",
              "      <td>0.048242</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420</th>\n",
              "      <td>0.533210</td>\n",
              "      <td>-0.080240</td>\n",
              "      <td>0.046441</td>\n",
              "      <td>-0.151484</td>\n",
              "      <td>0.764258</td>\n",
              "      <td>-0.291168</td>\n",
              "      <td>-0.311342</td>\n",
              "      <td>-0.346221</td>\n",
              "      <td>-0.454957</td>\n",
              "      <td>0.054607</td>\n",
              "      <td>0.593331</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.314121</td>\n",
              "      <td>0.366503</td>\n",
              "      <td>0.438772</td>\n",
              "      <td>-0.166050</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.576187</td>\n",
              "      <td>-0.164076</td>\n",
              "      <td>-0.580926</td>\n",
              "      <td>-0.070300</td>\n",
              "      <td>-0.013216</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>-0.028330</td>\n",
              "      <td>-0.008946</td>\n",
              "      <td>0.351103</td>\n",
              "      <td>0.156272</td>\n",
              "      <td>1.451785</td>\n",
              "      <td>0.182851</td>\n",
              "      <td>-0.290181</td>\n",
              "      <td>-0.293224</td>\n",
              "      <td>-0.236843</td>\n",
              "      <td>0.188427</td>\n",
              "      <td>1.089826</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.320716</td>\n",
              "      <td>0.674439</td>\n",
              "      <td>0.759627</td>\n",
              "      <td>-0.013623</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.124983</td>\n",
              "      <td>0.029503</td>\n",
              "      <td>-0.554293</td>\n",
              "      <td>-0.069895</td>\n",
              "      <td>-0.081436</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>0.334487</td>\n",
              "      <td>-0.264613</td>\n",
              "      <td>-0.450354</td>\n",
              "      <td>-0.293239</td>\n",
              "      <td>-0.221321</td>\n",
              "      <td>-0.458328</td>\n",
              "      <td>-0.465614</td>\n",
              "      <td>-0.167191</td>\n",
              "      <td>-0.496205</td>\n",
              "      <td>-0.592163</td>\n",
              "      <td>-0.350079</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.451251</td>\n",
              "      <td>-0.533861</td>\n",
              "      <td>-0.539256</td>\n",
              "      <td>-0.220679</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.593362</td>\n",
              "      <td>-0.444354</td>\n",
              "      <td>0.385381</td>\n",
              "      <td>-0.069726</td>\n",
              "      <td>0.030557</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>0.917870</td>\n",
              "      <td>-0.158708</td>\n",
              "      <td>-0.150505</td>\n",
              "      <td>-0.254048</td>\n",
              "      <td>0.178760</td>\n",
              "      <td>-0.427450</td>\n",
              "      <td>-0.303998</td>\n",
              "      <td>-0.363674</td>\n",
              "      <td>-0.452417</td>\n",
              "      <td>-0.113810</td>\n",
              "      <td>0.132071</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.295659</td>\n",
              "      <td>0.031812</td>\n",
              "      <td>0.085065</td>\n",
              "      <td>-0.299037</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.044369</td>\n",
              "      <td>-0.349224</td>\n",
              "      <td>-0.536808</td>\n",
              "      <td>-0.070366</td>\n",
              "      <td>0.044750</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>-0.037354</td>\n",
              "      <td>-0.185150</td>\n",
              "      <td>0.426329</td>\n",
              "      <td>0.809137</td>\n",
              "      <td>0.777295</td>\n",
              "      <td>1.318265</td>\n",
              "      <td>-0.159124</td>\n",
              "      <td>-0.179545</td>\n",
              "      <td>0.676135</td>\n",
              "      <td>-0.268791</td>\n",
              "      <td>0.372646</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.237357</td>\n",
              "      <td>-0.061309</td>\n",
              "      <td>-0.036771</td>\n",
              "      <td>-0.182193</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.330372</td>\n",
              "      <td>-0.281883</td>\n",
              "      <td>-0.180602</td>\n",
              "      <td>-0.068626</td>\n",
              "      <td>-0.015289</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>-0.174719</td>\n",
              "      <td>-0.261437</td>\n",
              "      <td>-0.356339</td>\n",
              "      <td>-0.266416</td>\n",
              "      <td>-0.103181</td>\n",
              "      <td>-0.372137</td>\n",
              "      <td>-0.463269</td>\n",
              "      <td>-0.010660</td>\n",
              "      <td>-0.383946</td>\n",
              "      <td>-0.617714</td>\n",
              "      <td>-0.323130</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.446945</td>\n",
              "      <td>-0.519836</td>\n",
              "      <td>-0.536671</td>\n",
              "      <td>-0.090489</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.457434</td>\n",
              "      <td>-0.411783</td>\n",
              "      <td>0.188281</td>\n",
              "      <td>-0.068867</td>\n",
              "      <td>0.036072</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>0.071945</td>\n",
              "      <td>-0.272185</td>\n",
              "      <td>-0.236234</td>\n",
              "      <td>-0.275848</td>\n",
              "      <td>-0.115551</td>\n",
              "      <td>-0.207954</td>\n",
              "      <td>-0.158879</td>\n",
              "      <td>-0.270953</td>\n",
              "      <td>-0.392243</td>\n",
              "      <td>-0.404968</td>\n",
              "      <td>-0.237185</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.409905</td>\n",
              "      <td>-0.426844</td>\n",
              "      <td>-0.415497</td>\n",
              "      <td>-0.306917</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.138944</td>\n",
              "      <td>0.508050</td>\n",
              "      <td>-1.165746</td>\n",
              "      <td>-0.068243</td>\n",
              "      <td>0.015048</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>0.776341</td>\n",
              "      <td>-0.271935</td>\n",
              "      <td>-0.449796</td>\n",
              "      <td>-0.314749</td>\n",
              "      <td>-0.301592</td>\n",
              "      <td>-0.431004</td>\n",
              "      <td>-0.316152</td>\n",
              "      <td>-0.394488</td>\n",
              "      <td>-0.570765</td>\n",
              "      <td>-0.470636</td>\n",
              "      <td>-0.324374</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.435587</td>\n",
              "      <td>-0.496551</td>\n",
              "      <td>-0.480793</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.520735</td>\n",
              "      <td>-0.008685</td>\n",
              "      <td>-0.162341</td>\n",
              "      <td>-0.069956</td>\n",
              "      <td>0.016671</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>1.126163</td>\n",
              "      <td>-0.243256</td>\n",
              "      <td>-0.447649</td>\n",
              "      <td>-0.329124</td>\n",
              "      <td>-0.316908</td>\n",
              "      <td>-0.555095</td>\n",
              "      <td>-0.389787</td>\n",
              "      <td>-0.385790</td>\n",
              "      <td>-0.562076</td>\n",
              "      <td>-0.404308</td>\n",
              "      <td>-0.296337</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.376138</td>\n",
              "      <td>-0.390073</td>\n",
              "      <td>-0.365462</td>\n",
              "      <td>-0.401375</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.644796</td>\n",
              "      <td>-0.485482</td>\n",
              "      <td>0.123182</td>\n",
              "      <td>-0.070712</td>\n",
              "      <td>0.051723</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>0.280152</td>\n",
              "      <td>-0.236235</td>\n",
              "      <td>-0.281575</td>\n",
              "      <td>-0.284172</td>\n",
              "      <td>-0.114783</td>\n",
              "      <td>-0.409424</td>\n",
              "      <td>-0.380619</td>\n",
              "      <td>-0.124081</td>\n",
              "      <td>-0.368357</td>\n",
              "      <td>-0.436934</td>\n",
              "      <td>-0.245219</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.363557</td>\n",
              "      <td>-0.355787</td>\n",
              "      <td>-0.349372</td>\n",
              "      <td>-0.183634</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.408354</td>\n",
              "      <td>-0.430735</td>\n",
              "      <td>-0.242005</td>\n",
              "      <td>-0.069261</td>\n",
              "      <td>0.062858</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>-0.262959</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.567476</td>\n",
              "      <td>-0.314618</td>\n",
              "      <td>0.630170</td>\n",
              "      <td>-0.202987</td>\n",
              "      <td>-0.300438</td>\n",
              "      <td>-0.373526</td>\n",
              "      <td>-0.346353</td>\n",
              "      <td>-0.234483</td>\n",
              "      <td>-0.134889</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.421768</td>\n",
              "      <td>-0.313230</td>\n",
              "      <td>-0.283531</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.137690</td>\n",
              "      <td>0.098083</td>\n",
              "      <td>-0.442633</td>\n",
              "      <td>-0.033066</td>\n",
              "      <td>-5.556950</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>-0.345737</td>\n",
              "      <td>-0.202946</td>\n",
              "      <td>0.469012</td>\n",
              "      <td>0.858508</td>\n",
              "      <td>0.834023</td>\n",
              "      <td>1.481242</td>\n",
              "      <td>-0.084682</td>\n",
              "      <td>-0.178608</td>\n",
              "      <td>0.721065</td>\n",
              "      <td>-0.299862</td>\n",
              "      <td>0.383004</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.270589</td>\n",
              "      <td>-0.116131</td>\n",
              "      <td>-0.097087</td>\n",
              "      <td>-0.180567</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.206034</td>\n",
              "      <td>0.101241</td>\n",
              "      <td>-0.476404</td>\n",
              "      <td>-0.067932</td>\n",
              "      <td>-0.041896</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432</th>\n",
              "      <td>-0.006126</td>\n",
              "      <td>-0.002766</td>\n",
              "      <td>0.335487</td>\n",
              "      <td>0.111227</td>\n",
              "      <td>1.459382</td>\n",
              "      <td>0.106884</td>\n",
              "      <td>-0.297650</td>\n",
              "      <td>-0.301044</td>\n",
              "      <td>-0.291613</td>\n",
              "      <td>0.206443</td>\n",
              "      <td>1.106987</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.324690</td>\n",
              "      <td>0.699844</td>\n",
              "      <td>0.787610</td>\n",
              "      <td>-0.010973</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.177573</td>\n",
              "      <td>0.037805</td>\n",
              "      <td>-0.574285</td>\n",
              "      <td>-0.069975</td>\n",
              "      <td>-0.082150</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>0.025259</td>\n",
              "      <td>0.001321</td>\n",
              "      <td>0.257898</td>\n",
              "      <td>-0.037311</td>\n",
              "      <td>1.403108</td>\n",
              "      <td>-0.133294</td>\n",
              "      <td>-0.327886</td>\n",
              "      <td>-0.300794</td>\n",
              "      <td>-0.448932</td>\n",
              "      <td>0.213011</td>\n",
              "      <td>1.081106</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.342463</td>\n",
              "      <td>0.708339</td>\n",
              "      <td>0.797396</td>\n",
              "      <td>-0.001601</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.247672</td>\n",
              "      <td>0.039144</td>\n",
              "      <td>-0.600095</td>\n",
              "      <td>-0.070106</td>\n",
              "      <td>-0.077627</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>1.181606</td>\n",
              "      <td>-0.215771</td>\n",
              "      <td>-0.300319</td>\n",
              "      <td>-0.325886</td>\n",
              "      <td>-0.236538</td>\n",
              "      <td>-0.525067</td>\n",
              "      <td>-0.305928</td>\n",
              "      <td>-0.376623</td>\n",
              "      <td>-0.459291</td>\n",
              "      <td>-0.244746</td>\n",
              "      <td>-0.198045</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.290020</td>\n",
              "      <td>-0.216335</td>\n",
              "      <td>-0.177545</td>\n",
              "      <td>-0.392570</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.490359</td>\n",
              "      <td>-0.478961</td>\n",
              "      <td>-0.457203</td>\n",
              "      <td>-0.070436</td>\n",
              "      <td>0.082599</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>0.815853</td>\n",
              "      <td>-0.254334</td>\n",
              "      <td>-0.380564</td>\n",
              "      <td>-0.076041</td>\n",
              "      <td>-0.141487</td>\n",
              "      <td>-0.160264</td>\n",
              "      <td>-0.411591</td>\n",
              "      <td>-0.347470</td>\n",
              "      <td>-0.373965</td>\n",
              "      <td>-0.514004</td>\n",
              "      <td>-0.232266</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.420583</td>\n",
              "      <td>-0.468771</td>\n",
              "      <td>-0.456632</td>\n",
              "      <td>-0.359829</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.709763</td>\n",
              "      <td>-0.445433</td>\n",
              "      <td>0.564710</td>\n",
              "      <td>-0.070486</td>\n",
              "      <td>0.009533</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>0.474924</td>\n",
              "      <td>-0.234654</td>\n",
              "      <td>-0.107892</td>\n",
              "      <td>0.262532</td>\n",
              "      <td>0.190760</td>\n",
              "      <td>0.399129</td>\n",
              "      <td>-0.335680</td>\n",
              "      <td>-0.285371</td>\n",
              "      <td>0.003048</td>\n",
              "      <td>-0.459728</td>\n",
              "      <td>-0.024821</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.371782</td>\n",
              "      <td>-0.355818</td>\n",
              "      <td>-0.342457</td>\n",
              "      <td>-0.293916</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.602766</td>\n",
              "      <td>-0.384352</td>\n",
              "      <td>0.423500</td>\n",
              "      <td>-0.069842</td>\n",
              "      <td>-0.007674</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>-0.574712</td>\n",
              "      <td>-0.259465</td>\n",
              "      <td>-0.273094</td>\n",
              "      <td>-0.245143</td>\n",
              "      <td>-0.007835</td>\n",
              "      <td>-0.292753</td>\n",
              "      <td>-0.444100</td>\n",
              "      <td>0.101838</td>\n",
              "      <td>-0.293150</td>\n",
              "      <td>-0.626422</td>\n",
              "      <td>-0.296347</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.441344</td>\n",
              "      <td>-0.503160</td>\n",
              "      <td>-0.527698</td>\n",
              "      <td>0.003080</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.328864</td>\n",
              "      <td>-0.333139</td>\n",
              "      <td>-0.048746</td>\n",
              "      <td>-0.068133</td>\n",
              "      <td>0.039363</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>0.577225</td>\n",
              "      <td>-0.256057</td>\n",
              "      <td>-0.244672</td>\n",
              "      <td>-0.298823</td>\n",
              "      <td>-0.176641</td>\n",
              "      <td>-0.290611</td>\n",
              "      <td>-0.140132</td>\n",
              "      <td>-0.388506</td>\n",
              "      <td>-0.433355</td>\n",
              "      <td>-0.299151</td>\n",
              "      <td>-0.208587</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.361759</td>\n",
              "      <td>-0.336953</td>\n",
              "      <td>-0.308299</td>\n",
              "      <td>-0.403920</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.205719</td>\n",
              "      <td>0.377941</td>\n",
              "      <td>-1.203135</td>\n",
              "      <td>-0.068969</td>\n",
              "      <td>0.032266</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>-0.287912</td>\n",
              "      <td>-0.081185</td>\n",
              "      <td>0.533655</td>\n",
              "      <td>0.682856</td>\n",
              "      <td>1.362974</td>\n",
              "      <td>1.070936</td>\n",
              "      <td>-0.202863</td>\n",
              "      <td>-0.201801</td>\n",
              "      <td>0.403446</td>\n",
              "      <td>-0.022191</td>\n",
              "      <td>0.889206</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.274257</td>\n",
              "      <td>0.377438</td>\n",
              "      <td>0.432500</td>\n",
              "      <td>-0.044603</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.510187</td>\n",
              "      <td>-0.067558</td>\n",
              "      <td>-0.320577</td>\n",
              "      <td>-0.068959</td>\n",
              "      <td>-0.073086</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>-0.169900</td>\n",
              "      <td>-0.267428</td>\n",
              "      <td>-0.250020</td>\n",
              "      <td>-0.264365</td>\n",
              "      <td>-0.075266</td>\n",
              "      <td>-0.239668</td>\n",
              "      <td>-0.265550</td>\n",
              "      <td>-0.131532</td>\n",
              "      <td>-0.355183</td>\n",
              "      <td>-0.487791</td>\n",
              "      <td>-0.259311</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.421663</td>\n",
              "      <td>-0.455385</td>\n",
              "      <td>-0.457459</td>\n",
              "      <td>-0.190980</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.209972</td>\n",
              "      <td>0.193451</td>\n",
              "      <td>-0.747996</td>\n",
              "      <td>-0.068201</td>\n",
              "      <td>0.024142</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>-0.463688</td>\n",
              "      <td>-0.254933</td>\n",
              "      <td>-0.276361</td>\n",
              "      <td>-0.250153</td>\n",
              "      <td>-0.020383</td>\n",
              "      <td>-0.316564</td>\n",
              "      <td>-0.447724</td>\n",
              "      <td>0.082599</td>\n",
              "      <td>-0.300806</td>\n",
              "      <td>-0.605722</td>\n",
              "      <td>-0.290732</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.429870</td>\n",
              "      <td>-0.482261</td>\n",
              "      <td>-0.503402</td>\n",
              "      <td>-0.012725</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.351699</td>\n",
              "      <td>-0.391597</td>\n",
              "      <td>-0.025271</td>\n",
              "      <td>-0.068316</td>\n",
              "      <td>0.044521</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.274779</td>\n",
              "      <td>0.949080</td>\n",
              "      <td>-0.324653</td>\n",
              "      <td>1.025318</td>\n",
              "      <td>-0.225107</td>\n",
              "      <td>-0.452084</td>\n",
              "      <td>-0.315689</td>\n",
              "      <td>-0.295274</td>\n",
              "      <td>-0.215371</td>\n",
              "      <td>-0.102084</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.436658</td>\n",
              "      <td>-0.279868</td>\n",
              "      <td>-0.251597</td>\n",
              "      <td>-0.370133</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.192048</td>\n",
              "      <td>-0.345487</td>\n",
              "      <td>0.233608</td>\n",
              "      <td>-0.015866</td>\n",
              "      <td>-8.269482</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>-0.613588</td>\n",
              "      <td>-0.206359</td>\n",
              "      <td>0.350023</td>\n",
              "      <td>0.712665</td>\n",
              "      <td>0.753444</td>\n",
              "      <td>1.198007</td>\n",
              "      <td>-0.239588</td>\n",
              "      <td>-0.003076</td>\n",
              "      <td>0.612828</td>\n",
              "      <td>-0.426893</td>\n",
              "      <td>0.264829</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.306320</td>\n",
              "      <td>-0.198959</td>\n",
              "      <td>-0.199171</td>\n",
              "      <td>-0.041652</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.292608</td>\n",
              "      <td>-0.266609</td>\n",
              "      <td>-0.009161</td>\n",
              "      <td>-0.067921</td>\n",
              "      <td>-0.021489</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>444</th>\n",
              "      <td>0.139479</td>\n",
              "      <td>-0.263397</td>\n",
              "      <td>-0.414350</td>\n",
              "      <td>-0.282967</td>\n",
              "      <td>-0.176078</td>\n",
              "      <td>-0.425320</td>\n",
              "      <td>-0.464716</td>\n",
              "      <td>-0.107245</td>\n",
              "      <td>-0.453213</td>\n",
              "      <td>-0.601948</td>\n",
              "      <td>-0.339759</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.449602</td>\n",
              "      <td>-0.528490</td>\n",
              "      <td>-0.538266</td>\n",
              "      <td>-0.170821</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.541307</td>\n",
              "      <td>-0.431880</td>\n",
              "      <td>0.309898</td>\n",
              "      <td>-0.069397</td>\n",
              "      <td>0.032669</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>0.238481</td>\n",
              "      <td>-0.214197</td>\n",
              "      <td>-0.115088</td>\n",
              "      <td>-0.230643</td>\n",
              "      <td>0.196278</td>\n",
              "      <td>-0.167960</td>\n",
              "      <td>-0.118781</td>\n",
              "      <td>-0.379714</td>\n",
              "      <td>-0.432111</td>\n",
              "      <td>-0.205453</td>\n",
              "      <td>0.075295</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.386141</td>\n",
              "      <td>-0.150820</td>\n",
              "      <td>-0.112185</td>\n",
              "      <td>-0.323162</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.224689</td>\n",
              "      <td>0.632947</td>\n",
              "      <td>-1.344405</td>\n",
              "      <td>-0.068692</td>\n",
              "      <td>-0.012437</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>-0.522798</td>\n",
              "      <td>-0.177059</td>\n",
              "      <td>0.690077</td>\n",
              "      <td>1.253356</td>\n",
              "      <td>1.163072</td>\n",
              "      <td>2.036179</td>\n",
              "      <td>-0.113528</td>\n",
              "      <td>-0.103637</td>\n",
              "      <td>1.106365</td>\n",
              "      <td>-0.300889</td>\n",
              "      <td>0.582261</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.228964</td>\n",
              "      <td>-0.025265</td>\n",
              "      <td>-0.008326</td>\n",
              "      <td>-0.101024</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.289641</td>\n",
              "      <td>-0.205598</td>\n",
              "      <td>0.010251</td>\n",
              "      <td>-0.067957</td>\n",
              "      <td>-0.058028</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>0.307381</td>\n",
              "      <td>-0.034173</td>\n",
              "      <td>0.162065</td>\n",
              "      <td>-0.091269</td>\n",
              "      <td>1.107996</td>\n",
              "      <td>-0.211159</td>\n",
              "      <td>-0.315653</td>\n",
              "      <td>-0.335975</td>\n",
              "      <td>-0.456448</td>\n",
              "      <td>0.153482</td>\n",
              "      <td>0.864129</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.324960</td>\n",
              "      <td>0.562994</td>\n",
              "      <td>0.646429</td>\n",
              "      <td>-0.087975</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.940506</td>\n",
              "      <td>-0.055379</td>\n",
              "      <td>-0.606828</td>\n",
              "      <td>-0.070261</td>\n",
              "      <td>-0.047247</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>1.084160</td>\n",
              "      <td>-0.220601</td>\n",
              "      <td>-0.280674</td>\n",
              "      <td>-0.321122</td>\n",
              "      <td>-0.220869</td>\n",
              "      <td>-0.483657</td>\n",
              "      <td>-0.272260</td>\n",
              "      <td>-0.377973</td>\n",
              "      <td>-0.447746</td>\n",
              "      <td>-0.242696</td>\n",
              "      <td>-0.192931</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.296018</td>\n",
              "      <td>-0.224394</td>\n",
              "      <td>-0.186310</td>\n",
              "      <td>-0.393856</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.431825</td>\n",
              "      <td>-0.334840</td>\n",
              "      <td>-0.622899</td>\n",
              "      <td>-0.070170</td>\n",
              "      <td>0.076322</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>0.957231</td>\n",
              "      <td>-0.268497</td>\n",
              "      <td>-0.565332</td>\n",
              "      <td>-0.326044</td>\n",
              "      <td>-0.365803</td>\n",
              "      <td>-0.563738</td>\n",
              "      <td>-0.468483</td>\n",
              "      <td>-0.358624</td>\n",
              "      <td>-0.633494</td>\n",
              "      <td>-0.560914</td>\n",
              "      <td>-0.383037</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.456518</td>\n",
              "      <td>-0.551013</td>\n",
              "      <td>-0.542418</td>\n",
              "      <td>-0.379897</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.759599</td>\n",
              "      <td>-0.484189</td>\n",
              "      <td>0.626429</td>\n",
              "      <td>-0.070777</td>\n",
              "      <td>0.023812</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>-0.567238</td>\n",
              "      <td>-0.172890</td>\n",
              "      <td>0.735634</td>\n",
              "      <td>1.300535</td>\n",
              "      <td>1.213402</td>\n",
              "      <td>2.115417</td>\n",
              "      <td>-0.098606</td>\n",
              "      <td>-0.094536</td>\n",
              "      <td>1.164082</td>\n",
              "      <td>-0.285016</td>\n",
              "      <td>0.616200</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.217689</td>\n",
              "      <td>-0.000504</td>\n",
              "      <td>0.017345</td>\n",
              "      <td>-0.091412</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.266716</td>\n",
              "      <td>-0.196776</td>\n",
              "      <td>-0.039680</td>\n",
              "      <td>-0.067853</td>\n",
              "      <td>-0.058804</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>1.069660</td>\n",
              "      <td>-0.268063</td>\n",
              "      <td>-0.583291</td>\n",
              "      <td>-0.330936</td>\n",
              "      <td>-0.385213</td>\n",
              "      <td>-0.581596</td>\n",
              "      <td>-0.468415</td>\n",
              "      <td>-0.394168</td>\n",
              "      <td>-0.658370</td>\n",
              "      <td>-0.551745</td>\n",
              "      <td>-0.382905</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.457012</td>\n",
              "      <td>-0.548807</td>\n",
              "      <td>-0.537249</td>\n",
              "      <td>-0.408040</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.782052</td>\n",
              "      <td>-0.489387</td>\n",
              "      <td>0.666225</td>\n",
              "      <td>-0.070971</td>\n",
              "      <td>0.022115</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>1.029784</td>\n",
              "      <td>-0.269625</td>\n",
              "      <td>-0.566578</td>\n",
              "      <td>-0.329604</td>\n",
              "      <td>-0.379346</td>\n",
              "      <td>-0.560914</td>\n",
              "      <td>-0.446409</td>\n",
              "      <td>-0.394457</td>\n",
              "      <td>-0.646109</td>\n",
              "      <td>-0.542578</td>\n",
              "      <td>-0.379616</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.454262</td>\n",
              "      <td>-0.545693</td>\n",
              "      <td>-0.533807</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.750777</td>\n",
              "      <td>-0.420208</td>\n",
              "      <td>0.548199</td>\n",
              "      <td>-0.070823</td>\n",
              "      <td>0.021680</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>0.807549</td>\n",
              "      <td>-0.194216</td>\n",
              "      <td>-0.356788</td>\n",
              "      <td>-0.251581</td>\n",
              "      <td>0.100297</td>\n",
              "      <td>-0.462641</td>\n",
              "      <td>-0.429925</td>\n",
              "      <td>-0.376139</td>\n",
              "      <td>-0.606302</td>\n",
              "      <td>-0.340418</td>\n",
              "      <td>0.017955</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.425755</td>\n",
              "      <td>-0.204144</td>\n",
              "      <td>-0.170740</td>\n",
              "      <td>-0.302727</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.231092</td>\n",
              "      <td>-0.345782</td>\n",
              "      <td>0.327955</td>\n",
              "      <td>-0.070775</td>\n",
              "      <td>-0.005553</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454</th>\n",
              "      <td>-0.122303</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.364803</td>\n",
              "      <td>-0.307297</td>\n",
              "      <td>0.430072</td>\n",
              "      <td>-0.197060</td>\n",
              "      <td>-0.239975</td>\n",
              "      <td>-0.378909</td>\n",
              "      <td>-0.366516</td>\n",
              "      <td>-0.259338</td>\n",
              "      <td>-0.156061</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.416062</td>\n",
              "      <td>-0.335844</td>\n",
              "      <td>-0.307798</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.121887</td>\n",
              "      <td>0.274476</td>\n",
              "      <td>-0.722627</td>\n",
              "      <td>-0.042083</td>\n",
              "      <td>-4.132219</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>0.842140</td>\n",
              "      <td>-0.271335</td>\n",
              "      <td>-0.480115</td>\n",
              "      <td>-0.318606</td>\n",
              "      <td>-0.321778</td>\n",
              "      <td>-0.464731</td>\n",
              "      <td>-0.349969</td>\n",
              "      <td>-0.394480</td>\n",
              "      <td>-0.590326</td>\n",
              "      <td>-0.489314</td>\n",
              "      <td>-0.338716</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.440435</td>\n",
              "      <td>-0.509309</td>\n",
              "      <td>-0.494556</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.580458</td>\n",
              "      <td>-0.115524</td>\n",
              "      <td>0.022128</td>\n",
              "      <td>-0.070181</td>\n",
              "      <td>0.017971</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>-0.557166</td>\n",
              "      <td>-0.173123</td>\n",
              "      <td>0.729754</td>\n",
              "      <td>1.291194</td>\n",
              "      <td>1.205112</td>\n",
              "      <td>2.100264</td>\n",
              "      <td>-0.099757</td>\n",
              "      <td>-0.096152</td>\n",
              "      <td>1.154807</td>\n",
              "      <td>-0.284708</td>\n",
              "      <td>0.611570</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.218063</td>\n",
              "      <td>-0.001660</td>\n",
              "      <td>0.016316</td>\n",
              "      <td>-0.093137</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.267926</td>\n",
              "      <td>-0.198394</td>\n",
              "      <td>-0.042359</td>\n",
              "      <td>-0.067867</td>\n",
              "      <td>-0.057977</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>457</th>\n",
              "      <td>-0.087180</td>\n",
              "      <td>-0.202206</td>\n",
              "      <td>0.341673</td>\n",
              "      <td>0.820750</td>\n",
              "      <td>0.738548</td>\n",
              "      <td>1.321422</td>\n",
              "      <td>-0.210522</td>\n",
              "      <td>-0.182984</td>\n",
              "      <td>0.624643</td>\n",
              "      <td>-0.370240</td>\n",
              "      <td>0.317202</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.291320</td>\n",
              "      <td>-0.169589</td>\n",
              "      <td>-0.154212</td>\n",
              "      <td>-0.185243</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.426355</td>\n",
              "      <td>-0.283644</td>\n",
              "      <td>0.190681</td>\n",
              "      <td>-0.068780</td>\n",
              "      <td>-0.036043</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>-0.503547</td>\n",
              "      <td>-0.141195</td>\n",
              "      <td>0.685302</td>\n",
              "      <td>1.120291</td>\n",
              "      <td>1.289198</td>\n",
              "      <td>1.808672</td>\n",
              "      <td>-0.130328</td>\n",
              "      <td>-0.125856</td>\n",
              "      <td>0.935335</td>\n",
              "      <td>-0.197152</td>\n",
              "      <td>0.722550</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.235663</td>\n",
              "      <td>0.130718</td>\n",
              "      <td>0.160753</td>\n",
              "      <td>-0.070338</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.000526</td>\n",
              "      <td>-0.148186</td>\n",
              "      <td>-0.126429</td>\n",
              "      <td>-0.068182</td>\n",
              "      <td>-0.066150</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>-0.388947</td>\n",
              "      <td>-0.184786</td>\n",
              "      <td>0.583023</td>\n",
              "      <td>1.120430</td>\n",
              "      <td>1.032629</td>\n",
              "      <td>1.816556</td>\n",
              "      <td>-0.143331</td>\n",
              "      <td>-0.128018</td>\n",
              "      <td>0.958347</td>\n",
              "      <td>-0.322199</td>\n",
              "      <td>0.500817</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.248124</td>\n",
              "      <td>-0.069611</td>\n",
              "      <td>-0.053152</td>\n",
              "      <td>-0.126902</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.331649</td>\n",
              "      <td>-0.229579</td>\n",
              "      <td>0.065691</td>\n",
              "      <td>-0.068210</td>\n",
              "      <td>-0.051273</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>-0.504571</td>\n",
              "      <td>-0.141480</td>\n",
              "      <td>0.686022</td>\n",
              "      <td>1.122368</td>\n",
              "      <td>1.288847</td>\n",
              "      <td>1.812174</td>\n",
              "      <td>-0.129984</td>\n",
              "      <td>-0.125495</td>\n",
              "      <td>0.937861</td>\n",
              "      <td>-0.197982</td>\n",
              "      <td>0.721759</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.235480</td>\n",
              "      <td>0.129547</td>\n",
              "      <td>0.159463</td>\n",
              "      <td>-0.070460</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.002951</td>\n",
              "      <td>-0.148569</td>\n",
              "      <td>-0.125507</td>\n",
              "      <td>-0.068178</td>\n",
              "      <td>-0.066117</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>461</th>\n",
              "      <td>0.153316</td>\n",
              "      <td>-0.189561</td>\n",
              "      <td>0.315031</td>\n",
              "      <td>0.632315</td>\n",
              "      <td>0.620369</td>\n",
              "      <td>1.031423</td>\n",
              "      <td>-0.180901</td>\n",
              "      <td>-0.210135</td>\n",
              "      <td>0.500556</td>\n",
              "      <td>-0.262953</td>\n",
              "      <td>0.285006</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.244435</td>\n",
              "      <td>-0.083189</td>\n",
              "      <td>-0.056244</td>\n",
              "      <td>-0.214860</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.353277</td>\n",
              "      <td>-0.312507</td>\n",
              "      <td>-0.231311</td>\n",
              "      <td>-0.068905</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>1.161232</td>\n",
              "      <td>-0.225871</td>\n",
              "      <td>-0.354459</td>\n",
              "      <td>-0.327076</td>\n",
              "      <td>-0.266072</td>\n",
              "      <td>-0.536102</td>\n",
              "      <td>-0.336744</td>\n",
              "      <td>-0.379991</td>\n",
              "      <td>-0.497062</td>\n",
              "      <td>-0.303381</td>\n",
              "      <td>-0.234165</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.321666</td>\n",
              "      <td>-0.280180</td>\n",
              "      <td>-0.246600</td>\n",
              "      <td>-0.395806</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.547111</td>\n",
              "      <td>-0.481357</td>\n",
              "      <td>-0.243925</td>\n",
              "      <td>-0.070537</td>\n",
              "      <td>0.071253</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>1.141650</td>\n",
              "      <td>-0.235578</td>\n",
              "      <td>-0.406495</td>\n",
              "      <td>-0.328219</td>\n",
              "      <td>-0.294458</td>\n",
              "      <td>-0.546707</td>\n",
              "      <td>-0.366362</td>\n",
              "      <td>-0.383229</td>\n",
              "      <td>-0.533364</td>\n",
              "      <td>-0.359737</td>\n",
              "      <td>-0.268881</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.352082</td>\n",
              "      <td>-0.341542</td>\n",
              "      <td>-0.312970</td>\n",
              "      <td>-0.398916</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.601656</td>\n",
              "      <td>-0.483661</td>\n",
              "      <td>-0.038941</td>\n",
              "      <td>-0.070635</td>\n",
              "      <td>0.060348</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>0.189056</td>\n",
              "      <td>-0.274489</td>\n",
              "      <td>-0.229559</td>\n",
              "      <td>-0.281409</td>\n",
              "      <td>-0.135058</td>\n",
              "      <td>-0.192597</td>\n",
              "      <td>-0.107225</td>\n",
              "      <td>-0.338466</td>\n",
              "      <td>-0.410189</td>\n",
              "      <td>-0.364863</td>\n",
              "      <td>-0.226471</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.404211</td>\n",
              "      <td>-0.413022</td>\n",
              "      <td>-0.395178</td>\n",
              "      <td>-0.363058</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.104549</td>\n",
              "      <td>0.660391</td>\n",
              "      <td>-1.368036</td>\n",
              "      <td>-0.068262</td>\n",
              "      <td>0.010644</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>0.826403</td>\n",
              "      <td>-0.254943</td>\n",
              "      <td>-0.389001</td>\n",
              "      <td>-0.086517</td>\n",
              "      <td>-0.151768</td>\n",
              "      <td>-0.177574</td>\n",
              "      <td>-0.413940</td>\n",
              "      <td>-0.349392</td>\n",
              "      <td>-0.385631</td>\n",
              "      <td>-0.515684</td>\n",
              "      <td>-0.238685</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.422094</td>\n",
              "      <td>-0.472266</td>\n",
              "      <td>-0.460165</td>\n",
              "      <td>-0.361869</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.713074</td>\n",
              "      <td>-0.447324</td>\n",
              "      <td>0.569080</td>\n",
              "      <td>-0.070506</td>\n",
              "      <td>0.010065</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>0.724313</td>\n",
              "      <td>-0.245770</td>\n",
              "      <td>-0.255118</td>\n",
              "      <td>-0.305293</td>\n",
              "      <td>-0.189474</td>\n",
              "      <td>-0.346624</td>\n",
              "      <td>-0.178469</td>\n",
              "      <td>-0.385450</td>\n",
              "      <td>-0.437530</td>\n",
              "      <td>-0.282771</td>\n",
              "      <td>-0.204044</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.342684</td>\n",
              "      <td>-0.304294</td>\n",
              "      <td>-0.272904</td>\n",
              "      <td>-0.401000</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.271324</td>\n",
              "      <td>0.171127</td>\n",
              "      <td>-1.034779</td>\n",
              "      <td>-0.069317</td>\n",
              "      <td>0.045049</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>0.890126</td>\n",
              "      <td>-0.270898</td>\n",
              "      <td>-0.502227</td>\n",
              "      <td>-0.321418</td>\n",
              "      <td>-0.336500</td>\n",
              "      <td>-0.489328</td>\n",
              "      <td>-0.374632</td>\n",
              "      <td>-0.394474</td>\n",
              "      <td>-0.604591</td>\n",
              "      <td>-0.502935</td>\n",
              "      <td>-0.349175</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.443971</td>\n",
              "      <td>-0.518614</td>\n",
              "      <td>-0.504594</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.624015</td>\n",
              "      <td>-0.193442</td>\n",
              "      <td>0.156662</td>\n",
              "      <td>-0.070345</td>\n",
              "      <td>0.018920</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>-0.221238</td>\n",
              "      <td>-0.099007</td>\n",
              "      <td>0.052162</td>\n",
              "      <td>-0.116790</td>\n",
              "      <td>0.861199</td>\n",
              "      <td>-0.196788</td>\n",
              "      <td>-0.379343</td>\n",
              "      <td>-0.136792</td>\n",
              "      <td>-0.386526</td>\n",
              "      <td>-0.116018</td>\n",
              "      <td>0.548248</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.381345</td>\n",
              "      <td>0.239116</td>\n",
              "      <td>0.283511</td>\n",
              "      <td>0.007396</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.634966</td>\n",
              "      <td>-0.124013</td>\n",
              "      <td>-0.361449</td>\n",
              "      <td>-0.069342</td>\n",
              "      <td>-0.031923</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>469</th>\n",
              "      <td>0.394111</td>\n",
              "      <td>-0.275420</td>\n",
              "      <td>-0.273672</td>\n",
              "      <td>-0.292345</td>\n",
              "      <td>-0.184326</td>\n",
              "      <td>-0.235080</td>\n",
              "      <td>-0.119704</td>\n",
              "      <td>-0.394536</td>\n",
              "      <td>-0.457135</td>\n",
              "      <td>-0.362138</td>\n",
              "      <td>-0.241061</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.407423</td>\n",
              "      <td>-0.422438</td>\n",
              "      <td>-0.400841</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.173796</td>\n",
              "      <td>0.611953</td>\n",
              "      <td>-1.233942</td>\n",
              "      <td>-0.068648</td>\n",
              "      <td>0.009116</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c72b0de1-b763-4d6c-b0aa-7902e86f58e7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c72b0de1-b763-4d6c-b0aa-7902e86f58e7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c72b0de1-b763-4d6c-b0aa-7902e86f58e7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     openYear  salescost1  interest1      ctax1   profit1  quickAsset1  \\\n",
              "0    0.286341   -0.276402  -0.617597  -0.205325 -0.036654    -0.445741   \n",
              "1   -0.501097   -0.276402   3.419262   0.367563  0.558400     1.167249   \n",
              "2    0.061359   -0.276402   3.795674  -0.332867 -0.163469     0.300603   \n",
              "3   -2.525937    0.272113   2.546376  -0.332867  0.524769     1.121564   \n",
              "4   -1.963482    2.282292   0.926513   1.781553  0.163683     1.219979   \n",
              "5   -0.501097    0.540174   0.867023  -0.116245 -0.835136     0.002257   \n",
              "6    0.398832    0.813740   4.325947  -0.285140  0.079059     1.111742   \n",
              "7    0.398832   -0.259322   0.571773  -0.332867 -0.232834    -0.413541   \n",
              "8    0.398832   -0.276402   3.856058  -0.332867 -0.955170     0.898376   \n",
              "9   -2.638429    4.129403   2.396504   0.222862 -3.530603     2.290492   \n",
              "10  -2.525937   -0.276402   0.427624  -0.332867 -1.958237    -0.033736   \n",
              "11   0.848797    0.765085   2.891795  -0.173120  0.114419     2.824658   \n",
              "12  -1.176044   -0.276402   0.557236  -0.007898 -0.114114     0.230034   \n",
              "13   0.061359   -0.134787   0.947165  -0.332867 -0.254478    -0.257839   \n",
              "14   0.173850   -0.276402   9.028202   0.973668  0.209211     2.194315   \n",
              "15  -2.525937   -0.276402   0.125449  -0.332867 -1.811782    -0.390557   \n",
              "16   0.398832   -0.184720   0.532729   0.599440  0.631646    -0.201404   \n",
              "17  -0.951061    0.516099   0.094727   0.039989  0.113957    -0.055739   \n",
              "18  -0.051132   -0.276402   0.253057  -0.257636 -0.152458    -0.422903   \n",
              "19  -0.838570    0.438402   0.462978   0.315572 -0.186000    -0.278473   \n",
              "20  -0.163623   -0.096146   0.437162  -0.292900 -0.137870    -0.498448   \n",
              "21  -0.388606    0.710018   0.348849   1.998534  0.536067     0.322413   \n",
              "22   0.398832    0.040210   0.509802   0.872403  0.797390    -0.180503   \n",
              "23  -0.388606    0.007300  -0.073679  -0.057915  0.001905    -0.305346   \n",
              "24   0.398832   -0.218258  -0.125105  -0.332867 -0.479270    -0.454082   \n",
              "25  -0.051132   -0.164877  -0.084477   1.163972  0.620810     2.096655   \n",
              "26   1.073779   -0.276402   1.089551  -0.332867  0.885374    -0.208105   \n",
              "27  -0.163623    0.034039  -0.061440  -0.332867 -0.724426    -0.269630   \n",
              "28   0.398832   -0.276402  -0.103861  -0.332867 -1.366429    -0.329160   \n",
              "29   0.061359   11.865412  -0.474031   0.898139  0.637125     5.798874   \n",
              "30   0.961288    0.233070   1.696367   0.430139  0.136929     0.113168   \n",
              "31   0.736306    0.024721  -0.434000   2.869688  2.144820     2.497966   \n",
              "32   0.398832   -0.231135  -0.103038  -0.332867 -0.401125    -0.008166   \n",
              "33   0.961288   -0.206067  -0.240598   0.113011  0.186962    -0.306939   \n",
              "34  -0.163623   -0.052975   0.102007  -0.332867 -0.050497    -0.366199   \n",
              "35  -1.850991    0.494343   0.195210   0.100467 -0.165348     0.039473   \n",
              "36   0.286341   -0.214190  -0.228380  -0.261833 -0.082479    -0.369934   \n",
              "37  -0.726079   -0.276402   0.469471  -0.233295 -0.197694     0.367768   \n",
              "38   0.286341   -0.216052  -0.056753  -0.332867 -0.125691    -0.476031   \n",
              "39  -0.726079   -0.276402   1.795039  -0.232414 -0.173753    -0.086613   \n",
              "40   0.511323   -0.257169  -0.086125  -0.332867  0.153035    -0.383865   \n",
              "41  -0.501097   -0.276402   0.017977  -0.318138 -0.191384    -0.321819   \n",
              "42   0.286341   -0.259637  -0.039636  -0.332867 -0.708562    -0.446660   \n",
              "43   0.398832   -0.123449  -0.162182   0.277302  0.348843    -0.273271   \n",
              "44   1.411252    0.225971   0.299881  -0.332867 -0.194585    -0.137495   \n",
              "45  -1.288535   -0.187186   0.930632  -0.332867 -0.091576    -0.303518   \n",
              "46  -2.413446   -0.276402  -0.280551   0.020255 -0.060847    -0.050828   \n",
              "47   0.623815   -0.266304  -0.026462  -0.332867 -0.744164    -0.372857   \n",
              "48   1.073779   -0.091667  -0.304066  -0.191388  0.111626    -0.402957   \n",
              "49  -1.850991    0.003455  -0.360150  -0.332867 -0.708183     0.412100   \n",
              "50  -0.613588   -0.238415  -0.469249  -0.262642 -0.227424    -0.531692   \n",
              "51  -2.525937    2.105869   1.637513   0.231692 -0.180190     0.164093   \n",
              "52   0.848797   -0.249935  -0.386751  -0.315209 -0.394770    -0.528626   \n",
              "53  -2.188464   -0.015299   1.048615  -0.332867 -0.870133    -0.195961   \n",
              "54  -0.163623   -0.258700  -0.275310  -0.243297 -0.001360    -0.297851   \n",
              "55  -0.613588   -0.258700  -0.275310  -0.243297 -0.001360    -0.297851   \n",
              "56   1.073779   -0.264175  -0.659188  -0.332867 -0.064623    -0.546783   \n",
              "57   0.848797    0.882990   0.697981  -0.331150  0.344123     0.259596   \n",
              "58   1.298761   -0.255913  -0.394401  -0.332867 -0.608742    -0.574589   \n",
              "59   0.286341   -0.276402   0.097083   0.325020  2.859758    -0.077456   \n",
              "60  -0.951061    0.371104  -0.289928  -0.332867 -0.643577     0.116173   \n",
              "61   0.398832   -0.250258  -0.287087  -0.332867 -0.383024    -0.300096   \n",
              "62   0.173850   -0.251754  -0.562970  -0.135949 -0.002033    -0.162143   \n",
              "63   0.736306   -0.246228  -0.323553  -0.097074  0.052903    -0.266219   \n",
              "64   0.398832   -0.218046  -0.435144   0.523078 -0.236279     0.027907   \n",
              "65   0.173850    0.280814  -0.179285  -0.322986 -0.228813    -0.407856   \n",
              "66  -0.276115   -0.263383  -0.414951  -0.152464  0.113848    -0.164931   \n",
              "67  -0.051132   -0.232584  -0.298732  -0.332867  0.126365    -0.448028   \n",
              "68   0.173850   -0.255539  -0.483342  -0.259757 -0.093121    -0.287623   \n",
              "69  -1.850991   -0.187720   0.488945  -0.317987 -0.614185    -0.348426   \n",
              "70  -0.838570   -0.106506  -0.020465  -0.266247 -0.014273     0.007815   \n",
              "71   0.511323   -0.182407  -0.120413  -0.332867 -0.264097    -0.501310   \n",
              "72   0.961288   -0.276402  -0.151613  -0.332370 -0.224561    -0.375909   \n",
              "73   0.736306   -0.276402  -0.450795  -0.277490 -0.152243    -0.304880   \n",
              "74   0.286341   -0.276402  -0.224014  -0.286028 -0.151263    -0.179840   \n",
              "75   0.286341   -0.111725  -0.662740  -0.332867  0.041342    -0.261430   \n",
              "76   1.186270   -0.208545   0.194371  -0.332867 -0.207619    -0.440242   \n",
              "77   0.173850   -0.250467  -0.427030  -0.101319 -0.019928     0.064828   \n",
              "78   0.398832   -0.044083   0.195733  -0.332867 -0.169675    -0.148024   \n",
              "79   0.623815   -0.172243  -0.061917  -0.332867  0.033416    -0.368658   \n",
              "80   0.173850    0.007014   2.920838  -0.332867 -0.085458    -0.033545   \n",
              "81   0.398832   -0.246970  -0.463587  -0.329972 -0.233497    -0.479997   \n",
              "82   0.961288   -0.276402  -0.452207  -0.256527 -0.115071    -0.216715   \n",
              "83   0.286341   -0.223353   0.068817  -0.305398 -0.385848    -0.413269   \n",
              "84   0.511323   -0.261049  -0.223124  -0.329995 -0.229214    -0.148175   \n",
              "85  -1.850991    0.063734  -0.491618   0.106601  0.285010     0.380528   \n",
              "86  -1.850991    0.666788  -0.415228  -0.263320 -0.166946    -0.382361   \n",
              "87   0.848797   -0.276402   0.818721  -0.154085  0.081219    -0.097311   \n",
              "88   0.398832   -0.242691  -0.118411  -0.332867 -0.178742    -0.423756   \n",
              "89   0.511323   -0.211926  -0.248851  -0.332867 -0.710158    -0.280342   \n",
              "90   0.848797    1.137289   0.537783  -0.005545 -0.174763     0.644135   \n",
              "91  -1.288535   -0.276402  -0.419969   0.022732  0.018779     0.510892   \n",
              "92   0.173850   -0.276402  -0.480935  -0.332867 -0.189919    -0.387945   \n",
              "93  -0.726079   -0.276402  -0.169021  -0.307251 -0.162992    -0.481192   \n",
              "94  -0.051132   -0.140117  -0.189634   1.241994  0.222011    -0.024515   \n",
              "95   0.848797   -0.263871  -0.548668  -0.330626 -0.231631    -0.440634   \n",
              "96  -0.951061   -0.276402  -0.547581  -0.332867 -0.186906    -0.507248   \n",
              "97  -0.613588   -0.139691  -0.121810   0.028933  0.104304    -0.393875   \n",
              "98   1.073779   -0.276402  -0.391716  -0.277529 -0.174103    -0.218114   \n",
              "99  -2.300955   -0.262495  -0.512110  -0.332867 -0.239160    -0.577522   \n",
              "100 -2.975902   -0.262495  -0.512110  -0.332867 -0.239160    -0.577522   \n",
              "101  0.061359   -0.207376  -0.367538  -0.184592 -0.164113    -0.229274   \n",
              "102  0.961288   -0.227294  -0.088857  -0.332867 -0.206553    -0.386004   \n",
              "103  0.736306   -0.276402  -0.548939  -0.277432 -0.042041    -0.475019   \n",
              "104  0.286341   -0.276402  -0.342460  -0.079788  0.051115    -0.408200   \n",
              "105  0.511323   -0.153434   0.090132  -0.332867  0.539346    -0.135059   \n",
              "106  0.623815   -0.261517  -0.550027  -0.332867 -0.131951    -0.488412   \n",
              "107  0.398832   -0.262295  -0.239262  -0.332625 -0.501236    -0.304866   \n",
              "108 -1.738499   -0.157121   0.189625  -0.112252 -0.486282    -0.062500   \n",
              "109 -0.388606    0.434942  -0.662740  -0.332867 -0.109322    -0.451762   \n",
              "110  0.736306   -0.264409   0.063228  -0.330480 -0.046898    -0.412164   \n",
              "111  0.286341   -0.143387   0.369543  -0.332867 -0.096053    -0.472190   \n",
              "112  0.511323   -0.254780   0.205964  -0.332867 -0.177838    -0.372395   \n",
              "113 -0.613588   -0.276402  -0.421095  -0.015811 -0.056057    -0.407442   \n",
              "114 -2.413446    1.385676   0.674407  -0.332867 -0.194276    -0.219772   \n",
              "115 -1.850991   -0.087548   0.074880  -0.332867  0.022675     0.277109   \n",
              "116  0.173850   -0.229704  -0.323595  -0.239303 -0.129640    -0.430092   \n",
              "117  0.511323   -0.253549  -0.446677  -0.332867 -0.084542    -0.430518   \n",
              "118  0.286341   -0.253157  -0.569685  -0.295318 -0.161619    -0.484505   \n",
              "119 -0.163623   -0.201112  -0.277338  -0.332867 -0.151178    -0.212457   \n",
              "120 -0.613588   -0.276402   1.072698  -0.332867  1.128974    -0.217762   \n",
              "121  0.848797   -0.164119   0.255388  -0.332867 -0.286670    -0.358855   \n",
              "122  0.061359   -0.261669  -0.469152  -0.303170 -0.182032    -0.521523   \n",
              "123  0.398832   -0.276402  -0.448287  -0.331080 -0.475491    -0.558990   \n",
              "124  1.298761   -0.276402  -0.033035  -0.270069 -0.037508    -0.320924   \n",
              "125  0.961288   -0.238967  -0.288492  -0.296952 -0.112375    -0.367904   \n",
              "126  1.073779   -0.276402  -0.069751  -0.254329 -0.131281    -0.419945   \n",
              "127 -0.613588   -0.276402  -0.452080  -0.288103  0.556651    -0.267549   \n",
              "128  0.511323   -0.276402   1.351978  -0.332867 -0.234375    -0.213372   \n",
              "129  0.398832   -0.206104   0.098874   0.068507  0.310333     0.257691   \n",
              "130  0.961288   -0.276402  -0.160332  -0.332867 -0.114937    -0.219067   \n",
              "131  0.511323   -0.210288   0.217537  -0.261437  0.028739    -0.098397   \n",
              "132  0.736306   -0.276402   0.265906  -0.332867 -0.163669    -0.127712   \n",
              "133  1.298761   -0.217395  -0.662740  -0.332867 -0.013750    -0.478444   \n",
              "134  1.186270   -0.213459  -0.287926  -0.325614 -0.229777    -0.522542   \n",
              "135  0.848797   -0.276402  -0.354209  -0.332867 -1.469729    -0.571740   \n",
              "136 -0.276115   -0.276402  -0.191190  -0.332867 -0.232442    -0.241160   \n",
              "137  0.173850   -0.212706  -0.484874  -0.293479 -0.178470    -0.374127   \n",
              "138  0.848797   -0.230302  -0.590690  -0.332867 -0.245154    -0.405415   \n",
              "139  0.398832   -0.261188  -0.627753  -0.283627 -0.407303    -0.500558   \n",
              "140  0.511323   -0.257342  -0.495542  -0.332867 -0.167225    -0.512643   \n",
              "141 -2.750920   -0.276402  -0.629029  -0.302202 -0.241870    -0.443259   \n",
              "142 -0.613588   -0.173967  -0.653995  -0.332867 -0.233908    -0.368696   \n",
              "143  0.623815   -0.190396  -0.307175  -0.320226 -0.213453    -0.506987   \n",
              "144  0.961288   -0.255907  -0.138128  -0.123042 -0.075041    -0.408222   \n",
              "145  0.623815   -0.208378  -0.438012  -0.303573 -0.197820    -0.451962   \n",
              "146  0.286341   -0.276402  -0.662671  -0.260427 -0.120927    -0.369169   \n",
              "147  0.286341   -0.260525  -0.389326  -0.332867 -0.213267    -0.544263   \n",
              "148  1.411252   -0.276402  -0.389795  -0.332867 -1.526884    -0.575812   \n",
              "149  0.286341   -0.257292  -0.183139  -0.332867 -0.121434    -0.383148   \n",
              "150  0.511323   -0.276402  -0.544127  -0.317669 -0.206046    -0.567063   \n",
              "151 -1.850991   -0.049979  -0.577207   1.857285  1.435703     0.382495   \n",
              "152  0.848797    0.016014   0.288029  -0.025671  1.482471    -0.123995   \n",
              "153  0.061359    0.016014   0.288029  -0.025671  1.482471    -0.123995   \n",
              "154 -0.051132   -0.258164  -0.440125  -0.288285 -0.106848    -0.521315   \n",
              "155 -0.051132   -0.171631  -0.191039  -0.261212 -0.177870    -0.320020   \n",
              "156  0.961288   -0.251563  -0.355794  -0.328736 -0.199893    -0.489107   \n",
              "157  0.173850   -0.276402  -0.604876  -0.301306 -0.013511    -0.426096   \n",
              "158 -2.413446   -0.276402  -0.185845  -0.332867 -1.452491     1.936591   \n",
              "159  0.848797   -0.276402   2.689982  -0.332867 -0.276913     0.706369   \n",
              "160 -2.075973   -0.025183  -0.662740  -0.332867  0.354207     0.427099   \n",
              "161 -2.413446    0.545362  -0.662740  -0.330496 -1.376307     1.622064   \n",
              "162  0.736306   -0.276402   0.614447  -0.332867 -0.043020    -0.141141   \n",
              "163 -0.388606   -0.276402  -0.109922  -0.326684 -0.135826    -0.401479   \n",
              "164  1.298761    0.165375  -0.232811  -0.332867 -0.220013    -0.525370   \n",
              "165  0.061359    0.022002  -0.662740   0.809971  0.431208    -0.017844   \n",
              "166 -0.051132   -0.260430  -0.538183  -0.323538 -0.297878    -0.366694   \n",
              "167 -0.276115   -0.271883  -0.643282  -0.327855 -0.241242    -0.590597   \n",
              "168 -0.501097   -0.261606  -0.646125  -0.332867 -0.219413    -0.580219   \n",
              "169  1.073779   -0.276402  -0.328703  -0.332867 -0.146157    -0.483037   \n",
              "170 -1.963482   -0.248113  -0.633813  -0.015690  0.143903    -0.341429   \n",
              "171  1.073779   -0.104722  -0.099839  -0.332867 -0.451654    -0.508965   \n",
              "172  0.398832   -0.276402  -0.451175  -0.332583 -0.302667    -0.468331   \n",
              "173  0.736306   -0.271829  -0.449730  -0.258513 -0.110432    -0.568636   \n",
              "174  0.511323   -0.271550  -0.662740  -0.332867 -0.092833    -0.516481   \n",
              "175  1.186270   -0.240973   0.659702  -0.332867  0.362634    -0.349700   \n",
              "176  0.173850   -0.266673  -0.068724  -0.332867 -0.266186    -0.573705   \n",
              "177  1.073779    0.176585  -0.662740  -0.332867 -0.220929    -0.286091   \n",
              "178  0.736306   -0.276402  -0.456569  -0.332867 -0.154085    -0.519123   \n",
              "179  0.286341   -0.194056   0.045046  -0.332867  0.145413     0.047939   \n",
              "180 -0.726079   -0.276402  -0.662740  -0.177215 -0.022373    -0.171416   \n",
              "181 -0.613588   -0.171818   0.762689   1.343518  1.251549     2.185145   \n",
              "182  1.298761   -0.269223  -0.586850  -0.332183 -0.392844    -0.583465   \n",
              "183  1.073779   -0.269223  -0.586850  -0.332183 -0.392844    -0.583465   \n",
              "184  0.961288   -0.276402  -0.588628  -0.148009  0.090268    -0.345768   \n",
              "185  0.398832   -0.276402  -0.321157  -0.332867 -0.062763    -0.181765   \n",
              "186  0.398832   -0.253871  -0.413258  -0.245055 -0.082478    -0.479785   \n",
              "187  1.073779   -0.235899  -0.489946  -0.303527 -0.186730    -0.494270   \n",
              "188 -0.163623   -0.276402  -0.408790  -0.182111 -0.250364    -0.418621   \n",
              "189 -0.726079   -0.276402  -0.436487   0.234882  0.358233    -0.264328   \n",
              "190  0.286341   -0.164168  -0.451016  -0.307794 -0.082266    -0.130523   \n",
              "191  0.173850   -0.234930  -0.312821  -0.332867 -0.136769    -0.357308   \n",
              "192 -0.501097    2.058272  -0.662740  10.989161  0.618244     5.157264   \n",
              "193  0.398832   -0.273920  -0.662740  -0.332867 -0.567163    -0.595753   \n",
              "194  0.286341   -0.265450  -0.354038  -0.332867 -0.470823    -0.553206   \n",
              "195  0.623815   -0.250569  -0.508628  -0.326285 -0.222780    -0.470407   \n",
              "196 -0.276115   -0.020888  -0.439644   0.137802  0.498933     0.122827   \n",
              "197  0.623815   -0.276402  -0.432724   0.026048 -0.296428    -0.261534   \n",
              "198 -0.613588   -0.250153  -0.662740  -0.332867 -0.215289     0.338598   \n",
              "199  0.511323   -0.064817  -0.594783   0.706498 -0.229812    -0.242646   \n",
              "200  1.186270   -0.276402  -0.116930  -0.062827  0.022937    -0.309586   \n",
              "201 -0.163623   -0.276402  -0.658085  -0.257732 -0.138405    -0.394639   \n",
              "202  1.411252   -0.276402  -0.658085  -0.257732 -0.138405    -0.394639   \n",
              "203 -1.401026   -0.276402  -0.662740   2.697792  1.921241     2.032747   \n",
              "204  0.623815    0.770496  -0.400564   2.193357  1.907546     0.807715   \n",
              "205 -0.838570    3.928141  -0.662740   3.350724  3.239462     5.446834   \n",
              "206 -0.051132   -0.154178  -0.661018  -0.321718 -0.215168    -0.397501   \n",
              "207 -0.726079    4.925319  -0.662740   0.295678 -4.358549     3.301342   \n",
              "208 -0.051132   -0.276402  -0.331427  -0.297069 -0.060811    -0.400540   \n",
              "209  0.061359   -0.276402  -0.293378  -0.332433 -0.181647    -0.464470   \n",
              "210 -2.413446   -0.276402  -0.596399   0.631641  1.576874     4.553706   \n",
              "211 -2.525937   -0.189491  -0.662182  -0.286359 -0.276053    -0.446690   \n",
              "212  1.073779   -0.276402  -0.412223  -0.332867 -0.215800    -0.470560   \n",
              "213  1.073779   -0.276402  -0.586067  -0.327613 -0.192337    -0.587938   \n",
              "214  0.848797   -0.257616  -0.390439  -0.332867 -0.214358    -0.473537   \n",
              "215  0.061359   -0.245282  -0.421009  -0.263036 -0.180164    -0.056392   \n",
              "216 -0.276115   -0.128763  -0.485131  -0.332867 -0.137376    -0.475004   \n",
              "217 -0.613588   -0.276402  -0.576899   1.113488  0.713283     0.493486   \n",
              "218 -1.401026    1.052459  -0.662740   4.182753  3.248100     2.907835   \n",
              "219 -1.850991   -0.218487   0.510269  -0.332867 -1.120981    -0.419754   \n",
              "220  0.736306   -0.276402  -0.176314  -0.332867 -0.559975    -0.542773   \n",
              "221  0.736306   -0.276402  -0.299028  -0.318586 -0.162979    -0.320065   \n",
              "222 -0.051132   -0.268453  -0.624333  -0.321363 -0.181425    -0.473486   \n",
              "223  1.298761   -0.276402   1.750047   3.342037  4.957269     2.576934   \n",
              "224  1.073779   -0.276402   1.750047   3.342037  4.957269     2.576934   \n",
              "225  1.186270   -0.276402  -0.419623  -0.309213 -0.043948    -0.408216   \n",
              "226  0.173850   -0.276402  -0.662740  -0.274827 -0.231397    -0.543060   \n",
              "227  0.286341   -0.276402  -0.662740  -0.332867  5.196472    -0.466662   \n",
              "228  0.961288   -0.248149  -0.417138  -0.332867 -0.109348    -0.489400   \n",
              "229 -1.063553   -0.244845  -0.270963  -0.332861 -0.586632    -0.424019   \n",
              "230  0.511323   -0.265386  -0.270803  -0.112863  0.015877    -0.408655   \n",
              "231  0.511323   -0.261832  -0.553698  -0.332867 -0.222512    -0.480145   \n",
              "232  0.511323   -0.223136  -0.407947  -0.332867 -0.219749    -0.570495   \n",
              "233 -1.850991   -0.276402  -0.570503  -0.244432 -0.142689    -0.047746   \n",
              "234  0.398832   -0.072101   0.187164  -0.332867 -1.317488    -0.391010   \n",
              "235 -0.163623   -0.159858   0.208986  -0.332867 -0.673799    -0.302996   \n",
              "236 -0.276115    2.986397  -0.103229   2.798688  6.329799     3.960875   \n",
              "237  0.848797   -0.246525  -0.366468  -0.332867  0.065045    -0.530431   \n",
              "238  0.623815   -0.210959   0.029199   1.468136  0.186066    -0.083141   \n",
              "239  0.736306   -0.276402  -0.480441  -0.129625 -0.018631    -0.023323   \n",
              "240  0.061359   -0.225206  -0.023005  -0.332867  0.287556    -0.174512   \n",
              "241 -0.388606   -0.176455  -0.411866  -0.332867 -0.274337    -0.503588   \n",
              "242  0.275351   -0.192384   0.243797   0.519144  0.519931     0.847836   \n",
              "243 -0.492892   -0.224112   0.932381  -0.277933  1.192187    -0.200995   \n",
              "244  0.574913   -0.273772  -0.356982  -0.302943 -0.239795    -0.327756   \n",
              "245  0.898485   -0.154753  -0.140580  -0.248879  0.208266    -0.420582   \n",
              "246  0.375277   -0.048023   0.127303  -0.109373  1.004651    -0.235214   \n",
              "247 -0.146566   -0.068615   0.114486  -0.092713  1.025361    -0.177553   \n",
              "248 -0.125340   -0.276402   0.369180  -0.307455  0.434394    -0.197188   \n",
              "249 -0.155691   -0.276402   0.412913  -0.309035  0.477571    -0.198467   \n",
              "250  0.705595   -0.165492  -0.268685  -0.220715  0.289147    -0.416371   \n",
              "251  0.314931   -0.055427   0.068905  -0.102440  1.012776    -0.239075   \n",
              "252  0.381741   -0.264908  -0.459079  -0.295728 -0.232285    -0.466326   \n",
              "253  0.281592   -0.270230  -0.213206  -0.280533 -0.116780    -0.178661   \n",
              "254 -0.175555   -0.267317  -0.250342  -0.264096 -0.074324    -0.240410   \n",
              "255  0.978712   -0.242439  -0.504698  -0.303401 -0.216749    -0.540320   \n",
              "256  0.119395   -0.214130   0.176457   0.615603  0.537234     0.982476   \n",
              "257 -0.298542   -0.264898  -0.257352  -0.258256 -0.053838    -0.256538   \n",
              "258  0.787043   -0.132020  -0.083522  -0.219165  0.377894    -0.381099   \n",
              "259  0.547043   -0.274026  -0.344140  -0.301309 -0.231245    -0.313470   \n",
              "260 -0.613588   -0.261105  -0.092207  -0.255464  0.152176    -0.286972   \n",
              "261  0.113146   -0.051296   0.170164  -0.085601  1.106409    -0.136850   \n",
              "262 -0.275570   -0.077751   0.524976   0.657819  1.367196     1.028712   \n",
              "263 -0.613588   -0.221515   0.168949   0.435853  0.534880     0.764861   \n",
              "264  0.078056   -0.252197   0.004355   0.091124  0.173412     0.367526   \n",
              "265  1.147030   -0.216204  -0.285139  -0.323887 -0.226354    -0.507598   \n",
              "266  0.057681    0.014421   0.292305  -0.027345  1.480545    -0.124506   \n",
              "267  0.179381   -0.263646  -0.421717  -0.285069 -0.185335    -0.432074   \n",
              "268  0.034757   -0.271454  -0.238354  -0.274082 -0.109356    -0.212831   \n",
              "269  1.055211   -0.186724  -0.220824  -0.290668 -0.030290    -0.476108   \n",
              "270  0.118118   -0.057757   0.158850  -0.091354  1.070310    -0.138084   \n",
              "271  1.079058   -0.266607  -0.572822  -0.331875 -0.385191    -0.580606   \n",
              "272 -0.390349   -0.167839  -0.088986  -0.171317  0.489418    -0.240348   \n",
              "273 -0.311728   -0.087813   0.550404   0.731169  1.354825     1.152417   \n",
              "274  0.352079   -0.272294   0.122951  -0.332476  0.258049    -0.427051   \n",
              "275  0.065784    0.010263   0.277958  -0.030792  1.450338    -0.125094   \n",
              "276 -0.562733   -0.157666   0.726925   1.240355  1.268948     2.011160   \n",
              "277 -0.338482   -0.264112  -0.259629  -0.256360 -0.047185    -0.261775   \n",
              "278  0.842657   -0.205510  -0.087352  -0.006958  0.053024    -0.005614   \n",
              "279 -0.371980   -0.199896   0.497784   0.906026  0.874929     1.550206   \n",
              "280 -0.150376   -0.267812  -0.248907  -0.265292 -0.078518    -0.237108   \n",
              "281  0.579809   -0.273727  -0.359238  -0.303230 -0.241297    -0.330266   \n",
              "282  0.407790   -0.230778  -0.054199   0.329202  0.256184     0.509282   \n",
              "283  0.810909   -0.270342  -0.328315  -0.332290 -0.155764    -0.526494   \n",
              "284 -0.487628   -0.255534  -0.276193  -0.249058 -0.017345    -0.313575   \n",
              "285 -0.566061   -0.239356  -0.235642  -0.227973  0.103126    -0.285609   \n",
              "286 -0.425576   -0.262399  -0.264593  -0.252225 -0.032677    -0.273196   \n",
              "287  0.890323   -0.234159  -0.266908  -0.312596 -0.203957    -0.409842   \n",
              "288 -0.371521   -0.160175  -0.073271  -0.165246  0.530811    -0.235498   \n",
              "289 -0.156129   -0.182402   0.495661   0.919285  0.875050     1.496949   \n",
              "290  1.153053   -0.229926  -0.376195  -0.327553 -0.277929    -0.540532   \n",
              "291 -0.395883   -0.111233   0.609587   0.901885  1.326033     1.440330   \n",
              "292  0.214569   -0.027151   0.155633  -0.072055  1.198679    -0.193527   \n",
              "293  0.163392   -0.262114  -0.089209  -0.063397  0.040391     0.143268   \n",
              "294  0.582605   -0.271313  -0.103774  -0.332382  0.050141    -0.477013   \n",
              "295  1.135966   -0.238396  -0.421599  -0.328551 -0.302697    -0.549785   \n",
              "296 -0.413459   -0.189698   0.840036  -0.241780  1.233789    -0.189959   \n",
              "297  0.616892   -0.273389  -0.376326  -0.305403 -0.252674    -0.349274   \n",
              "298 -0.181273   -0.274563   0.647510  -0.332692  0.739074    -0.311458   \n",
              "299 -0.351380   -0.151978  -0.056461  -0.158752  0.575088    -0.230310   \n",
              "300  0.153145   -0.002710   0.241034  -0.050144  1.342761    -0.156514   \n",
              "301  1.181604   -0.213786  -0.287594  -0.325408 -0.229370    -0.520765   \n",
              "302  1.115638   -0.248473  -0.475619  -0.329739 -0.332166    -0.560795   \n",
              "303  0.258046   -0.039400   0.118062  -0.085218  1.118146    -0.213258   \n",
              "304 -0.565707   -0.158493   0.729016   1.246387  1.267931     2.021332   \n",
              "305 -0.206449   -0.058515   0.476366   0.517601  1.390845     0.792234   \n",
              "306 -0.522212   -0.173932   0.709351   1.258779  1.176344     2.047680   \n",
              "307 -0.015169   -0.017141   0.376997  -0.060501  1.442390    -0.134627   \n",
              "308 -0.563882   -0.257451  -0.275659  -0.245571 -0.007668    -0.304056   \n",
              "309 -0.329344   -0.092715   0.562793   0.766905  1.348798     1.212685   \n",
              "310 -0.108255   -0.268641  -0.246506  -0.267292 -0.085534    -0.231585   \n",
              "311  0.135147   -0.079891   0.120092  -0.111061  0.946649    -0.142311   \n",
              "312  0.393587   -0.077587   0.000935  -0.126254  0.867081    -0.274772   \n",
              "313 -0.334535   -0.251686  -0.277266  -0.256060 -0.036774    -0.332687   \n",
              "314  1.059480   -0.269354  -0.580262  -0.331345 -0.388457    -0.576136   \n",
              "315  0.910712   -0.223281  -0.445937  -0.282814 -0.090793    -0.509460   \n",
              "316  0.160715   -0.276402  -0.042999  -0.292567  0.027452    -0.185134   \n",
              "317  0.735853   -0.224781  -0.284769  -0.305014 -0.172615    -0.466312   \n",
              "318  0.316921   -0.276124  -0.238104  -0.287821 -0.160645    -0.195514   \n",
              "319  0.562059   -0.239684  -0.177582   0.175999  0.105844     0.256159   \n",
              "320  0.167524   -0.216909   0.137964   0.567807  0.490331     0.903507   \n",
              "321  0.816561   -0.271569  -0.468329  -0.317107 -0.313931    -0.451620   \n",
              "322 -0.613588   -0.186773   0.584017   1.070378  1.035885     1.757744   \n",
              "323 -0.369219   -0.200217   0.494757   0.901026  0.870625     1.542950   \n",
              "324 -0.582317   -0.259316  -0.273528  -0.244782 -0.006569    -0.293750   \n",
              "325  0.183465   -0.142691   0.010125  -0.166976  0.595787    -0.154304   \n",
              "326  0.949382   -0.270358  -0.529531  -0.324892 -0.354679    -0.519702   \n",
              "327 -0.613588   -0.272962   0.810672  -0.315456  0.909260    -0.233330   \n",
              "328  0.657316   -0.151890  -0.226965  -0.206098  0.378574    -0.394460   \n",
              "329  0.522957   -0.274245  -0.333042  -0.299897 -0.223855    -0.301124   \n",
              "330 -0.032022   -0.021994   0.210089  -0.055780  1.277178    -0.148049   \n",
              "331  0.195753   -0.021850   0.171892  -0.066359  1.233531    -0.184988   \n",
              "332 -0.562186   -0.174785   0.721578   1.292471  1.201456     2.100804   \n",
              "333  0.222130   -0.192945  -0.077874  -0.211721  0.315013    -0.163901   \n",
              "334 -0.613588   -0.175386   0.720061   1.278351  1.200095     2.083174   \n",
              "335  0.400822   -0.230376  -0.048626   0.336122  0.262975     0.520714   \n",
              "336  0.166688   -0.239087  -0.280779  -0.278983 -0.100383    -0.395259   \n",
              "337 -0.181694   -0.196750   0.417265   0.914610  0.830655     1.476499   \n",
              "338 -0.190294   -0.274602   0.656383  -0.332695  0.747210    -0.309503   \n",
              "339 -0.302549   -0.260640  -0.332737  -0.259682 -0.073523    -0.350499   \n",
              "340 -0.468028   -0.255042  -0.276330  -0.249954 -0.019832    -0.316022   \n",
              "341 -0.613588   -0.221487   0.169280   0.436359  0.535279     0.765652   \n",
              "342 -0.148887   -0.042496   0.435885   0.400832  1.410539     0.595302   \n",
              "343 -0.515394   -0.275985   0.976123  -0.332827  1.040414    -0.239044   \n",
              "344 -0.613588   -0.251334  -0.187302  -0.108757  0.104870    -0.087327   \n",
              "345  0.186820   -0.147051   0.002489  -0.170859  0.571423    -0.155137   \n",
              "346 -0.451744   -0.206285   0.884544  -0.259205  1.213738    -0.195278   \n",
              "347 -0.550229   -0.248952   0.999039  -0.304029  1.162158    -0.208960   \n",
              "348 -0.490163   -0.137470   0.675890   1.093140  1.293777     1.762882   \n",
              "349  1.068585   -0.217575  -0.198960  -0.326088 -0.140934    -0.502613   \n",
              "350 -0.003218   -0.262507  -0.388003  -0.275450 -0.142971    -0.401166   \n",
              "351  0.201923   -0.263786  -0.425879  -0.286256 -0.190565    -0.435889   \n",
              "352 -0.170073   -0.078182   0.094866  -0.100292  0.973682    -0.183608   \n",
              "353  0.121552   -0.214255   0.174731   0.613460  0.535132     0.978936   \n",
              "354 -0.431379   -0.184538  -0.123231  -0.184547  0.399216    -0.250917   \n",
              "355 -0.581382   -0.262450   1.035257  -0.318209  1.145841    -0.213288   \n",
              "356  0.405199   -0.195389   0.168002   0.398727  0.413063     0.652494   \n",
              "357 -0.328047   -0.266417   0.856840  -0.331716  0.913413    -0.266114   \n",
              "358  0.871017   -0.267959  -0.549414  -0.321502 -0.345801    -0.549144   \n",
              "359 -0.021326   -0.270351  -0.241551  -0.271419 -0.100014    -0.220185   \n",
              "360  0.082012   -0.010830   0.241024  -0.049571  1.332496    -0.129122   \n",
              "361 -0.528853   -0.181665   0.669784   1.190085  1.119464     1.962465   \n",
              "362 -0.333338   -0.264213  -0.259336  -0.256604 -0.048042    -0.261101   \n",
              "363  1.167831   -0.214749  -0.286616  -0.324802 -0.228168    -0.515520   \n",
              "364 -0.455803   -0.190155   0.589690   1.057809  1.005594     1.770491   \n",
              "365 -0.598144   -0.173613   0.745756   1.315553  1.227475     2.144559   \n",
              "366 -0.187678   -0.181672   0.514076   0.948542  0.901015     1.544410   \n",
              "367  0.747970   -0.250416  -0.326272  -0.008627 -0.075333    -0.048883   \n",
              "368 -0.301939   -0.276402   0.623641  -0.316646  0.685622    -0.204630   \n",
              "369 -0.195415   -0.220416   0.304194   0.586311  0.599699     1.086198   \n",
              "370 -0.377204   -0.106034   0.596450   0.863992  1.332424     1.376423   \n",
              "371  1.035936   -0.269568  -0.569413  -0.329965 -0.381234    -0.564067   \n",
              "372  1.094740   -0.215760  -0.287284  -0.321427 -0.218161    -0.511115   \n",
              "373 -0.613588   -0.248855  -0.157684  -0.063480  0.140619    -0.016479   \n",
              "374 -0.144016   -0.259981   0.717720  -0.330975  0.774484    -0.297277   \n",
              "375 -0.587764   -0.258862  -0.280078  -0.244658 -0.007351    -0.302222   \n",
              "376  1.126368   -0.217649  -0.283672  -0.322979 -0.224551    -0.499730   \n",
              "377  0.680879   -0.272806  -0.405809  -0.309154 -0.272305    -0.382072   \n",
              "378 -0.613588   -0.218680   0.202816   0.487626  0.575758     0.845873   \n",
              "379  0.132461   -0.076400   0.126205  -0.107953  0.966152    -0.141644   \n",
              "380  0.588834   -0.273645  -0.363397  -0.303759 -0.244066    -0.334892   \n",
              "381  1.016974   -0.253219  -0.537762  -0.314985 -0.287623    -0.557685   \n",
              "382  0.023732   -0.000288   0.331773  -0.042796  1.462764    -0.129223   \n",
              "383 -0.389504   -0.260098  -0.316683  -0.255101 -0.053349    -0.335781   \n",
              "384 -0.000067   -0.262527  -0.388585  -0.275616 -0.143702    -0.401699   \n",
              "385  0.872256   -0.212447  -0.412705  -0.271172 -0.019561    -0.492007   \n",
              "386 -0.613588   -0.264415   0.159870  -0.272213  0.363548    -0.271996   \n",
              "387 -0.097598   -0.268850  -0.245898  -0.267798 -0.087309    -0.230187   \n",
              "388 -0.396761   -0.260053  -0.315343  -0.254719 -0.051666    -0.334552   \n",
              "389 -0.273782   -0.250159  -0.277692  -0.258838 -0.044484    -0.340272   \n",
              "390  0.124929   -0.273227  -0.233214  -0.278364 -0.124376    -0.201006   \n",
              "391 -0.050287   -0.269781  -0.243202  -0.270044 -0.095190    -0.223983   \n",
              "392  0.124738   -0.257622  -0.046828   0.006594  0.100644     0.244848   \n",
              "393 -0.378359   -0.162958  -0.078978  -0.167451  0.515778    -0.237260   \n",
              "394  0.725027   -0.267049  -0.522460  -0.313812 -0.311930    -0.524433   \n",
              "395  0.858052   -0.271190  -0.487447  -0.319538 -0.326660    -0.472888   \n",
              "396 -0.227358   -0.248992  -0.278017  -0.260961 -0.050376    -0.346067   \n",
              "397  0.761887   -0.126888  -0.070642  -0.212457  0.416184    -0.372186   \n",
              "398 -0.578524   -0.259390  -0.273311  -0.244962 -0.007200    -0.293253   \n",
              "399 -0.196300   -0.055690   0.469228   0.497012  1.394317     0.757510   \n",
              "400  0.505889   -0.265682  -0.482000  -0.302268 -0.261088    -0.487341   \n",
              "401  0.876445   -0.235129  -0.265922  -0.311985 -0.202747    -0.404557   \n",
              "402  0.423473   -0.265168  -0.466784  -0.297927 -0.241967    -0.473390   \n",
              "403 -0.153535   -0.071451   0.108669  -0.094960  1.010040    -0.179349   \n",
              "404 -0.307050   -0.276402   0.631006  -0.316913  0.692894    -0.204845   \n",
              "405  0.717057   -0.225253  -0.284637  -0.304154 -0.170230    -0.463966   \n",
              "406  0.563557   -0.125474  -0.145944  -0.177712  0.552245    -0.351910   \n",
              "407 -0.613588   -0.198500   0.443910   0.856192  0.866769     1.422593   \n",
              "408  0.721116   -0.169865  -0.282098  -0.225414  0.260397    -0.423415   \n",
              "409  1.129258   -0.217447  -0.283877  -0.323106 -0.224803    -0.500831   \n",
              "410  0.070175    0.004555   0.267964  -0.035873  1.418451    -0.126184   \n",
              "411 -0.237466   -0.276402   0.530743  -0.313291  0.593904    -0.201913   \n",
              "412  0.315516   -0.274362  -0.226086  -0.287312 -0.153809    -0.190950   \n",
              "413  0.642285   -0.273157  -0.388026  -0.306892 -0.260464    -0.362290   \n",
              "414  0.191068   -0.152573  -0.007179  -0.175774  0.540576    -0.156191   \n",
              "415  0.797969   -0.240618  -0.260349  -0.308533 -0.195900    -0.374673   \n",
              "416  0.667979   -0.249710  -0.251117  -0.302815 -0.184559    -0.325171   \n",
              "417  0.703274   -0.202285  -0.005991   0.122302  0.167740     0.204072   \n",
              "418 -0.398129   -0.260044  -0.315091  -0.254647 -0.051348    -0.334321   \n",
              "419  0.736276   -0.203048  -0.025255   0.091697  0.140579     0.154425   \n",
              "420  0.533210   -0.080240   0.046441  -0.151484  0.764258    -0.291168   \n",
              "421 -0.028330   -0.008946   0.351103   0.156272  1.451785     0.182851   \n",
              "422  0.334487   -0.264613  -0.450354  -0.293239 -0.221321    -0.458328   \n",
              "423  0.917870   -0.158708  -0.150505  -0.254048  0.178760    -0.427450   \n",
              "424 -0.037354   -0.185150   0.426329   0.809137  0.777295     1.318265   \n",
              "425 -0.174719   -0.261437  -0.356339  -0.266416 -0.103181    -0.372137   \n",
              "426  0.071945   -0.272185  -0.236234  -0.275848 -0.115551    -0.207954   \n",
              "427  0.776341   -0.271935  -0.449796  -0.314749 -0.301592    -0.431004   \n",
              "428  1.126163   -0.243256  -0.447649  -0.329124 -0.316908    -0.555095   \n",
              "429  0.280152   -0.236235  -0.281575  -0.284172 -0.114783    -0.409424   \n",
              "430 -0.262959   -0.276402   0.567476  -0.314618  0.630170    -0.202987   \n",
              "431 -0.345737   -0.202946   0.469012   0.858508  0.834023     1.481242   \n",
              "432 -0.006126   -0.002766   0.335487   0.111227  1.459382     0.106884   \n",
              "433  0.025259    0.001321   0.257898  -0.037311  1.403108    -0.133294   \n",
              "434  1.181606   -0.215771  -0.300319  -0.325886 -0.236538    -0.525067   \n",
              "435  0.815853   -0.254334  -0.380564  -0.076041 -0.141487    -0.160264   \n",
              "436  0.474924   -0.234654  -0.107892   0.262532  0.190760     0.399129   \n",
              "437 -0.574712   -0.259465  -0.273094  -0.245143 -0.007835    -0.292753   \n",
              "438  0.577225   -0.256057  -0.244672  -0.298823 -0.176641    -0.290611   \n",
              "439 -0.287912   -0.081185   0.533655   0.682856  1.362974     1.070936   \n",
              "440 -0.169900   -0.267428  -0.250020  -0.264365 -0.075266    -0.239668   \n",
              "441 -0.463688   -0.254933  -0.276361  -0.250153 -0.020383    -0.316564   \n",
              "442 -0.613588   -0.274779   0.949080  -0.324653  1.025318    -0.225107   \n",
              "443 -0.613588   -0.206359   0.350023   0.712665  0.753444     1.198007   \n",
              "444  0.139479   -0.263397  -0.414350  -0.282967 -0.176078    -0.425320   \n",
              "445  0.238481   -0.214197  -0.115088  -0.230643  0.196278    -0.167960   \n",
              "446 -0.522798   -0.177059   0.690077   1.253356  1.163072     2.036179   \n",
              "447  0.307381   -0.034173   0.162065  -0.091269  1.107996    -0.211159   \n",
              "448  1.084160   -0.220601  -0.280674  -0.321122 -0.220869    -0.483657   \n",
              "449  0.957231   -0.268497  -0.565332  -0.326044 -0.365803    -0.563738   \n",
              "450 -0.567238   -0.172890   0.735634   1.300535  1.213402     2.115417   \n",
              "451  1.069660   -0.268063  -0.583291  -0.330936 -0.385213    -0.581596   \n",
              "452  1.029784   -0.269625  -0.566578  -0.329604 -0.379346    -0.560914   \n",
              "453  0.807549   -0.194216  -0.356788  -0.251581  0.100297    -0.462641   \n",
              "454 -0.122303   -0.276402   0.364803  -0.307297  0.430072    -0.197060   \n",
              "455  0.842140   -0.271335  -0.480115  -0.318606 -0.321778    -0.464731   \n",
              "456 -0.557166   -0.173123   0.729754   1.291194  1.205112     2.100264   \n",
              "457 -0.087180   -0.202206   0.341673   0.820750  0.738548     1.321422   \n",
              "458 -0.503547   -0.141195   0.685302   1.120291  1.289198     1.808672   \n",
              "459 -0.388947   -0.184786   0.583023   1.120430  1.032629     1.816556   \n",
              "460 -0.504571   -0.141480   0.686022   1.122368  1.288847     1.812174   \n",
              "461  0.153316   -0.189561   0.315031   0.632315  0.620369     1.031423   \n",
              "462  1.161232   -0.225871  -0.354459  -0.327076 -0.266072    -0.536102   \n",
              "463  1.141650   -0.235578  -0.406495  -0.328219 -0.294458    -0.546707   \n",
              "464  0.189056   -0.274489  -0.229559  -0.281409 -0.135058    -0.192597   \n",
              "465  0.826403   -0.254943  -0.389001  -0.086517 -0.151768    -0.177574   \n",
              "466  0.724313   -0.245770  -0.255118  -0.305293 -0.189474    -0.346624   \n",
              "467  0.890126   -0.270898  -0.502227  -0.321418 -0.336500    -0.489328   \n",
              "468 -0.221238   -0.099007   0.052162  -0.116790  0.861199    -0.196788   \n",
              "469  0.394111   -0.275420  -0.273672  -0.292345 -0.184326    -0.235080   \n",
              "\n",
              "     inventoryAsset1  OnonCAsset1  liquidLiabilities1  NCLiabilities1  \\\n",
              "0          -0.432738    -0.253380           -0.671966       -0.583419   \n",
              "1           4.976478     3.487949            3.452640        1.075875   \n",
              "2           1.122079    -0.175744            3.588785        1.866315   \n",
              "3           0.133545     0.067749            2.318304       -0.190194   \n",
              "4           2.693899    -0.358995            3.092185        1.906199   \n",
              "5           1.745185    -0.394550            1.769989        1.017079   \n",
              "6           0.465331     0.041722            2.923905        2.760236   \n",
              "7          -0.269543    -0.182693            1.133007       -0.299875   \n",
              "8           4.904688     1.541228            4.486770        2.105641   \n",
              "9           0.028943     9.530314            5.141929        3.579425   \n",
              "10         -0.401957     1.245904            1.587122       -0.247918   \n",
              "11          1.263253     1.989936            2.337322        3.140412   \n",
              "12          0.949135     1.629977            1.313517       -0.419088   \n",
              "13         -0.244588     2.457257            0.982147        0.022897   \n",
              "14          3.560993     1.685868            4.922489        5.188743   \n",
              "15          0.029380    -0.394550            0.375746       -0.563695   \n",
              "16         -0.406969    -0.394095            0.883783       -0.471860   \n",
              "17          0.013262    -0.072765            1.008014       -0.247680   \n",
              "18         -0.188600    -0.362490            0.505165       -0.326209   \n",
              "19          0.390213    -0.326750            1.097777        0.519472   \n",
              "20          0.387293    -0.264049            0.597032        0.301352   \n",
              "21          2.019599     0.385846            1.676109        1.379570   \n",
              "22         -0.045281    -0.394550            2.446462        0.553623   \n",
              "23         -0.024241    -0.334184            0.322413       -0.637951   \n",
              "24         -0.176884    -0.053183            0.373347       -0.495858   \n",
              "25         -0.160225    -0.296827            0.325854       -0.001624   \n",
              "26          0.737657    -0.134881            0.119062        0.100493   \n",
              "27          0.096048    -0.392522            0.193353       -0.518754   \n",
              "28         -0.487229    -0.050113            0.144017       -0.488055   \n",
              "29          3.200101     5.006392            3.728543        0.517201   \n",
              "30          0.862865     0.468620            0.678402        1.159005   \n",
              "31         -0.290516     2.900726            1.389342        0.745168   \n",
              "32         -0.411799    -0.374868            0.443549       -0.166627   \n",
              "33         -0.366741    -0.039094           -0.023583       -0.510876   \n",
              "34         -0.083230    -0.286335            0.454482       -0.417308   \n",
              "35          0.750157    -0.394550            0.393561       -0.049036   \n",
              "36         -0.396673    -0.213524            0.049902       -0.640937   \n",
              "37          2.537180     1.200157            1.358069        1.493049   \n",
              "38         -0.350022    -0.314347            0.067410       -0.490423   \n",
              "39          0.275737    -0.232779            0.571446        1.120748   \n",
              "40         -0.477656    -0.359584            0.101884       -0.640937   \n",
              "41          0.525423    -0.382101            0.311311       -0.515168   \n",
              "42         -0.429329    -0.232172           -0.137351       -0.043657   \n",
              "43         -0.378530     0.071974           -0.001607       -0.563862   \n",
              "44         -0.168950     1.204044            0.063477       -0.132171   \n",
              "45         -0.363085    -0.368579            0.077174        0.171364   \n",
              "46          0.602539    -0.394550            0.242105        0.441084   \n",
              "47         -0.450242    -0.042860           -0.211022       -0.600263   \n",
              "48         -0.451526    -0.345344           -0.136213       -0.562254   \n",
              "49         -0.376124     0.660505            0.022068       -0.515281   \n",
              "50         -0.424058    -0.385488           -0.224573       -0.183224   \n",
              "51          0.636417    -0.240114            0.671337        0.317958   \n",
              "52         -0.477874    -0.284784           -0.116197       -0.594862   \n",
              "53         -0.366055    -0.394550            0.053687        0.630104   \n",
              "54         -0.461247     0.124250           -0.287193       -0.639736   \n",
              "55         -0.461247     0.124250           -0.287193       -0.639736   \n",
              "56         -0.419669    -0.394550           -0.578031       -0.638491   \n",
              "57          0.178184     0.062780            0.181810        0.177525   \n",
              "58         -0.257114    -0.375614           -0.257503       -0.640937   \n",
              "59         -0.239459    -0.394550           -0.122295        0.750823   \n",
              "60          0.197469    -0.394550            0.826412       -0.465357   \n",
              "61         -0.402160    -0.348297           -0.265237       -0.570458   \n",
              "62         -0.452040    -0.229535           -0.321368       -0.452420   \n",
              "63         -0.428197    -0.359781           -0.357763       -0.614259   \n",
              "64         -0.219537    -0.294712           -0.246645        0.158664   \n",
              "65         -0.160718    -0.293527           -0.267016       -0.402025   \n",
              "66         -0.475693    -0.374712           -0.367590       -0.640937   \n",
              "67         -0.355780    -0.343376           -0.357388       -0.637694   \n",
              "68         -0.427095    -0.349070           -0.379088       -0.614983   \n",
              "69         -0.167470    -0.376639           -0.073555       -0.378134   \n",
              "70          0.126503     2.659140           -0.179904       -0.300488   \n",
              "71         -0.405082    -0.383847           -0.341872       -0.054208   \n",
              "72         -0.422251    -0.389334           -0.139886       -0.437862   \n",
              "73         -0.487229     0.017470           -0.356312       -0.640937   \n",
              "74         -0.064316    -0.394550           -0.425097       -0.331546   \n",
              "75         -0.117247    -0.394550           -0.287399       -0.640937   \n",
              "76          0.225691    -0.345334           -0.286343       -0.079945   \n",
              "77         -0.433202    -0.197901           -0.375270       -0.418650   \n",
              "78          0.145051    -0.392581           -0.076736        0.427125   \n",
              "79         -0.327212     0.905823           -0.408552       -0.505143   \n",
              "80         -0.381203    -0.091425            1.027168        1.502577   \n",
              "81         -0.472134    -0.313307           -0.325902       -0.573851   \n",
              "82         -0.426346    -0.377820           -0.218700       -0.542458   \n",
              "83         -0.393448    -0.357935           -0.072557       -0.110854   \n",
              "84         -0.426428    -0.382740           -0.418125       -0.413971   \n",
              "85          0.495570    -0.253778            0.243071        0.538462   \n",
              "86         -0.301945    -0.375781           -0.278086       -0.506355   \n",
              "87         -0.105673    -0.193969            1.007615       -0.321594   \n",
              "88         -0.400405    -0.394550           -0.402759       -0.239565   \n",
              "89         -0.487229    -0.394550           -0.386400       -0.047893   \n",
              "90          0.869758     0.938322            0.316612       -0.083377   \n",
              "91          0.319701    -0.394550           -0.152272       -0.147641   \n",
              "92         -0.487229    -0.383801           -0.288896       -0.640937   \n",
              "93         -0.454079    -0.383882           -0.472498       -0.452197   \n",
              "94         -0.487229    -0.362074            0.036935        0.647283   \n",
              "95          0.009673     0.210675           -0.453360       -0.636058   \n",
              "96         -0.487229    -0.394254           -0.225944       -0.640937   \n",
              "97         -0.223628     2.350483           -0.214970       -0.150726   \n",
              "98         -0.219704    -0.107771           -0.504826       -0.509536   \n",
              "99         -0.416055    -0.394363           -0.467436       -0.640937   \n",
              "100        -0.416055    -0.394363           -0.467436       -0.640937   \n",
              "101        -0.433062    -0.387169           -0.516850       -0.293442   \n",
              "102        -0.399903    -0.394550            0.115341       -0.278631   \n",
              "103        -0.487229    -0.317789           -0.511221       -0.616812   \n",
              "104        -0.417376    -0.139155           -0.458774       -0.406402   \n",
              "105        -0.261852    -0.043878           -0.264309        0.113683   \n",
              "106        -0.483581    -0.275158           -0.551556       -0.640937   \n",
              "107        -0.424166    -0.341506           -0.483145       -0.404074   \n",
              "108        -0.128470    -0.221294           -0.456840        0.138813   \n",
              "109        -0.376303    -0.035302           -0.523422       -0.503326   \n",
              "110        -0.449790     0.088154           -0.545688       -0.138878   \n",
              "111         0.068306    -0.114085           -0.189836       -0.075620   \n",
              "112        -0.408467    -0.394550           -0.265293       -0.011795   \n",
              "113        -0.358575     0.281457           -0.512892       -0.509536   \n",
              "114         0.029816    -0.394550            0.228118        0.447381   \n",
              "115         0.357047    -0.393110            1.207769       -0.235982   \n",
              "116        -0.465924    -0.393374           -0.556474       -0.345400   \n",
              "117        -0.476738    -0.357017           -0.539625       -0.521481   \n",
              "118        -0.472083    -0.392493           -0.575535       -0.615902   \n",
              "119        -0.442360    -0.389117           -0.408263       -0.313416   \n",
              "120        -0.451159    -0.360106           -0.296089       -0.172526   \n",
              "121        -0.271533    -0.333535           -0.611337        0.209900   \n",
              "122        -0.439161    -0.374868           -0.604427       -0.507025   \n",
              "123        -0.276518    -0.355144           -0.535436       -0.127316   \n",
              "124        -0.005722    -0.278819           -0.561207       -0.343966   \n",
              "125        -0.211178    -0.370876           -0.571637       -0.322017   \n",
              "126        -0.487229    -0.371861           -0.562039       -0.252268   \n",
              "127        -0.387314    -0.355970           -0.614554       -0.433943   \n",
              "128         0.116361    -0.363644           -0.520466       -0.020485   \n",
              "129        -0.396709     0.127990           -0.581004        0.124899   \n",
              "130        -0.458633    -0.394550           -0.414703       -0.110553   \n",
              "131        -0.458390    -0.296363           -0.465628        0.340138   \n",
              "132        -0.487229     0.189408           -0.268626        1.682420   \n",
              "133        -0.166061    -0.384765           -0.417699       -0.038568   \n",
              "134        -0.298874    -0.375852           -0.450645       -0.231324   \n",
              "135        -0.487229    -0.365026           -0.670031       -0.366189   \n",
              "136        -0.371364    -0.387210           -0.569712       -0.088266   \n",
              "137        -0.141581    -0.282386           -0.570752       -0.513143   \n",
              "138        -0.487229    -0.392581           -0.642877       -0.598426   \n",
              "139        -0.438795    -0.377495           -0.613756       -0.178554   \n",
              "140        -0.448797    -0.382248           -0.486930       -0.640937   \n",
              "141        -0.250596    -0.368188           -0.592919       -0.637212   \n",
              "142        -0.405431    -0.348493           -0.597802       -0.521994   \n",
              "143        -0.292999    -0.384709           -0.625715       -0.285122   \n",
              "144        -0.415640    -0.292842           -0.575987       -0.139861   \n",
              "145        -0.419428    -0.380496           -0.682626       -0.443835   \n",
              "146        -0.487229    -0.058720           -0.669663       -0.416336   \n",
              "147        -0.123730    -0.375852           -0.635771       -0.453911   \n",
              "148        -0.452073    -0.366601           -0.600346        0.152297   \n",
              "149        -0.473110    -0.389629           -0.686819       -0.334280   \n",
              "150        -0.430083    -0.374868           -0.686624       -0.541789   \n",
              "151         0.515713     0.972972           -0.085100        0.594548   \n",
              "152        -0.320350    -0.324812           -0.458072        0.261198   \n",
              "153        -0.320350    -0.324812           -0.458072        0.261198   \n",
              "154        -0.397689    -0.359975           -0.655631       -0.455374   \n",
              "155        -0.156135    -0.273816           -0.597004       -0.213163   \n",
              "156        -0.468246    -0.357154           -0.528870       -0.353999   \n",
              "157        -0.487229    -0.394550           -0.618657       -0.589751   \n",
              "158         0.495733     0.334054            1.054506        0.345217   \n",
              "159         0.327481     1.675352            0.761428        2.134933   \n",
              "160         0.543430    -0.394550           -0.614418       -0.640937   \n",
              "161         0.245009     0.825361           -0.200441        2.012078   \n",
              "162        -0.023135     1.422845            0.091051        0.242336   \n",
              "163        -0.348914    -0.385890           -0.630918       -0.022512   \n",
              "164        -0.439724    -0.394550           -0.466038       -0.308850   \n",
              "165         0.430757    -0.204479           -0.450130       -0.094857   \n",
              "166        -0.477529    -0.379486           -0.537914       -0.491614   \n",
              "167        -0.457090    -0.262611           -0.685000       -0.623844   \n",
              "168        -0.471547    -0.374206           -0.674836       -0.627797   \n",
              "169        -0.439520    -0.364086           -0.641881       -0.434166   \n",
              "170        -0.234946    -0.366503           -0.658348       -0.611174   \n",
              "171        -0.184796    -0.367941           -0.574298       -0.120269   \n",
              "172         0.570134    -0.393841           -0.462748       -0.392345   \n",
              "173        -0.487229    -0.394550           -0.590548       -0.419089   \n",
              "174        -0.487229    -0.327557           -0.668604       -0.035824   \n",
              "175        -0.342885    -0.354256           -0.335367        0.760939   \n",
              "176        -0.410653    -0.346419           -0.672950       -0.061576   \n",
              "177        -0.391529    -0.345344           -0.633839       -0.640032   \n",
              "178        -0.487229    -0.386989           -0.645756       -0.418857   \n",
              "179        -0.469017    -0.051444           -0.635968        0.433456   \n",
              "180        -0.268537    -0.355034           -0.614447       -0.573125   \n",
              "181        -0.093313    -0.087100            1.206764       -0.286436   \n",
              "182        -0.469020    -0.394451           -0.659188       -0.555066   \n",
              "183        -0.469020    -0.394451           -0.659188       -0.555066   \n",
              "184        -0.470629    -0.379581           -0.635662       -0.482751   \n",
              "185        -0.446360    -0.204867           -0.626003       -0.267350   \n",
              "186        -0.464758    -0.388645           -0.677236       -0.367983   \n",
              "187        -0.372275    -0.394550           -0.658514       -0.494007   \n",
              "188        -0.014175    -0.394550           -0.661885       -0.314403   \n",
              "189        -0.457337    -0.378679           -0.695282       -0.349474   \n",
              "190        -0.476327    -0.350835           -0.627999       -0.431889   \n",
              "191        -0.475392    -0.394550           -0.608724       -0.383753   \n",
              "192         5.501729     0.932294            0.280420        0.275625   \n",
              "193        -0.487229    -0.393597           -0.650872       -0.639755   \n",
              "194        -0.487229    -0.392581           -0.635033       -0.452197   \n",
              "195        -0.478159    -0.392335           -0.673716       -0.473699   \n",
              "196         0.737199     2.800829            0.513492       -0.223282   \n",
              "197        -0.487229     0.097503           -0.530794       -0.086744   \n",
              "198        -0.408967    -0.393487           -0.608790       -0.485645   \n",
              "199        -0.129453    -0.161435           -0.149655       -0.419546   \n",
              "200        -0.487229    -0.356170           -0.597927       -0.035295   \n",
              "201        -0.487229    -0.128841           -0.687759       -0.640893   \n",
              "202        -0.487229    -0.128841           -0.687759       -0.640893   \n",
              "203         0.373840     1.661810           -0.513394        0.557116   \n",
              "204         0.283231    -0.233919            0.188829       -0.360669   \n",
              "205         7.010583     3.286945            2.324962        0.348737   \n",
              "206        -0.272109    -0.345344           -0.611423       -0.623097   \n",
              "207         3.741463    -0.394550            2.299245        5.137371   \n",
              "208        -0.473247     0.074075           -0.635625       -0.267382   \n",
              "209        -0.487229    -0.342195           -0.594441       -0.213285   \n",
              "210         2.765551     3.716207            1.440509       -0.083383   \n",
              "211        -0.138750    -0.371386           -0.645640       -0.472348   \n",
              "212        -0.304966    -0.384610           -0.528988       -0.480866   \n",
              "213        -0.487229    -0.365026           -0.697368       -0.583334   \n",
              "214        -0.436673    -0.370734           -0.650583       -0.404415   \n",
              "215        -0.408970    -0.197289           -0.648770       -0.086322   \n",
              "216         0.012481    -0.295450           -0.648553       -0.457059   \n",
              "217        -0.487229    -0.392588           -0.501945       -0.448050   \n",
              "218         1.269136     1.658121           -0.139948        2.803441   \n",
              "219        -0.267619    -0.394550           -0.617623        5.485736   \n",
              "220        -0.429523    -0.369455           -0.343006       -0.344687   \n",
              "221        -0.487229    -0.394550           -0.656221       -0.374227   \n",
              "222        -0.409776    -0.288412           -0.592813       -0.461741   \n",
              "223         0.730973     0.203145            2.103118        2.246944   \n",
              "224         0.730973     0.203145            2.103118        2.246944   \n",
              "225        -0.011969    -0.394550           -0.578941       -0.460492   \n",
              "226        -0.463111    -0.359294           -0.500703       -0.095122   \n",
              "227        -0.109509    -0.320028           -0.623505       -0.218111   \n",
              "228        -0.487229    -0.365731           -0.441410       -0.604626   \n",
              "229        -0.427346    -0.392515           -0.366353       -0.317212   \n",
              "230        -0.475285    -0.377525           -0.691559       -0.192935   \n",
              "231        -0.477915    -0.394550           -0.672381       -0.521158   \n",
              "232        -0.473930    -0.374350           -0.548668       -0.428927   \n",
              "233         0.116655    -0.178593           -0.444182       -0.257463   \n",
              "234         0.152841    -0.338357           -0.531771       -0.006188   \n",
              "235        -0.244908    -0.057491            0.268203       -0.055426   \n",
              "236         2.624268    -0.394371            1.201830        5.931218   \n",
              "237        -0.463760    -0.340703           -0.270630       -0.397613   \n",
              "238        -0.276255    -0.290702            0.326127       -0.501174   \n",
              "239        -0.432161    -0.394550           -0.559698       -0.220341   \n",
              "240        -0.461534    -0.276598           -0.476251        0.218592   \n",
              "241        -0.332624    -0.309001           -0.228747       -0.374440   \n",
              "242        -0.194838    -0.229713            0.388180       -0.259216   \n",
              "243        -0.427767    -0.353795           -0.325056       -0.094966   \n",
              "244        -0.212627    -0.394514           -0.510884       -0.413459   \n",
              "245        -0.304368    -0.362794           -0.452545       -0.105322   \n",
              "246        -0.314357    -0.339055           -0.455999        0.123755   \n",
              "247        -0.363755    -0.186474           -0.405431       -0.016344   \n",
              "248        -0.241281    -0.378793           -0.366081       -0.258801   \n",
              "249        -0.254328    -0.377631           -0.361730       -0.253438   \n",
              "250        -0.414954    -0.369126           -0.586049       -0.258217   \n",
              "251        -0.357586    -0.342254           -0.508444        0.056755   \n",
              "252        -0.465832    -0.181717           -0.506622       -0.589792   \n",
              "253        -0.069720    -0.393078           -0.425793       -0.319035   \n",
              "254        -0.268044    -0.128272           -0.354316       -0.489727   \n",
              "255        -0.455060    -0.387912           -0.640303       -0.478418   \n",
              "256        -0.256518    -0.220612            0.396205       -0.403127   \n",
              "257        -0.322290    -0.057371           -0.335470       -0.531845   \n",
              "258        -0.306496    -0.357738           -0.453281       -0.056529   \n",
              "259        -0.198304    -0.394517           -0.502599       -0.405548   \n",
              "260        -0.459877     0.058459           -0.288401       -0.576274   \n",
              "261        -0.261415    -0.340865           -0.450481        0.124757   \n",
              "262        -0.207015    -0.206148            0.373003       -0.012177   \n",
              "263        -0.303773     0.033793            0.352214       -0.488525   \n",
              "264        -0.071027    -0.323392           -0.047409       -0.321106   \n",
              "265        -0.288646    -0.376667           -0.449531       -0.235694   \n",
              "266        -0.321063    -0.325004           -0.457189        0.258834   \n",
              "267        -0.464900    -0.119511           -0.462010       -0.599946   \n",
              "268        -0.175282    -0.249514           -0.386544       -0.417704   \n",
              "269        -0.301376    -0.369905           -0.451511       -0.173942   \n",
              "270        -0.255757    -0.342406           -0.449753        0.111660   \n",
              "271        -0.461035    -0.393578           -0.649402       -0.539873   \n",
              "272        -0.414645    -0.024277           -0.343711       -0.341752   \n",
              "273        -0.194852    -0.193413            0.462191       -0.041515   \n",
              "274        -0.461381    -0.379761           -0.503888       -0.391451   \n",
              "275        -0.315314    -0.326184           -0.457423        0.249540   \n",
              "276        -0.110419    -0.105011            1.081325       -0.245173   \n",
              "277        -0.339906    -0.034346           -0.329350       -0.545523   \n",
              "278        -0.259630    -0.320726           -0.134228       -0.241845   \n",
              "279        -0.085528    -0.169642            0.768650       -0.298547   \n",
              "280        -0.256938    -0.142787           -0.358175       -0.481105   \n",
              "281        -0.215144    -0.394513           -0.512339       -0.414849   \n",
              "282        -0.320732    -0.273142            0.077287       -0.449040   \n",
              "283        -0.466238    -0.389101           -0.602622       -0.495471   \n",
              "284        -0.449884     0.089251           -0.298632       -0.611154   \n",
              "285        -0.451326     0.092629           -0.299226       -0.576296   \n",
              "286        -0.378321     0.015863           -0.316004       -0.575349   \n",
              "287        -0.221738    -0.382001           -0.442243       -0.264283   \n",
              "288        -0.410715    -0.036805           -0.348478       -0.316619   \n",
              "289        -0.145559    -0.160490            0.785511       -0.272428   \n",
              "290        -0.349116    -0.381344           -0.512226       -0.326922   \n",
              "291        -0.166544    -0.163774            0.669770       -0.109796   \n",
              "292        -0.342848    -0.335351           -0.488507        0.137673   \n",
              "293        -0.068277    -0.352545           -0.202150       -0.325383   \n",
              "294        -0.463821    -0.384454           -0.553494       -0.443713   \n",
              "295        -0.374960    -0.384169           -0.543902       -0.376095   \n",
              "296        -0.412373    -0.349641           -0.344119       -0.043922   \n",
              "297        -0.234203    -0.394508           -0.523364       -0.425376   \n",
              "298        -0.455735    -0.368905           -0.389118       -0.270535   \n",
              "299        -0.406511    -0.050205           -0.353577       -0.289735   \n",
              "300        -0.318598    -0.328977           -0.457466        0.221011   \n",
              "301        -0.297657    -0.375949           -0.450513       -0.231843   \n",
              "302        -0.405708    -0.387530           -0.581588       -0.434600   \n",
              "303        -0.349233    -0.338341           -0.497143        0.102619   \n",
              "304        -0.109419    -0.103963            1.088659       -0.247586   \n",
              "305        -0.230265    -0.230492            0.202508        0.043906   \n",
              "306        -0.103749    -0.101759            1.122619       -0.283638   \n",
              "307        -0.335182    -0.328814           -0.439706        0.212021   \n",
              "308        -0.456763     0.110439           -0.291707       -0.628457   \n",
              "309        -0.188926    -0.187209            0.505643       -0.055808   \n",
              "310        -0.238360    -0.167069           -0.364629       -0.466679   \n",
              "311        -0.236378    -0.347684           -0.447257        0.066793   \n",
              "312        -0.369137    -0.347664           -0.524069       -0.006661   \n",
              "313        -0.436072     0.046713           -0.312535       -0.576415   \n",
              "314        -0.461671    -0.394453           -0.654937       -0.551007   \n",
              "315        -0.445074    -0.383235           -0.626795       -0.423593   \n",
              "316        -0.118317    -0.389741           -0.407088       -0.309348   \n",
              "317        -0.339508    -0.250701           -0.409741       -0.333530   \n",
              "318        -0.080032    -0.394546           -0.434187       -0.340227   \n",
              "319        -0.355081    -0.301242           -0.093309       -0.473600   \n",
              "320        -0.267234    -0.229378            0.342982       -0.410789   \n",
              "321        -0.336823    -0.394483           -0.582722       -0.482053   \n",
              "322        -0.156646    -0.050720            0.949607       -0.347250   \n",
              "323        -0.085439    -0.170586            0.763644       -0.298685   \n",
              "324        -0.447454     0.106223           -0.291985       -0.629027   \n",
              "325        -0.181391    -0.362661           -0.440175       -0.060505   \n",
              "326        -0.405086    -0.394467           -0.622207       -0.519755   \n",
              "327        -0.453120    -0.265957           -0.294360       -0.263342   \n",
              "328        -0.407864    -0.365805           -0.576458       -0.219293   \n",
              "329        -0.185924    -0.394520           -0.495438       -0.398711   \n",
              "330        -0.339844    -0.262683           -0.434430        0.136551   \n",
              "331        -0.340085    -0.334056           -0.484769        0.152843   \n",
              "332        -0.104758    -0.096463            1.149921       -0.294619   \n",
              "333        -0.137389    -0.374646           -0.434508       -0.162374   \n",
              "334        -0.108423    -0.078420            1.145411       -0.300945   \n",
              "335        -0.319180    -0.271873            0.084992       -0.447931   \n",
              "336        -0.390855    -0.092555           -0.358053       -0.462681   \n",
              "337        -0.189478    -0.165769            0.729160       -0.355194   \n",
              "338        -0.455640    -0.368722           -0.387177       -0.268490   \n",
              "339        -0.462680     0.028636           -0.355764       -0.624128   \n",
              "340        -0.448115     0.083805           -0.300412       -0.606706   \n",
              "341        -0.303655     0.033726            0.352690       -0.488412   \n",
              "342        -0.249628    -0.250765            0.060525        0.090610   \n",
              "343        -0.452198    -0.362105           -0.317219       -0.194787   \n",
              "344        -0.430051     0.106330           -0.160526       -0.609781   \n",
              "345        -0.177573    -0.363701           -0.439683       -0.069345   \n",
              "346        -0.419793    -0.351643           -0.334931       -0.068524   \n",
              "347        -0.438880    -0.356793           -0.311295       -0.131811   \n",
              "348        -0.134830    -0.130569            0.902322       -0.186292   \n",
              "349        -0.308831    -0.374822           -0.440540       -0.227479   \n",
              "350        -0.464059    -0.063380           -0.421755       -0.609108   \n",
              "351        -0.465004    -0.126440           -0.466980       -0.598815   \n",
              "352        -0.368662    -0.170834           -0.399479       -0.047722   \n",
              "353        -0.256998    -0.221005            0.393819       -0.403471   \n",
              "354        -0.423210     0.003021           -0.333323       -0.396519   \n",
              "355        -0.444917    -0.358422           -0.303819       -0.151830   \n",
              "356        -0.209668    -0.250544            0.268608       -0.255240   \n",
              "357        -0.427000    -0.362604           -0.320609       -0.181854   \n",
              "358        -0.468086    -0.332122           -0.614488       -0.565240   \n",
              "359        -0.200018    -0.217183           -0.377950       -0.436910   \n",
              "360        -0.296846    -0.331214           -0.455045        0.206785   \n",
              "361        -0.090583    -0.116048            1.053113       -0.290683   \n",
              "362        -0.337637    -0.037311           -0.330138       -0.543762   \n",
              "363        -0.294068    -0.376235           -0.450122       -0.233377   \n",
              "364        -0.088229    -0.141005            0.920649       -0.294345   \n",
              "365        -0.092815    -0.092376            1.178759       -0.287210   \n",
              "366        -0.141956    -0.155429            0.814562       -0.273394   \n",
              "367        -0.396476    -0.335106           -0.298897       -0.503197   \n",
              "368        -0.317194    -0.372034           -0.340765       -0.227595   \n",
              "369        -0.079839    -0.229964            0.448481       -0.307397   \n",
              "370        -0.172827    -0.170353            0.623694       -0.094640   \n",
              "371        -0.449570    -0.394456           -0.647938       -0.544324   \n",
              "372        -0.307131    -0.350419           -0.442333       -0.252093   \n",
              "373        -0.419553     0.100300           -0.117898       -0.599700   \n",
              "374        -0.411429    -0.364214           -0.336412       -0.187866   \n",
              "375        -0.461366     0.116311           -0.292886       -0.638440   \n",
              "376        -0.283261    -0.377096           -0.448945       -0.237995   \n",
              "377        -0.267089    -0.394500           -0.542386       -0.443539   \n",
              "378        -0.291768     0.026897            0.400957       -0.476998   \n",
              "379        -0.239434    -0.346852           -0.447650        0.073869   \n",
              "380        -0.219782    -0.394512           -0.515023       -0.417411   \n",
              "381        -0.460678    -0.390544           -0.647904       -0.509267   \n",
              "382        -0.327643    -0.326780           -0.449042        0.237019   \n",
              "383        -0.462279     0.055366           -0.336594       -0.628492   \n",
              "384        -0.464073    -0.064348           -0.422449       -0.608950   \n",
              "385        -0.439427    -0.380589           -0.619156       -0.392588   \n",
              "386        -0.457990    -0.032116           -0.290065       -0.488906   \n",
              "387        -0.233660    -0.173213           -0.366262       -0.463030   \n",
              "388        -0.462246     0.057597           -0.334994       -0.628856   \n",
              "389        -0.430592     0.029833           -0.318052       -0.562629   \n",
              "390        -0.135510    -0.301497           -0.400362       -0.386823   \n",
              "391        -0.212792    -0.200487           -0.373512       -0.446828   \n",
              "392        -0.069523    -0.339340           -0.132058       -0.323446   \n",
              "393        -0.412142    -0.032255           -0.346747       -0.325747   \n",
              "394        -0.467413    -0.287244           -0.582303       -0.572566   \n",
              "395        -0.358147    -0.394478           -0.595056       -0.493830   \n",
              "396        -0.426403     0.016934           -0.322268       -0.552095   \n",
              "397        -0.306976    -0.356596           -0.453447       -0.045515   \n",
              "398        -0.445781     0.104036           -0.292566       -0.627728   \n",
              "399        -0.233680    -0.234066            0.177473        0.052141   \n",
              "400        -0.466404    -0.219880           -0.533992       -0.583562   \n",
              "401        -0.218121    -0.382289           -0.441849       -0.265828   \n",
              "402        -0.466024    -0.194546           -0.515822       -0.587698   \n",
              "403        -0.365210    -0.181837           -0.403666       -0.025647   \n",
              "404        -0.319391    -0.371838           -0.340032       -0.226692   \n",
              "405        -0.341203    -0.245478           -0.408034       -0.337795   \n",
              "406        -0.394096    -0.359356           -0.557833       -0.143700   \n",
              "407        -0.206309    -0.022192            0.747955       -0.394938   \n",
              "408        -0.417233    -0.370193           -0.589132       -0.270731   \n",
              "409        -0.284014    -0.377036           -0.449027       -0.237673   \n",
              "410        -0.310317    -0.327545           -0.456780        0.237971   \n",
              "411        -0.289479    -0.374502           -0.350007       -0.238988   \n",
              "412        -0.071920    -0.393943           -0.425925       -0.328297   \n",
              "413        -0.247253    -0.394505           -0.530913       -0.432584   \n",
              "414        -0.172739    -0.365018           -0.439061       -0.080536   \n",
              "415        -0.197667    -0.383919           -0.439622       -0.274568   \n",
              "416        -0.163786    -0.386620           -0.435931       -0.289044   \n",
              "417        -0.243711    -0.298364           -0.005876       -0.246113   \n",
              "418        -0.462240     0.058017           -0.334693       -0.628924   \n",
              "419        -0.247480    -0.303659           -0.036266       -0.245103   \n",
              "420        -0.311342    -0.346221           -0.454957        0.054607   \n",
              "421        -0.290181    -0.293224           -0.236843        0.188427   \n",
              "422        -0.465614    -0.167191           -0.496205       -0.592163   \n",
              "423        -0.303998    -0.363674           -0.452417       -0.113810   \n",
              "424        -0.159124    -0.179545            0.676135       -0.268791   \n",
              "425        -0.463269    -0.010660           -0.383946       -0.617714   \n",
              "426        -0.158879    -0.270953           -0.392243       -0.404968   \n",
              "427        -0.316152    -0.394488           -0.570765       -0.470636   \n",
              "428        -0.389787    -0.385790           -0.562076       -0.404308   \n",
              "429        -0.380619    -0.124081           -0.368357       -0.436934   \n",
              "430        -0.300438    -0.373526           -0.346353       -0.234483   \n",
              "431        -0.084682    -0.178608            0.721065       -0.299862   \n",
              "432        -0.297650    -0.301044           -0.291613        0.206443   \n",
              "433        -0.327886    -0.300794           -0.448932        0.213011   \n",
              "434        -0.305928    -0.376623           -0.459291       -0.244746   \n",
              "435        -0.411591    -0.347470           -0.373965       -0.514004   \n",
              "436        -0.335680    -0.285371            0.003048       -0.459728   \n",
              "437        -0.444100     0.101838           -0.293150       -0.626422   \n",
              "438        -0.140132    -0.388506           -0.433355       -0.299151   \n",
              "439        -0.202863    -0.201801            0.403446       -0.022191   \n",
              "440        -0.265550    -0.131532           -0.355183       -0.487791   \n",
              "441        -0.447724     0.082599           -0.300806       -0.605722   \n",
              "442        -0.452084    -0.315689           -0.295274       -0.215371   \n",
              "443        -0.239588    -0.003076            0.612828       -0.426893   \n",
              "444        -0.464716    -0.107245           -0.453213       -0.601948   \n",
              "445        -0.118781    -0.379714           -0.432111       -0.205453   \n",
              "446        -0.113528    -0.103637            1.106365       -0.300889   \n",
              "447        -0.315653    -0.335975           -0.456448        0.153482   \n",
              "448        -0.272260    -0.377973           -0.447746       -0.242696   \n",
              "449        -0.468483    -0.358624           -0.633494       -0.560914   \n",
              "450        -0.098606    -0.094536            1.164082       -0.285016   \n",
              "451        -0.468415    -0.394168           -0.658370       -0.551745   \n",
              "452        -0.446409    -0.394457           -0.646109       -0.542578   \n",
              "453        -0.429925    -0.376139           -0.606302       -0.340418   \n",
              "454        -0.239975    -0.378909           -0.366516       -0.259338   \n",
              "455        -0.349969    -0.394480           -0.590326       -0.489314   \n",
              "456        -0.099757    -0.096152            1.154807       -0.284708   \n",
              "457        -0.210522    -0.182984            0.624643       -0.370240   \n",
              "458        -0.130328    -0.125856            0.935335       -0.197152   \n",
              "459        -0.143331    -0.128018            0.958347       -0.322199   \n",
              "460        -0.129984    -0.125495            0.937861       -0.197982   \n",
              "461        -0.180901    -0.210135            0.500556       -0.262953   \n",
              "462        -0.336744    -0.379991           -0.497062       -0.303381   \n",
              "463        -0.366362    -0.383229           -0.533364       -0.359737   \n",
              "464        -0.107225    -0.338466           -0.410189       -0.364863   \n",
              "465        -0.413940    -0.349392           -0.385631       -0.515684   \n",
              "466        -0.178469    -0.385450           -0.437530       -0.282771   \n",
              "467        -0.374632    -0.394474           -0.604591       -0.502935   \n",
              "468        -0.379343    -0.136792           -0.386526       -0.116018   \n",
              "469        -0.119704    -0.394536           -0.457135       -0.362138   \n",
              "\n",
              "     netAsset1   surplus1  inventoryAsset2  nonCAsset2  tanAsset2  \\\n",
              "0    -0.290859   0.005048        -0.423697   -0.559528  -0.558825   \n",
              "1     0.514328   0.797258         4.124054    1.563348   1.459804   \n",
              "2    -0.492521   0.839587         1.173996    1.961598   2.129604   \n",
              "3     0.555297   0.854295        -0.198369    0.869795   0.970115   \n",
              "4    -0.172511  -0.160814         2.738065    1.674385   1.820745   \n",
              "5     0.943427   0.844547         1.612173    2.223990   2.298452   \n",
              "6     0.906190  -0.160814         0.793332    2.247521   2.373852   \n",
              "7     0.082828  -0.160814        -0.436053    0.585193   0.662358   \n",
              "8     0.048221  -0.160814         4.880046    2.639711   2.430828   \n",
              "9    -0.435326  -0.160814        -0.249824    2.561691   2.258991   \n",
              "10   -0.271551  -0.160814        -0.402469    0.404265   0.359636   \n",
              "11    3.826493  -0.058609         1.662156    3.636578   3.845196   \n",
              "12    0.018965   0.394611         0.952631    0.192789   0.107899   \n",
              "13   -0.199969  -0.049077        -0.211534    0.373862   0.271525   \n",
              "14    2.045889   1.050647         2.691613    5.083747   5.275722   \n",
              "15   -0.335374   0.506399        -0.074626   -0.206841  -0.176592   \n",
              "16    0.374715  -0.160814        -0.368705    0.588722   0.670381   \n",
              "17   -0.212576  -0.160814         0.031022    0.174449   0.231900   \n",
              "18   -0.480146  -0.243329        -0.474324   -0.080362  -0.048286   \n",
              "19    0.471761   1.363481         0.303359    0.463723   0.543248   \n",
              "20   -0.081644   0.302418         0.236165    0.162919   0.207474   \n",
              "21    0.219978  -0.160811         1.823645    1.448932   1.494415   \n",
              "22    0.132666  -0.160814         0.260842    1.497176   1.486063   \n",
              "23   -0.200549  -0.160814         0.097976   -0.199866  -0.203356   \n",
              "24   -0.308421  -0.160814        -0.121233   -0.115490  -0.098125   \n",
              "25    1.372793  -0.160814        -0.226125    0.404964   0.479545   \n",
              "26    0.378855   0.560232         1.329012    0.487543   0.567017   \n",
              "27   -0.375654  -0.160814         1.198819   -0.302478  -0.272182   \n",
              "28   -0.349113  -0.160814        -0.432253   -0.173147  -0.153111   \n",
              "29    1.655230  -0.156027         3.670407    0.317257   0.024399   \n",
              "30    0.716344  -0.160814         0.627836    1.329996   1.429656   \n",
              "31    1.155797  -0.160814        -0.140888    0.938613   0.875608   \n",
              "32   -0.325390  -0.160814        -0.409953   -0.143451  -0.147221   \n",
              "33   -0.034858  -0.160814        -0.369148   -0.139687  -0.111101   \n",
              "34   -0.416692  -0.160814         0.004001   -0.218748  -0.207932   \n",
              "35    0.133609  -0.160814         0.400752    0.329554   0.399710   \n",
              "36   -0.126658  -0.023600        -0.069405   -0.353728  -0.378384   \n",
              "37    0.012570   0.349655         4.048043    1.087575   1.155165   \n",
              "38   -0.370047  -0.086998        -0.344697   -0.276198  -0.251438   \n",
              "39    0.280628  -0.209351         0.360241    1.074382   1.185819   \n",
              "40   -0.295798  -0.160814        -0.463203   -0.317529  -0.287689   \n",
              "41   -0.028167  -0.160814         0.577586   -0.030509   0.024231   \n",
              "42   -0.430520  -0.160814        -0.368470   -0.187950  -0.160652   \n",
              "43   -0.062256   0.148556        -0.354424   -0.189319  -0.164910   \n",
              "44   -0.329557  -0.110909        -0.293790   -0.244635  -0.353809   \n",
              "45   -0.157413  -0.160814        -0.383842    0.127668   0.189375   \n",
              "46    0.004122  -0.160814         0.381996    0.338367   0.405393   \n",
              "47   -0.305130  -0.160814        -0.374982   -0.430574  -0.435452   \n",
              "48   -0.322139  -0.160814        -0.433867   -0.367670  -0.349997   \n",
              "49    0.440817  -0.160814        -0.364167    0.009405  -0.038838   \n",
              "50   -0.262260   0.057670        -0.395614   -0.134537  -0.134801   \n",
              "51   -0.030504   0.242711         0.327631    0.368968   0.447287   \n",
              "52   -0.375562  -0.152324        -0.463230   -0.389354  -0.386082   \n",
              "53   -0.332065  -0.160814        -0.377142    0.013516   0.035379   \n",
              "54   -0.299904  -0.160814        -0.443234   -0.507748  -0.534443   \n",
              "55   -0.299904  -0.160814        -0.443234   -0.507748  -0.534443   \n",
              "56   -0.385605   0.010835        -0.467941   -0.592624  -0.584690   \n",
              "57   -0.029314  -0.160814         0.153694    0.150316   0.178835   \n",
              "58   -0.312464  -0.221116        -0.453916   -0.420056  -0.398514   \n",
              "59    0.632652   0.234122        -0.288826    0.419872   0.284057   \n",
              "60    0.000591   0.227702         0.337411    0.215723   0.163447   \n",
              "61   -0.418733  -0.160814        -0.392784   -0.546315  -0.545812   \n",
              "62   -0.108418   0.041941        -0.439114   -0.331464  -0.308863   \n",
              "63   -0.229285  -0.088679        -0.338624   -0.473436  -0.457563   \n",
              "64    0.005477   0.174897        -0.203061   -0.314766  -0.307543   \n",
              "65   -0.379760  -0.146385        -0.130702   -0.399469  -0.385180   \n",
              "66   -0.260492  -0.160814        -0.460981   -0.560557  -0.562830   \n",
              "67   -0.472716  -0.160814        -0.203249   -0.611930  -0.608831   \n",
              "68   -0.094902  -0.160814        -0.417581   -0.348902  -0.328202   \n",
              "69   -0.344823  -0.160814        -0.422136   -0.308818  -0.278955   \n",
              "70   -0.239630  -0.160814         0.332700   -0.380633  -0.547645   \n",
              "71   -0.430506  -0.160814        -0.390114   -0.268068  -0.233318   \n",
              "72   -0.169492  -0.160814        -0.422942   -0.222148  -0.183167   \n",
              "73   -0.251327  -0.160814        -0.474324   -0.484075  -0.492954   \n",
              "74   -0.217571  -0.160814        -0.399482   -0.401541  -0.378298   \n",
              "75   -0.339359  -0.160814        -0.081853   -0.548415  -0.547279   \n",
              "76   -0.064004  -0.160814        -0.377563   -0.012045   0.043425   \n",
              "77   -0.309512  -0.160814        -0.392734   -0.605798  -0.609667   \n",
              "78   -0.153066  -0.160814         0.152328    0.014477   0.039395   \n",
              "79   -0.175255  -0.160814        -0.381403   -0.459609  -0.470044   \n",
              "80    0.953546  -0.160814         0.271834    1.916621   2.102186   \n",
              "81   -0.382640  -0.160814        -0.459688   -0.503628  -0.493294   \n",
              "82   -0.033194  -0.160814        -0.405427   -0.240103  -0.203057   \n",
              "83   -0.507866  -0.174534        -0.379812   -0.288279  -0.258663   \n",
              "84   -0.366833  -0.136327        -0.421430   -0.549154  -0.539003   \n",
              "85    0.197067  -0.160814         0.466252    0.367760   0.290498   \n",
              "86   -0.412052  -0.160814        -0.285302   -0.482301  -0.483318   \n",
              "87    0.236941   0.481403        -0.284063    0.602136   0.667489   \n",
              "88   -0.281297  -0.160814        -0.382692   -0.287205  -0.256292   \n",
              "89   -0.331164  -0.157914        -0.243326   -0.427902  -0.412782   \n",
              "90   -0.371800  -0.160814         0.534930   -0.452386  -0.505149   \n",
              "91   -0.078463  -0.160814         0.265709   -0.275880  -0.338584   \n",
              "92   -0.470273  -0.160814        -0.474324   -0.602850  -0.597088   \n",
              "93   -0.418029  -0.026101        -0.438014   -0.518354  -0.504779   \n",
              "94   -0.128951   0.297297        -0.474324   -0.317466  -0.331937   \n",
              "95   -0.382503  -0.160814        -0.205479   -0.587692  -0.619070   \n",
              "96   -0.395935  -0.160814        -0.474324   -0.480260  -0.465827   \n",
              "97   -0.368058  -0.160814        -0.129148   -0.345857  -0.382954   \n",
              "98   -0.279399  -0.160814        -0.367508   -0.551189  -0.541242   \n",
              "99   -0.437658  -0.160814        -0.363122   -0.573468  -0.564826   \n",
              "100  -0.437658  -0.160814        -0.363122   -0.573468  -0.564826   \n",
              "101  -0.246890  -0.160814        -0.262359   -0.410402  -0.438749   \n",
              "102  -0.396702  -0.160814        -0.382169   -0.203046  -0.169738   \n",
              "103  -0.385735  -0.160814        -0.474324   -0.583635  -0.581623   \n",
              "104   0.035820  -0.160814        -0.363114   -0.152472  -0.125997   \n",
              "105  -0.039118  -0.160814        -0.248004    0.053952   0.069574   \n",
              "106  -0.397602  -0.160814        -0.471455   -0.606396  -0.606398   \n",
              "107  -0.436893  -0.160814        -0.394117   -0.551077  -0.543874   \n",
              "108  -0.239424  -0.160814        -0.160411   -0.232132  -0.281022   \n",
              "109  -0.431466  -0.160814        -0.360929   -0.580864  -0.594599   \n",
              "110  -0.128537  -0.160814        -0.432442   -0.174850  -0.193002   \n",
              "111  -0.368849  -0.160814         0.096097   -0.184339  -0.171652   \n",
              "112  -0.987218  -0.160814        -0.425526   -0.704369  -0.707179   \n",
              "113  -0.171270  -0.160814        -0.354969   -0.378896  -0.395553   \n",
              "114  -0.224638  -0.160814        -0.021546    0.210307   0.282931   \n",
              "115  -0.571930  -0.160814         0.683829   -0.188395  -0.180460   \n",
              "116  -0.326291  -0.024696        -0.450280   -0.447692  -0.433864   \n",
              "117  -0.403736  -0.140964        -0.466674   -0.593287  -0.589155   \n",
              "118  -0.418772  -0.134488        -0.454584   -0.625239  -0.625724   \n",
              "119  -0.415769  -0.134926        -0.434356   -0.506517  -0.491299   \n",
              "120  -0.082112  -0.160814        -0.435994   -0.256860  -0.223040   \n",
              "121  -0.231060  -0.160814        -0.241859   -0.153859  -0.121266   \n",
              "122  -0.338649  -0.140549        -0.417684   -0.500092  -0.485541   \n",
              "123  -0.380083  -0.140493        -0.244829   -0.541300  -0.531244   \n",
              "124  -0.191864  -0.124720        -0.159558   -0.349913  -0.328350   \n",
              "125  -0.277008  -0.080939        -0.277063   -0.411664  -0.389995   \n",
              "126  -0.294669  -0.160814        -0.460623   -0.462889  -0.445380   \n",
              "127  -0.183552  -0.160814        -0.353779   -0.454281  -0.446147   \n",
              "128  -0.323259  -0.160814         0.144133   -0.351941  -0.331044   \n",
              "129  -0.170292  -0.207785        -0.328190   -0.384214  -0.407009   \n",
              "130  -0.257608   0.152859        -0.280556   -0.283320  -0.249247   \n",
              "131  -0.027724  -0.160814        -0.463528    0.070167   0.110159   \n",
              "132  -0.267769  -0.160814        -0.474324    0.867819   0.647536   \n",
              "133  -0.236033  -0.160814        -0.148937   -0.308260  -0.276573   \n",
              "134  -0.189777  -0.160814        -0.282776   -0.201721  -0.161738   \n",
              "135  -0.321164  -0.160814        -0.370076   -0.364995  -0.341395   \n",
              "136  -0.172371  -0.016325        -0.352675   -0.296748  -0.298705   \n",
              "137  -0.364870  -0.160814        -0.266510   -0.573846  -0.570663   \n",
              "138  -0.409281  -0.160814        -0.466689   -0.660383  -0.658276   \n",
              "139  -0.407600  -0.093333        -0.428869   -0.417360  -0.396045   \n",
              "140  -0.414317  -0.120592        -0.434501   -0.590412  -0.582509   \n",
              "141  -0.416495  -0.160814        -0.168030   -0.658774  -0.663328   \n",
              "142  -0.267874  -0.002773        -0.451777   -0.513775  -0.503297   \n",
              "143   0.021416  -0.147770        -0.366364   -0.141261  -0.096094   \n",
              "144  -0.239577  -0.160814        -0.474324   -0.366711  -0.345497   \n",
              "145  -0.189890  -0.160814        -0.388099   -0.412021  -0.389663   \n",
              "146  -0.292979  -0.160814        -0.474324   -0.508727  -0.514880   \n",
              "147  -0.459165  -0.160814        -0.228966   -0.582329  -0.576966   \n",
              "148  -0.539563  -0.160814        -0.474324   -0.549957  -0.538381   \n",
              "149  -0.296225  -0.160814        -0.454341   -0.471980  -0.454087   \n",
              "150  -0.338832  -0.160814        -0.406122   -0.541499  -0.529830   \n",
              "151   0.353127  -0.160814        -0.086147    0.207809   0.174145   \n",
              "152   1.159143  -0.160814        -0.336768    0.777057   0.872655   \n",
              "153   1.159143  -0.160814        -0.336768    0.777057   0.872655   \n",
              "154  -0.378417  -0.160814        -0.380036   -0.542131  -0.539597   \n",
              "155  -0.298740  -0.160814        -0.065874   -0.407746  -0.396421   \n",
              "156  -0.288816  -0.160814        -0.447111   -0.362951  -0.337713   \n",
              "157  -0.231506  -0.160814        -0.474324   -0.494303  -0.478019   \n",
              "158   0.994887   1.390865         0.468990    0.706308   0.717601   \n",
              "159  -1.975476  -0.160814         0.415362   -0.342027  -0.483047   \n",
              "160   1.701303   0.654054         0.525289    0.567921   0.636989   \n",
              "161  -0.234997  -0.160814         0.392440    0.162697   0.157504   \n",
              "162  -0.049256  -0.160814        -0.318118    0.322013   0.270448   \n",
              "163  -0.321039   0.004587        -0.431296   -0.312430  -0.321945   \n",
              "164  -0.213561  -0.211735        -0.459377   -0.263627  -0.227825   \n",
              "165   0.316842   0.198258         0.198359   -0.159079  -0.146839   \n",
              "166  -0.351030  -0.160814        -0.462153   -0.543969  -0.533463   \n",
              "167  -0.477339  -0.167027        -0.458084   -0.678994  -0.688741   \n",
              "168  -0.473749  -0.160814        -0.344071   -0.676878  -0.681753   \n",
              "169  -0.189318  -0.160814        -0.435171   -0.385090  -0.361069   \n",
              "170  -0.038972  -0.160814        -0.212195   -0.416828  -0.436462   \n",
              "171  -0.473561  -0.483913        -0.130041   -0.414808  -0.443855   \n",
              "172  -0.045927  -0.193832        -0.012167   -0.280946  -0.246912   \n",
              "173  -0.413579  -0.160814        -0.474324   -0.502677  -0.487174   \n",
              "174  -0.623865  -0.451253        -0.474324   -0.524100  -0.515796   \n",
              "175  -0.351770  -0.160814        -0.390574    0.117367   0.179593   \n",
              "176  -0.192505  -0.160814        -0.441825   -0.186979  -0.149111   \n",
              "177   0.277967  -0.160814        -0.348377   -0.189213  -0.152089   \n",
              "178  -0.337516  -0.146831        -0.474324   -0.493490  -0.478839   \n",
              "179  -0.127313  -0.160814        -0.457953   -0.104525  -0.076074   \n",
              "180  -0.112130  -0.160814        -0.251523   -0.492659  -0.480162   \n",
              "181   0.637504  -0.160814        -0.215969    0.004815   0.022079   \n",
              "182  -0.389205  -0.160814        -0.457504   -0.554224  -0.543009   \n",
              "183  -0.389205  -0.160814        -0.457504   -0.554224  -0.543009   \n",
              "184  -0.231433  -0.160814        -0.463098   -0.465899  -0.461648   \n",
              "185  -0.261358  -0.022733        -0.434222   -0.471991  -0.464372   \n",
              "186  -0.321402  -0.160814        -0.443626   -0.465503  -0.508237   \n",
              "187  -0.261741  -0.134266        -0.340664   -0.486542  -0.469601   \n",
              "188  -0.280742   0.148926        -0.287849   -0.445573  -0.425165   \n",
              "189  -0.222681  -0.043745        -0.447660   -0.467051  -0.452497   \n",
              "190  -0.182303  -0.160814        -0.455587   -0.492528  -0.481220   \n",
              "191  -0.237215  -0.016779        -0.474324   -0.426792  -0.406670   \n",
              "192   9.664131  14.691887         6.168130    5.083747   2.917045   \n",
              "193  -0.171405  -0.160814        -0.474324   -0.405560  -0.399858   \n",
              "194  -0.231602  -0.160814        -0.324948   -0.387163  -0.361938   \n",
              "195  -0.382755  -0.160814        -0.465990   -0.577582  -0.570526   \n",
              "196   0.081976  -0.160814         0.758792    0.103247   0.014177   \n",
              "197  -0.230727  -0.023388        -0.474324   -0.311867  -0.388599   \n",
              "198  -0.180947  -0.160814        -0.384881   -0.567027  -0.561800   \n",
              "199   0.110046   0.284099        -0.069451   -0.054373  -0.021299   \n",
              "200  -0.111465  -0.160814        -0.474324   -0.151863  -0.128895   \n",
              "201  -0.351908  -0.160814        -0.474324   -0.659914  -0.674428   \n",
              "202  -0.351908  -0.160814        -0.474324   -0.659914  -0.674428   \n",
              "203   1.426372  -0.160814         0.231002    0.392348   0.380649   \n",
              "204   0.377393  -0.160814         0.225732   -0.085321  -0.095473   \n",
              "205   1.584814  -0.084540         6.272286    0.095657  -0.483326   \n",
              "206  -0.279481  -0.160814        -0.214898   -0.563465  -0.564033   \n",
              "207   7.836260  -0.160814         3.361953    8.694915   9.426533   \n",
              "208  -0.413637  -0.160814        -0.459589   -0.501911  -0.514883   \n",
              "209   0.058277  -0.160814        -0.474324   -0.070663  -0.022663   \n",
              "210   2.627605  -0.148699         3.160511    0.639351   0.727646   \n",
              "211   0.245377  -0.160814        -0.218433   -0.072881  -0.025014   \n",
              "212  -0.229357  -0.041191        -0.283354   -0.402763  -0.379362   \n",
              "213  -0.447741  -0.160814        -0.474324   -0.636280  -0.633874   \n",
              "214  -0.319031  -0.160814        -0.447677   -0.486593  -0.475162   \n",
              "215  -0.381404  -0.096955        -0.418054   -0.524624  -0.523231   \n",
              "216  -0.414655  -0.120921        -0.269955   -0.584760  -0.582715   \n",
              "217   0.410680  -0.160814        -0.474324   -0.250169  -0.213604   \n",
              "218   2.200916   3.240786         1.340838    1.837419   1.011658   \n",
              "219  -0.586651  -0.145570        -0.296446    2.007380   2.235102   \n",
              "220  -0.470491  -0.160814        -0.364113   -0.425220  -0.404459   \n",
              "221  -0.280531  -0.160814        -0.474324   -0.489609  -0.479047   \n",
              "222  -0.395665  -0.160814        -0.470851   -0.545114  -0.534238   \n",
              "223   1.278456   0.360947         0.493706    1.686537   1.727151   \n",
              "224   1.278456   0.360947         0.493706    1.686537   1.727151   \n",
              "225  -0.391604  -0.160814        -0.258870   -0.565662  -0.555416   \n",
              "226  -0.800930  -0.160814        -0.447377   -0.620219  -0.616816   \n",
              "227  -0.061418  -0.160814        -0.287614   -0.214733  -0.179681   \n",
              "228  -0.394086  -0.160814        -0.287090   -0.530844  -0.520093   \n",
              "229  -0.419851  -0.160814        -0.364707   -0.426090  -0.404163   \n",
              "230  -0.204594  -0.160814        -0.462520   -0.345180  -0.318589   \n",
              "231  -0.383395  -0.160814        -0.454018   -0.588460  -0.581255   \n",
              "232  -0.391089  -0.160814        -0.440514   -0.532212  -0.522652   \n",
              "233  -0.132728  -0.160814         0.214740   -0.371800  -0.409119   \n",
              "234   0.096145  -0.160814         0.083501    0.057283   0.116688   \n",
              "235  -0.271723  -0.202645        -0.196420    0.156052   0.088857   \n",
              "236   2.467333  -0.106336         3.072000    1.678778   1.794618   \n",
              "237  -0.281039  -0.160814        -0.431463   -0.278492  -0.252290   \n",
              "238   0.198979  -0.160814        -0.355015    0.036353   0.092412   \n",
              "239  -0.163708  -0.160814        -0.410469   -0.402593  -0.378740   \n",
              "240  -0.062051  -0.004969        -0.443701   -0.003260   0.045367   \n",
              "241  -0.097382   0.534097        -0.309497   -0.110133  -0.069247   \n",
              "242   0.228915  -0.160814        -0.248964   -0.097192  -0.068707   \n",
              "243   0.139853  -0.160814        -0.418251   -0.071972  -0.027105   \n",
              "244  -0.280469  -0.160814        -0.420745   -0.457494  -0.438659   \n",
              "245   0.155316  -0.160814        -0.296589    0.048679   0.102890   \n",
              "246   0.782714  -0.160814        -0.321701    0.503919   0.583997   \n",
              "247   0.709668  -0.160814        -0.369566    0.381259   0.439183   \n",
              "248  -0.155604  -0.160814        -0.416185   -0.335355  -0.307274   \n",
              "249  -0.151035  -0.160814        -0.417416   -0.330476  -0.302037   \n",
              "250   0.173879  -0.160814        -0.413596   -0.070080  -0.028178   \n",
              "251   0.771341  -0.160814        -0.367008    0.443622   0.518085   \n",
              "252  -0.352580  -0.160814        -0.451651   -0.535163  -0.539496   \n",
              "253  -0.188512  -0.160814        -0.398158   -0.376664  -0.351894   \n",
              "254  -0.259829  -0.160814        -0.421938   -0.456053  -0.458441   \n",
              "255  -0.243813  -0.160814        -0.446167   -0.429215  -0.410077   \n",
              "256   0.191507  -0.160814        -0.320890   -0.238028  -0.223392   \n",
              "257  -0.271081  -0.160814        -0.427917   -0.470567  -0.479780   \n",
              "258   0.288950  -0.160814        -0.301938    0.145644   0.205364   \n",
              "259  -0.274395  -0.160814        -0.418691   -0.452091  -0.432830   \n",
              "260  -0.270320  -0.160814        -0.442250   -0.473669  -0.492144   \n",
              "261   0.842244  -0.160814        -0.351204    0.505761   0.584704   \n",
              "262   0.898745  -0.160814        -0.276466    0.391559   0.448053   \n",
              "263   0.101303  -0.160814        -0.345965   -0.288373  -0.296254   \n",
              "264  -0.019667  -0.160814        -0.357008   -0.307492  -0.285632   \n",
              "265  -0.190989  -0.160814        -0.287865   -0.210434  -0.171181   \n",
              "266   1.152379  -0.160814        -0.337309    0.771423   0.866684   \n",
              "267  -0.341870  -0.160814        -0.449940   -0.529589  -0.538469   \n",
              "268  -0.240588  -0.160814        -0.411713   -0.431232  -0.421950   \n",
              "269  -0.032620  -0.160814        -0.289067   -0.087688  -0.041225   \n",
              "270   0.811824  -0.160814        -0.352590    0.479719   0.557063   \n",
              "271  -0.379846  -0.160814        -0.449304   -0.537681  -0.525117   \n",
              "272   0.182677  -0.160814        -0.408020   -0.082799  -0.069045   \n",
              "273   0.870799  -0.160814        -0.269994    0.350188   0.402486   \n",
              "274  -0.257859  -0.160814        -0.448304   -0.427039  -0.406156   \n",
              "275   1.132066  -0.160814        -0.338002    0.753876   0.848051   \n",
              "276   0.676808  -0.160814        -0.225070    0.063000   0.086167   \n",
              "277  -0.274735  -0.160814        -0.429859   -0.475281  -0.486710   \n",
              "278  -0.031840  -0.160814        -0.270022   -0.162291  -0.126646   \n",
              "279   0.407938  -0.160814        -0.265237   -0.104282  -0.085412   \n",
              "280  -0.257525  -0.160814        -0.420714   -0.453081  -0.454072   \n",
              "281  -0.281537  -0.160814        -0.421106   -0.458444  -0.439684   \n",
              "282   0.016028  -0.160814        -0.362172   -0.333576  -0.319974   \n",
              "283  -0.341364  -0.160814        -0.454153   -0.507898  -0.493162   \n",
              "284  -0.292197  -0.160814        -0.432004   -0.486331  -0.508360   \n",
              "285  -0.197163  -0.160814        -0.435737   -0.417277  -0.435361   \n",
              "286  -0.282703  -0.160814        -0.434093   -0.485560  -0.501821   \n",
              "287  -0.198917  -0.160814        -0.321156   -0.267433  -0.232955   \n",
              "288   0.223379  -0.160814        -0.405050   -0.046957  -0.029792   \n",
              "289   0.427239  -0.160814        -0.232949   -0.047679  -0.024641   \n",
              "290  -0.248667  -0.160814        -0.334372   -0.305812  -0.274324   \n",
              "291   0.805759  -0.160814        -0.254933    0.253902   0.296433   \n",
              "292   0.924832  -0.160814        -0.355039    0.575594   0.658422   \n",
              "293  -0.100749  -0.160814        -0.374410   -0.346024  -0.323598   \n",
              "294  -0.299814  -0.160814        -0.451243   -0.467664  -0.449870   \n",
              "295  -0.278958  -0.160814        -0.360911   -0.359353  -0.332235   \n",
              "296   0.285933  -0.160814        -0.406573    0.049707   0.101845   \n",
              "297  -0.289620  -0.160814        -0.423838   -0.465634  -0.447441   \n",
              "298  -0.160791  -0.160814        -0.441505   -0.333047  -0.305018   \n",
              "299   0.266916  -0.160814        -0.401873   -0.008619   0.012195   \n",
              "300   1.049079  -0.160814        -0.332363    0.697194   0.788254   \n",
              "301  -0.189922  -0.160814        -0.283381   -0.202757  -0.162861   \n",
              "302  -0.314997  -0.160814        -0.392487   -0.423055  -0.401136   \n",
              "303   0.858340  -0.160814        -0.360224    0.518424   0.597628   \n",
              "304   0.674510  -0.160814        -0.224538    0.059598   0.082419   \n",
              "305   0.952165  -0.160814        -0.288837    0.470644   0.535160   \n",
              "306   0.595504  -0.160814        -0.219360   -0.005671   0.012747   \n",
              "307   1.018406  -0.160814        -0.348019    0.659828   0.748422   \n",
              "308  -0.296862  -0.160814        -0.438802   -0.499297  -0.524150   \n",
              "309   0.857185  -0.160814        -0.266842    0.330033   0.380286   \n",
              "310  -0.253672  -0.160814        -0.418666   -0.448110  -0.446763   \n",
              "311   0.707617  -0.160814        -0.357337    0.390508   0.462375   \n",
              "312   0.651049  -0.160814        -0.376388    0.340194   0.408101   \n",
              "313  -0.282829  -0.160814        -0.418356   -0.460301  -0.476658   \n",
              "314  -0.386088  -0.160814        -0.456450   -0.551451  -0.540018   \n",
              "315  -0.139818  -0.160814        -0.438057   -0.339799  -0.314993   \n",
              "316  -0.198661  -0.160814        -0.404579   -0.381344  -0.356625   \n",
              "317  -0.217337  -0.160814        -0.322931   -0.278305  -0.255008   \n",
              "318  -0.224236  -0.160814        -0.401735   -0.407471  -0.384694   \n",
              "319  -0.077840  -0.160814        -0.384254   -0.384687  -0.371638   \n",
              "320   0.162222  -0.160814        -0.327779   -0.253974  -0.239510   \n",
              "321  -0.333141  -0.160814        -0.438551   -0.504350  -0.489206   \n",
              "322   0.476147  -0.160814        -0.255088   -0.083413  -0.073716   \n",
              "323   0.405315  -0.160814        -0.265800   -0.105528  -0.086641   \n",
              "324  -0.297043  -0.160814        -0.441713   -0.504058  -0.529017   \n",
              "325   0.411953  -0.160814        -0.370805    0.137391   0.193719   \n",
              "326  -0.362091  -0.160814        -0.448338   -0.530103  -0.516989   \n",
              "327  -0.124446  -0.160814        -0.437402   -0.305628  -0.283570   \n",
              "328   0.247714  -0.160814        -0.407839   -0.006596   0.039330   \n",
              "329  -0.269145  -0.160814        -0.416917   -0.447420  -0.427792   \n",
              "330   0.957280  -0.160814        -0.351498    0.599300   0.677979   \n",
              "331   0.953607  -0.160814        -0.352795    0.600335   0.684732   \n",
              "332   0.606227  -0.160814        -0.223326   -0.012215   0.004864   \n",
              "333   0.175350  -0.160814        -0.381583   -0.065164  -0.021270   \n",
              "334   0.599007  -0.160814        -0.225302   -0.016235  -0.000776   \n",
              "335   0.020268  -0.160814        -0.361174   -0.331267  -0.317640   \n",
              "336  -0.252162  -0.160814        -0.373672   -0.375079  -0.372868   \n",
              "337   0.374710  -0.160814        -0.277791   -0.138275  -0.122560   \n",
              "338  -0.159149  -0.160814        -0.441390   -0.331457  -0.303308   \n",
              "339  -0.316365  -0.160814        -0.445864   -0.516315  -0.536022   \n",
              "340  -0.290997  -0.160814        -0.430257   -0.482999  -0.504301   \n",
              "341   0.101602  -0.160814        -0.345893   -0.288210  -0.296077   \n",
              "342   0.996653  -0.160814        -0.299139    0.536503   0.607700   \n",
              "343  -0.099983  -0.160814        -0.437246   -0.274165  -0.241660   \n",
              "344  -0.220424  -0.160814        -0.423965   -0.464290  -0.487258   \n",
              "345   0.391422  -0.160814        -0.371740    0.119814   0.175064   \n",
              "346   0.215525  -0.160814        -0.412201   -0.008940   0.039694   \n",
              "347   0.034409  -0.160814        -0.426680   -0.159803  -0.120183   \n",
              "348   0.732894  -0.160814        -0.238059    0.146031   0.177620   \n",
              "349  -0.182738  -0.160814        -0.292795   -0.205326  -0.165747   \n",
              "350  -0.332207  -0.160814        -0.448396   -0.524560  -0.537542   \n",
              "351  -0.343063  -0.160814        -0.450130   -0.530210  -0.538583   \n",
              "352   0.658853  -0.160814        -0.373274    0.336512   0.390177   \n",
              "353   0.190195  -0.160814        -0.321199   -0.238743  -0.224115   \n",
              "354   0.093982  -0.160814        -0.414492   -0.160901  -0.154582   \n",
              "355  -0.022884  -0.160814        -0.431260   -0.207526  -0.170758   \n",
              "356   0.169232  -0.160814        -0.253784   -0.112092  -0.081969   \n",
              "357  -0.099193  -0.160814        -0.411687   -0.248112  -0.213315   \n",
              "358  -0.378474  -0.160814        -0.455789   -0.548639  -0.541980   \n",
              "359  -0.245719  -0.160814        -0.414440   -0.437851  -0.431681   \n",
              "360   1.032762  -0.160814        -0.342525    0.668863   0.757819   \n",
              "361   0.556993  -0.160814        -0.233248   -0.033447  -0.015619   \n",
              "362  -0.274264  -0.160814        -0.429609   -0.474674  -0.485817   \n",
              "363  -0.190347  -0.160814        -0.285167   -0.205815  -0.166176   \n",
              "364   0.487584  -0.160814        -0.248144   -0.066432  -0.048119   \n",
              "365   0.622830  -0.160814        -0.219118   -0.002159   0.015208   \n",
              "366   0.441740  -0.160814        -0.231778   -0.044059  -0.021419   \n",
              "367  -0.190961  -0.160814        -0.410866   -0.446281  -0.433898   \n",
              "368  -0.129022  -0.160814        -0.423350   -0.306964  -0.276806   \n",
              "369   0.240173  -0.160814        -0.301242   -0.184008  -0.163966   \n",
              "370   0.820196  -0.160814        -0.258276    0.275274   0.319973   \n",
              "371  -0.380957  -0.160814        -0.454715   -0.546886  -0.535093   \n",
              "372  -0.195378  -0.160814        -0.290936   -0.217284  -0.180692   \n",
              "373  -0.193677  -0.160814        -0.417480   -0.449665  -0.471378   \n",
              "374  -0.110201  -0.160814        -0.396021   -0.242475  -0.207047   \n",
              "375  -0.301270  -0.160814        -0.443452   -0.508459  -0.534574   \n",
              "376  -0.191627  -0.160814        -0.290545   -0.215022  -0.176153   \n",
              "377  -0.303566  -0.160814        -0.428553   -0.478041  -0.460825   \n",
              "378   0.131888  -0.160814        -0.338550   -0.271650  -0.278097   \n",
              "379   0.724052  -0.160814        -0.356588    0.404577   0.477308   \n",
              "380  -0.283504  -0.160814        -0.421771   -0.460194  -0.441572   \n",
              "381  -0.302330  -0.160814        -0.450729   -0.479528  -0.463579   \n",
              "382   1.089945  -0.160814        -0.342300    0.719418   0.811572   \n",
              "383  -0.311763  -0.160814        -0.445129   -0.513920  -0.535581   \n",
              "384  -0.332373  -0.160814        -0.448422   -0.524646  -0.537558   \n",
              "385  -0.081005  -0.160814        -0.433471   -0.289231  -0.261220   \n",
              "386  -0.229594  -0.160814        -0.440897   -0.426754  -0.433912   \n",
              "387  -0.252697  -0.160814        -0.418148   -0.446853  -0.444915   \n",
              "388  -0.311379  -0.160814        -0.445067   -0.513720  -0.535544   \n",
              "389  -0.279112  -0.160814        -0.412940   -0.449972  -0.464078   \n",
              "390  -0.232338  -0.160814        -0.407329   -0.420591  -0.406304   \n",
              "391  -0.248368  -0.160814        -0.415848   -0.441269  -0.436706   \n",
              "392  -0.064022  -0.160814        -0.366528   -0.328571  -0.306401   \n",
              "393   0.208597  -0.160814        -0.406129   -0.059974  -0.044048   \n",
              "394  -0.370748  -0.160814        -0.454554   -0.544618  -0.541239   \n",
              "395  -0.342184  -0.160814        -0.441608   -0.512395  -0.497885   \n",
              "396  -0.276272  -0.160814        -0.408801   -0.442078  -0.454465   \n",
              "397   0.319115  -0.160814        -0.303145    0.167532   0.228496   \n",
              "398  -0.296696  -0.160814        -0.441529   -0.503610  -0.528359   \n",
              "399   0.960010  -0.160814        -0.290654    0.482256   0.547951   \n",
              "400  -0.359150  -0.160814        -0.452701   -0.538582  -0.540126   \n",
              "401  -0.199346  -0.160814        -0.322955   -0.270515  -0.236295   \n",
              "402  -0.354789  -0.160814        -0.452004   -0.536312  -0.539708   \n",
              "403   0.694603  -0.160814        -0.370665    0.367992   0.424654   \n",
              "404  -0.128252  -0.160814        -0.423557   -0.306142  -0.275925   \n",
              "405  -0.218487  -0.160814        -0.324607   -0.281501  -0.258900   \n",
              "406   0.391105  -0.160814        -0.396658    0.116692   0.170432   \n",
              "407   0.349617  -0.160814        -0.285764   -0.152598  -0.148834   \n",
              "408   0.150141  -0.160814        -0.415447   -0.090490  -0.049882   \n",
              "409  -0.191538  -0.160814        -0.290170   -0.214380  -0.175458   \n",
              "410   1.105195  -0.160814        -0.339226    0.730872   0.823635   \n",
              "411  -0.138726  -0.160814        -0.420734   -0.317329  -0.287929   \n",
              "412  -0.216670  -0.160814        -0.395698   -0.395063  -0.371277   \n",
              "413  -0.295154  -0.160814        -0.425709   -0.470558  -0.452752   \n",
              "414   0.365428  -0.160814        -0.372924    0.097561   0.151444   \n",
              "415  -0.201770  -0.160814        -0.333132   -0.287939  -0.255179   \n",
              "416  -0.205784  -0.160814        -0.349990   -0.316802  -0.286460   \n",
              "417   0.032225  -0.160814        -0.264848   -0.146297  -0.112411   \n",
              "418  -0.311306  -0.160814        -0.445056   -0.513683  -0.535537   \n",
              "419   0.017057  -0.160814        -0.266073   -0.150084  -0.115781   \n",
              "420   0.593331  -0.160814        -0.314121    0.366503   0.438772   \n",
              "421   1.089826  -0.160814        -0.320716    0.674439   0.759627   \n",
              "422  -0.350079  -0.160814        -0.451251   -0.533861  -0.539256   \n",
              "423   0.132071  -0.160814        -0.295659    0.031812   0.085065   \n",
              "424   0.372646  -0.160814        -0.237357   -0.061309  -0.036771   \n",
              "425  -0.323130  -0.160814        -0.446945   -0.519836  -0.536671   \n",
              "426  -0.237185  -0.160814        -0.409905   -0.426844  -0.415497   \n",
              "427  -0.324374  -0.160814        -0.435587   -0.496551  -0.480793   \n",
              "428  -0.296337  -0.160814        -0.376138   -0.390073  -0.365462   \n",
              "429  -0.245219  -0.160814        -0.363557   -0.355787  -0.349372   \n",
              "430  -0.134889  -0.160814        -0.421768   -0.313230  -0.283531   \n",
              "431   0.383004  -0.160814        -0.270589   -0.116131  -0.097087   \n",
              "432   1.106987  -0.160814        -0.324690    0.699844   0.787610   \n",
              "433   1.081106  -0.160814        -0.342463    0.708339   0.797396   \n",
              "434  -0.198045  -0.160814        -0.290020   -0.216335  -0.177545   \n",
              "435  -0.232266  -0.160814        -0.420583   -0.468771  -0.456632   \n",
              "436  -0.024821  -0.160814        -0.371782   -0.355818  -0.342457   \n",
              "437  -0.296347  -0.160814        -0.441344   -0.503160  -0.527698   \n",
              "438  -0.208587  -0.160814        -0.361759   -0.336953  -0.308299   \n",
              "439   0.889206  -0.160814        -0.274257    0.377438   0.432500   \n",
              "440  -0.259311  -0.160814        -0.421663   -0.455385  -0.457459   \n",
              "441  -0.290732  -0.160814        -0.429870   -0.482261  -0.503402   \n",
              "442  -0.102084  -0.160814        -0.436658   -0.279868  -0.251597   \n",
              "443   0.264829  -0.160814        -0.306320   -0.198959  -0.199171   \n",
              "444  -0.339759  -0.160814        -0.449602   -0.528490  -0.538266   \n",
              "445   0.075295  -0.160814        -0.386141   -0.150820  -0.112185   \n",
              "446   0.582261  -0.160814        -0.228964   -0.025265  -0.008326   \n",
              "447   0.864129  -0.160814        -0.324960    0.562994   0.646429   \n",
              "448  -0.192931  -0.160814        -0.296018   -0.224394  -0.186310   \n",
              "449  -0.383037  -0.160814        -0.456518   -0.551013  -0.542418   \n",
              "450   0.616200  -0.160814        -0.217689   -0.000504   0.017345   \n",
              "451  -0.382905  -0.160814        -0.457012   -0.548807  -0.537249   \n",
              "452  -0.379616  -0.160814        -0.454262   -0.545693  -0.533807   \n",
              "453   0.017955  -0.160814        -0.425755   -0.204144  -0.170740   \n",
              "454  -0.156061  -0.160814        -0.416062   -0.335844  -0.307798   \n",
              "455  -0.338716  -0.160814        -0.440435   -0.509309  -0.494556   \n",
              "456   0.611570  -0.160814        -0.218063   -0.001660   0.016316   \n",
              "457   0.317202  -0.160814        -0.291320   -0.169589  -0.154212   \n",
              "458   0.722550  -0.160814        -0.235663    0.130718   0.160753   \n",
              "459   0.500817  -0.160814        -0.248124   -0.069611  -0.053152   \n",
              "460   0.721759  -0.160814        -0.235480    0.129547   0.159463   \n",
              "461   0.285006  -0.160814        -0.244435   -0.083189  -0.056244   \n",
              "462  -0.234165  -0.160814        -0.321666   -0.280180  -0.246600   \n",
              "463  -0.268881  -0.160814        -0.352082   -0.341542  -0.312970   \n",
              "464  -0.226471  -0.160814        -0.404211   -0.413022  -0.395178   \n",
              "465  -0.238685  -0.160814        -0.422094   -0.472266  -0.460165   \n",
              "466  -0.204044  -0.160814        -0.342684   -0.304294  -0.272904   \n",
              "467  -0.349175  -0.160814        -0.443971   -0.518614  -0.504594   \n",
              "468   0.548248  -0.160814        -0.381345    0.239116   0.283511   \n",
              "469  -0.241061  -0.160814        -0.407423   -0.422438  -0.400841   \n",
              "\n",
              "     OnonCAsset2   surplus2  employee1   평균_당좌비율   당좌비율_증감  평균_총자본회전율  \\\n",
              "0      -0.256366   0.022217  -0.490424  0.632457  0.449676  -0.067667   \n",
              "1       3.705636   1.286116   3.432879 -0.432617 -0.637886  -0.055363   \n",
              "2       0.419844   1.537099   0.449602 -0.522229  0.476809  15.519711   \n",
              "3       0.105405   1.566155   0.547521 -0.378928 -0.705713  -0.068386   \n",
              "4      -0.396213   1.566741   2.297014 -0.409102 -0.709298  -0.041466   \n",
              "5      -0.409695   1.640450   1.520187 -0.513712  0.414152  -0.065557   \n",
              "6      -0.125335  -0.226129  -0.549176 -0.454163  0.188948  -0.062550   \n",
              "7      -0.147083  -0.226129  -0.542648 -0.511707 -1.334592  -0.071850   \n",
              "8       4.869975  -0.226129   0.469185 -0.494970 -0.066297  -0.046784   \n",
              "9       8.804797  -0.226129   5.312931 -0.431466 -0.284875   0.207476   \n",
              "10      1.326864  -0.226129   0.299459 -0.505456 -0.097423  -0.059510   \n",
              "11      2.201624  -0.123590   1.774777 -0.249669 -0.312353  -0.070796   \n",
              "12      1.752299  -0.226129   0.371266 -0.486769  1.418992  -0.056490   \n",
              "13      2.831924  -0.029189   1.422267 -0.506387 -0.603727  -0.060665   \n",
              "14      0.405878   1.977729   3.060785 -0.468109  1.321384  -0.066483   \n",
              "15     -0.409695   0.949849  -0.131387 -0.509642 -0.594799  -0.059896   \n",
              "16     -0.409210  -0.226129   0.175427 -0.410905 -1.383260  -0.069470   \n",
              "17     -0.149095  -0.226129   0.841279 -0.493273  0.234345  -0.051043   \n",
              "18     -0.375459  -0.139464   0.123204 -0.462550 -1.462570  -0.002578   \n",
              "19     -0.164004  -0.226129   0.691136 -0.527826  0.257833  -0.142841   \n",
              "20     -0.254570   0.568857   0.070980 -0.546373 -0.623826  -0.064227   \n",
              "21      0.541394  -0.226123   1.833529 -0.462856 -0.254803  -0.051305   \n",
              "22     -0.409695  -0.226129   0.730304 -0.540687  0.561391  -0.065686   \n",
              "23     -0.284800  -0.226129  -0.288058 -0.490049 -0.296517  -0.062978   \n",
              "24     -0.043888  -0.226129  -0.281530 -0.538360  0.241465  -0.064051   \n",
              "25     -0.184915  -0.226129   1.213373  0.112565 -0.010902  -0.070007   \n",
              "26     -0.128790   1.044729  -0.000828 -0.454665  0.245298  -0.070424   \n",
              "27     -0.407530  -0.226129  -0.307642 -0.412050 -1.068820  -0.057893   \n",
              "28     -0.013777  -0.226129  -0.248890 -0.479855 -0.329874  -0.065517   \n",
              "29      4.529687  -0.217690   2.068535 -0.118096 -0.553970  -0.056194   \n",
              "30      0.573963  -0.226129   0.443074 -0.410695 -0.537122  -0.068750   \n",
              "31      2.980070  -0.226129   0.475713 -0.238592  0.726185  -0.068580   \n",
              "32     -0.104926  -0.226129  -0.039995 -0.443670  0.201484  -0.063848   \n",
              "33     -0.110444  -0.226129  -0.066107 -0.192604 -1.529325  -0.068656   \n",
              "34     -0.310947  -0.226129  -0.111803 -0.507230 -0.590689  -0.051317   \n",
              "35     -0.409695  -0.226129   1.030590 -0.434936  0.480001  -0.062497   \n",
              "36      0.025336  -0.018080  -0.333753 -0.440676 -1.058849  -0.069153   \n",
              "37      0.474590   0.689061   1.063230 -0.446779 -0.077613  -0.053631   \n",
              "38     -0.270825  -0.120205  -0.418617 -0.523084 -0.472010  -0.057431   \n",
              "39     -0.248637  -0.325163  -0.118331 -0.465602 -0.042928  -0.068380   \n",
              "40     -0.386018  -0.226129  -0.412089 -0.503594  0.098315  -0.068223   \n",
              "41     -0.396401  -0.226129   0.834751 -0.455017 -1.048649  -0.064993   \n",
              "42     -0.236292  -0.226129  -0.562232 -0.521826  2.863590  -0.060671   \n",
              "43     -0.268468   0.189388   0.162371 -0.458991  0.312556  -0.065414   \n",
              "44      1.552925  -0.125760  -0.405561 -0.398565 -0.324982  -0.057635   \n",
              "45     -0.371452  -0.226129  -0.190138 -0.450524 -0.590307  -0.065978   \n",
              "46     -0.409695  -0.226129   0.867391 -0.384496 -0.610080  -0.059906   \n",
              "47     -0.010259  -0.226129  -0.607928 -0.434399 -0.427966  -0.070499   \n",
              "48     -0.252056  -0.226129  -0.425145 -0.501046  1.531851  -0.065623   \n",
              "49      0.724769  -0.226129  -0.053051 -0.174085 -0.301965  -0.070095   \n",
              "50     -0.400018   0.157408  -0.457785 -0.533740 -0.119582  -0.068382   \n",
              "51     -0.223633   0.480144   2.414517 -0.420260 -0.161404  -0.045229   \n",
              "52     -0.345443  -0.175479  -0.634039 -0.533926 -0.552660  -0.067936   \n",
              "53     -0.409695  -0.226129   0.358210 -0.417080 -0.351546  -0.057260   \n",
              "54      0.021717  -0.226129  -0.340281 -0.383710  0.018406  -0.068126   \n",
              "55      0.021717  -0.226129  -0.340281 -0.383710  0.018406  -0.068126   \n",
              "56     -0.409170   0.038640  -0.366393 -0.475119  0.570027  -0.069066   \n",
              "57      0.079728  -0.226129  -0.196666 -0.354216  0.758812  -0.064838   \n",
              "58     -0.392390  -0.249123  -0.666679 -0.545227 -1.183305  -0.072072   \n",
              "59      3.409404   0.366563   0.554049  0.184936 -1.511974  -0.069688   \n",
              "60     -0.409695   0.484356   0.769472 -0.451983  0.073614  -0.060851   \n",
              "61     -0.327513  -0.226129  -0.333753 -0.370075 -0.397816  -0.057024   \n",
              "62     -0.283584   0.079540  -0.431673 -0.269778 -0.042078  -0.069133   \n",
              "63     -0.372913  -0.162903  -0.627512 -0.329921  0.160586  -0.070317   \n",
              "64      0.000272   0.365993   0.181955  0.248884 -1.339422  -0.068010   \n",
              "65     -0.296559  -0.201933  -0.372921 -0.460852  0.125901  -0.059048   \n",
              "66     -0.388510  -0.226129  -0.477368 -0.235645  0.018168  -0.068245   \n",
              "67     -0.355047  -0.226129  -0.568760 -0.464866  0.275390  -0.242996   \n",
              "68     -0.361127  -0.226129  -0.503480 -0.329851  0.148004  -0.070572   \n",
              "69     -0.401077  -0.226129  -0.549176 -0.462386 -0.182147  -0.066510   \n",
              "70      2.851344  -0.226129  -0.072635 -0.306205  0.628775  -0.054820   \n",
              "71     -0.398266  -0.226129  -0.490424 -0.492245 -0.384453  -0.052355   \n",
              "72     -0.404125  -0.226129  -0.457785 -0.469206  0.032367  -0.069994   \n",
              "73      0.029251  -0.226129  -0.718903 -0.353315  0.052126  -0.069228   \n",
              "74     -0.409695  -0.226129  -0.075977  0.786942 -1.536081  -0.068279   \n",
              "75     -0.409695  -0.226129  -0.490424 -0.370887  0.223990  -0.065609   \n",
              "76     -0.394551  -0.226129  -0.542648 -0.478470  0.273724  -0.069445   \n",
              "77     -0.228494  -0.226129  -0.594872 -0.024003 -0.129328  -0.066385   \n",
              "78     -0.087060  -0.226129   0.175427 -0.385367  0.029302  -0.061311   \n",
              "79     -0.118588  -0.226129  -0.620984 -0.187483 -1.190629  -0.069046   \n",
              "80     -0.066760  -0.226129   1.755193 -0.497993  0.733304  -0.069956   \n",
              "81     -0.326089  -0.226129  -0.594872 -0.496498  0.405680  -0.067604   \n",
              "82     -0.397084  -0.226129  -0.555704 -0.377177  0.238201  -0.070345   \n",
              "83     -0.370300  -0.214285  -0.418617 -0.491868 -0.136255   0.023553   \n",
              "84     -0.384473  -0.184118  -0.392505 -0.142783 -0.136210  -0.065854   \n",
              "85     -0.250958  -0.226129   0.638912 -0.304773  0.041511  -0.064538   \n",
              "86     -0.281564  -0.226129  -0.307642 -0.438989  0.007664  -0.046047   \n",
              "87     -0.221768   0.905793   0.410434 -0.485841 -0.371266  -0.066609   \n",
              "88     -0.409695  -0.226129  -0.425145 -0.427152  0.217670  -0.067753   \n",
              "89     -0.409695  -0.115346  -0.869046 -0.123404 -1.095007  -0.067937   \n",
              "90      0.768919  -0.226129   0.097092 -0.224262 -0.300922  -0.035063   \n",
              "91     -0.409695  -0.226129   0.488769 -0.148193  1.247366  -0.061772   \n",
              "92     -0.398217  -0.226129  -0.581816 -0.431934 -0.178348  -0.046988   \n",
              "93     -0.398303   0.011306  -0.346809 -0.428105 -0.254751  -0.058510   \n",
              "94      0.330893   0.479765  -0.392505  0.624900 -1.673237  -0.067733   \n",
              "95      0.237150  -0.226129  -0.477368 -0.404658 -0.034336  -0.065367   \n",
              "96     -0.409380  -0.226129  -0.157499 -0.518765 -0.188275  -0.061281   \n",
              "97      0.694640  -0.226129  -0.327225 -0.395439 -1.028085  -0.053582   \n",
              "98     -0.383926  -0.226129  -0.470841 -0.148205  0.895620  -0.068884   \n",
              "99     -0.393732  -0.226129  -0.666679 -0.547642 -0.073078  -0.066006   \n",
              "100    -0.393732  -0.226129  -0.666679 -0.547642 -0.073078  -0.066006   \n",
              "101    -0.409695  -0.226129  -0.327225 -0.031749 -0.140671  -0.066962   \n",
              "102    -0.409695  -0.226129  -0.079163 -0.430446 -1.273218  -0.055434   \n",
              "103    -0.327723  -0.226129  -0.542648 -0.413470  0.275554  -0.065647   \n",
              "104    -0.134857  -0.226129  -0.209722 -0.379365  0.247184  -0.069204   \n",
              "105    -0.079262  -0.226129   0.214595 -0.336033  0.801569  -0.064302   \n",
              "106    -0.294713  -0.226129  -0.640567 -0.391967  0.252909  -0.067006   \n",
              "107    -0.354669  -0.226129  -0.568760 -0.211680 -0.109041  -0.063751   \n",
              "108    -0.224675  -0.226129   0.018756 -0.030095  0.206907  -0.062984   \n",
              "109    -0.026054  -0.226129  -0.457785 -0.347043 -0.170447  -0.049054   \n",
              "110     0.105785  -0.226129  -0.405561 -0.264007  0.036557  -0.070546   \n",
              "111     0.094446  -0.226129  -0.392505 -0.507965  0.046636  -0.056609   \n",
              "112    -0.409695  -0.226129  -0.555704 -0.424262 -0.296179  -0.075110   \n",
              "113     0.312213  -0.226129  -0.275002 -0.314674  0.095424  -0.068608   \n",
              "114    -0.409695  -0.226129   0.488769 -0.454971 -0.314102  -0.057660   \n",
              "115    -0.403951  -0.226129  -0.079163 -0.446780 -0.156499  -0.101318   \n",
              "116    -0.408439  -0.009519  -0.425145 -0.167150 -0.764639  -0.065683   \n",
              "117    -0.378278  -0.219265  -0.764599 -0.267995 -0.384059  -0.066069   \n",
              "118    -0.409695  -0.195915  -0.686263 -0.338593  0.030980  -0.068232   \n",
              "119    -0.409149  -0.176514  -0.503480 -0.193423 -0.330811  -0.057175   \n",
              "120    -0.409695  -0.226129  -0.177082 -0.341628  0.255335  -0.010590   \n",
              "121    -0.364505  -0.226129  -0.314170 -0.139492  6.421634  -0.067631   \n",
              "122    -0.388677  -0.202056  -0.653623 -0.206720 -1.134761  -0.068909   \n",
              "123    -0.378932  -0.136669  -0.575288 -0.390824 -1.470388  -0.068226   \n",
              "124    -0.305023  -0.206310  -0.320697 -0.200102  2.505661  -0.069182   \n",
              "125    -0.382522  -0.085346  -0.275002 -0.086720 -0.181245  -0.067622   \n",
              "126    -0.385466  -0.226129  -0.385977 -0.332493  2.595451  -0.064010   \n",
              "127    -0.388623  -0.226129  -0.568760  0.245472  1.022380  -0.069424   \n",
              "128    -0.373638  -0.226129  -0.438201 -0.081505  0.565057  -0.065871   \n",
              "129    -0.138893  -0.204446  -0.503480  0.686920  3.588337  -0.068538   \n",
              "130    -0.409695   0.326728  -0.457785 -0.239264  0.138034  -0.068730   \n",
              "131    -0.304841  -0.226129  -0.255418 -0.094261  0.708003  -0.070082   \n",
              "132     0.773017  -0.226129   1.507131 -0.362954  2.224427  -0.044775   \n",
              "133    -0.404440  -0.226129  -0.686263 -0.462386  0.031472  -0.070768   \n",
              "134    -0.391829  -0.226129  -0.477368 -0.478412 -0.506024  -0.070412   \n",
              "135    -0.378167  -0.226129  -0.895158  0.147759 -1.518663  -0.070382   \n",
              "136    -0.395552   0.028107   0.018756  0.175281 -0.209665  -0.067377   \n",
              "137    -0.302001  -0.226129  -0.562232 -0.247744  2.426522  -0.067033   \n",
              "138    -0.407593  -0.226129  -0.803766  0.393401 -0.399438  -0.068068   \n",
              "139    -0.394635  -0.107192  -0.451257 -0.331092  0.997808  -0.062432   \n",
              "140    -0.405491  -0.170175  -0.549176 -0.479970  0.614238  -0.061359   \n",
              "141    -0.409695  -0.226129  -0.418617 -0.196336 -0.045196  -0.053186   \n",
              "142    -0.400843   0.052319  -0.418617  0.026205 -0.143200  -0.067481   \n",
              "143    -0.409695  -0.207793  -0.379449 -0.053748 -1.061363  -0.070971   \n",
              "144    -0.310540  -0.226129  -0.399033  0.091352 -1.080945  -0.068120   \n",
              "145    -0.394687  -0.226129  -0.758071  0.966840  1.610789  -0.071460   \n",
              "146    -0.051062  -0.226129  -0.836406  1.303784 -0.045248  -0.071208   \n",
              "147    -0.389727  -0.226129  -0.542648 -0.406772  1.673247  -0.058417   \n",
              "148    -0.409695  -0.226129  -0.438201 -0.171652 -1.697962  -0.076339   \n",
              "149    -0.404966  -0.226129  -0.601400  4.105155 -0.637867  -0.069647   \n",
              "150    -0.399186  -0.226129  -0.888630 -0.009730 -0.163830  -0.070329   \n",
              "151     0.787269  -0.226129   0.769472 -0.132921 -0.204258  -0.064800   \n",
              "152    -0.002919  -0.226129   1.337404  0.063039 -0.635045  -0.070218   \n",
              "153    -0.002919  -0.226129   1.337404  0.063039 -0.635045  -0.070218   \n",
              "154    -0.368254  -0.226129  -0.764599 -0.039095 -0.588557  -0.069478   \n",
              "155    -0.282508  -0.226129  -0.379449  0.123421 -0.022658  -0.063328   \n",
              "156    -0.383422  -0.226129  -0.418617 -0.423583  0.491843  -0.068154   \n",
              "157    -0.409695  -0.226129  -0.399033 -0.072348  0.334697  -0.068944   \n",
              "158     1.004179   2.737997   1.794361 -0.155613 -0.336690  -0.066396   \n",
              "159     2.201172  -0.226129   0.456130 -0.334393 -0.079938  -0.078491   \n",
              "160    -0.409695   1.210093  -0.196666  2.339952  0.193029  -0.071526   \n",
              "161     0.893050  -0.226129   1.716026  0.575136  0.017267  -0.040925   \n",
              "162     1.520909  -0.226129   0.469185 -0.443667  0.826696  -0.067178   \n",
              "163    -0.403600   0.043469  -0.464313  0.172148 -0.106414  -0.065299   \n",
              "164    -0.409695  -0.319072  -0.385977 -0.488460 -0.101919  -0.070444   \n",
              "165    -0.343339   0.204780  -0.105275  0.367275 -0.993319  -0.067899   \n",
              "166    -0.383057  -0.226129  -0.470841 -0.191332 -0.120329  -0.065568   \n",
              "167    -0.270899  -0.235550  -0.849462 -0.426328 -0.529659  -0.068511   \n",
              "168    -0.387970  -0.226129  -0.797238 -0.416033  0.585103  -0.063688   \n",
              "169    -0.384729  -0.226129  -0.614456 -0.078337  0.059638  -0.070829   \n",
              "170    -0.379744  -0.226129  -0.503480  0.857472  0.314893  -0.070847   \n",
              "171    -0.380967  -0.747256  -0.529592 -0.385239 -0.073212  -0.035597   \n",
              "172    -0.408938  -0.269130   0.723776 -0.391182 -0.666195  -0.065135   \n",
              "173    -0.409695  -0.226129  -0.634039 -0.480022 -0.721641  -0.065662   \n",
              "174    -0.340255  -0.769522  -0.731959 -0.082648  1.811375  -0.075882   \n",
              "175    -0.366666  -0.226129  -0.372921 -0.448385  3.169109  -0.064782   \n",
              "176    -0.367658  -0.226129  -0.620984 -0.284457 -0.688607  -0.071552   \n",
              "177    -0.388677  -0.226129  -0.431673  0.565601  0.204993  -0.071150   \n",
              "178    -0.401621  -0.201482  -0.464313 -0.254440  0.746064  -0.068090   \n",
              "179    -0.061633  -0.226129   0.305987  1.199177  3.245778  -0.064383   \n",
              "180    -0.343325  -0.226129  -0.372921  0.607937  0.323982  -0.067850   \n",
              "181    -0.083471  -0.226129  -0.261147 -0.189332 -0.027353  -0.067785   \n",
              "182    -0.409695  -0.226129  -0.790710 -0.491644  0.671541  -0.070974   \n",
              "183    -0.409695  -0.226129  -0.790710 -0.491644  0.671541  -0.070974   \n",
              "184    -0.199289  -0.226129  -0.229306  0.229764  1.239398  -0.066841   \n",
              "185    -0.231304  -0.020937  -0.242362  0.914368 -0.141090  -0.066560   \n",
              "186    -0.400237  -0.226129  -0.470841  1.167820 -0.820172  -0.068374   \n",
              "187    -0.409695  -0.209188  -0.640567  0.259361 -0.746780  -0.070594   \n",
              "188    -0.409695   0.331557  -0.444729  1.988421 -1.308120  -0.066318   \n",
              "189    -0.392747  -0.151634  -0.542648  8.746198  2.968160  -0.069649   \n",
              "190    -0.323051  -0.226129  -0.483896  0.875320  0.627362  -0.069031   \n",
              "191    -0.409695   0.158504  -0.320697  0.344403 -0.787415  -0.067604   \n",
              "192     0.871140  13.161333   1.885752  0.885388  0.185527  -0.069541   \n",
              "193    -0.103885  -0.226129  -0.869046 -0.557094 -0.108802  -0.072921   \n",
              "194    -0.407593  -0.226129  -0.686263 -0.322553 -0.849232  -0.071454   \n",
              "195    -0.407330  -0.226129  -0.594872  0.443992  0.775344  -0.068392   \n",
              "196     2.220617  -0.226129   1.108925 -0.417984  0.024669  -0.057730   \n",
              "197     0.115768   0.029894   0.364738  0.062879 -0.609591  -0.064281   \n",
              "198    -0.408561  -0.226129  -0.490424  1.318598  2.541730  -0.069351   \n",
              "199    -0.067450   0.557026   0.423490 -0.393465 -0.208861  -0.065049   \n",
              "200    -0.368709  -0.226129   0.031812 -0.036542  2.180510  -0.067641   \n",
              "201    -0.125945  -0.226129  -0.823350  3.751035 -0.434930  -0.071687   \n",
              "202    -0.125945  -0.226129  -0.502887  3.751035 -0.434930  -0.071687   \n",
              "203     1.331485  -0.226129  -0.092219  2.943313  0.131261  -0.069749   \n",
              "204    -0.257605  -0.226129   0.606273 -0.177946  0.175389  -0.063641   \n",
              "205     3.241897  -0.121218   1.944504 -0.028332 -0.159250  -0.061809   \n",
              "206    -0.357149  -0.226129  -0.581816  0.046027 -0.228823  -0.067120   \n",
              "207    -0.409695  -0.226129   6.938392 -0.246101  0.155948  -0.069005   \n",
              "208     0.073970  -0.226129  -0.614456  0.371310 -0.564675  -0.059238   \n",
              "209    -0.353786   1.250708  -0.418617 -0.226424 -0.213890  -0.070892   \n",
              "210    -0.005549  -0.204775   2.858418  0.072536 -0.106745  -0.067964   \n",
              "211    -0.384959  -0.226129  -0.607928  0.113556  0.099509  -0.071537   \n",
              "212    -0.399081  -0.019423  -0.620984 -0.242531 -1.095498  -0.070941   \n",
              "213    -0.378167  -0.226129  -0.836406  3.370865 -1.687754  -0.061480   \n",
              "214    -0.370079  -0.226129  -0.568760  0.154214 -0.426327  -0.068224   \n",
              "215    -0.201353  -0.120551   0.260291  3.439268 -0.946728  -0.054344   \n",
              "216    -0.303867  -0.181939  -0.075977 -0.063677  0.896990  -0.063724   \n",
              "217    -0.403347  -0.226129   0.103620  0.957877 -0.252940  -0.069541   \n",
              "218     1.782386   4.994478   1.539771  0.995481  0.142082  -0.068094   \n",
              "219    -0.409695  -0.199260  -0.699319  1.064861 -1.476203  -0.073931   \n",
              "220    -0.386575  -0.226129  -0.333753 -0.521589 -0.600067  -0.045166   \n",
              "221    -0.398436  -0.226129  -0.523064  2.689198 -1.262565  -0.069370   \n",
              "222    -0.390934  -0.226129  -0.594872 -0.122700 -0.942236  -0.067309   \n",
              "223     0.100540   0.693487   1.951032 -0.225342 -0.491792  -0.065118   \n",
              "224     0.100540   0.693487   1.951032 -0.225342 -0.491792  -0.065118   \n",
              "225    -0.409695  -0.226129  -0.627512 -0.027630 -0.775932  -0.067982   \n",
              "226    -0.372045  -0.226129  -0.581816 -0.476268 -0.745085  -0.074851   \n",
              "227    -0.351132  -0.226129  -0.346809 -0.241468  1.936501  -0.069503   \n",
              "228    -0.368411  -0.226129  -0.405561 -0.434739 -0.636616  -0.064245   \n",
              "229    -0.407523  -0.226129  -0.503480 -0.397677 -0.683731  -0.062139   \n",
              "230    -0.391514  -0.226129  -0.562232  5.180161 -0.538214  -0.070120   \n",
              "231    -0.409695  -0.226129  -0.784183  0.364460  0.507802  -0.069750   \n",
              "232    -0.388124  -0.226129  -0.660151 -0.503720 -0.862016  -0.061212   \n",
              "233    -0.344533  -0.226129  -0.075977  0.021466 -0.223969  -0.066651   \n",
              "234    -0.349687  -0.226129  -0.235834 -0.195802 -0.509256  -0.069210   \n",
              "235    -0.368919   0.139917   0.279875 -0.470513 -0.666217  -0.057628   \n",
              "236    -0.409504  -0.130109   4.372905  0.371772 -0.950464  -0.064066   \n",
              "237    -0.365013  -0.226129  -0.405561 -0.539233  1.525804  -0.067994   \n",
              "238    -0.344485  -0.226129   0.279875 -0.087410 -1.560450  -0.068782   \n",
              "239    -0.409695  -0.226129  -0.490424  0.561579 -0.253529  -0.068755   \n",
              "240    -0.328924  -0.067567   0.364738 -0.181250  1.334954  -0.068572   \n",
              "241    -0.318337   0.998666  -0.797238 -0.523936  0.430761  -0.072082   \n",
              "242    -0.235767  -0.226129  -0.367938 -0.332107 -0.263765  -0.069083   \n",
              "243    -0.336954  -0.226129   0.093742 -0.269265  0.096115  -0.021253   \n",
              "244    -0.409695  -0.226129  -0.337904  0.318380 -0.727056  -0.069267   \n",
              "245    -0.292335  -0.226129  -0.013096 -0.339893 -0.539031  -0.070363   \n",
              "246    -0.111448  -0.226129   0.830973 -0.088059 -0.599041  -0.070273   \n",
              "247     0.004670  -0.226129   0.820575 -0.074587 -0.433743  -0.069574   \n",
              "248    -0.409695  -0.226129  -0.122229  0.270666 -0.716580  -0.041888   \n",
              "249    -0.409695  -0.226129  -0.125638  0.232604 -0.656162  -0.039943   \n",
              "250    -0.261764  -0.226129  -0.016785 -0.289924  0.196378  -0.070699   \n",
              "251    -0.104801  -0.226129   0.804393 -0.075888 -0.307795  -0.070408   \n",
              "252    -0.232760  -0.226129  -0.605976 -0.447377  0.403671  -0.069806   \n",
              "253    -0.401109  -0.226129  -0.046144  0.771662 -1.517062  -0.068320   \n",
              "254    -0.188269  -0.226129  -0.211633  0.186096 -0.738228  -0.068200   \n",
              "255    -0.371498  -0.226129  -0.590878 -0.439559  0.548851  -0.070903   \n",
              "256    -0.225181  -0.226129  -0.491187 -0.320655  0.276243  -0.069170   \n",
              "257    -0.129311  -0.226129  -0.247754  0.026110 -0.525786  -0.068180   \n",
              "258    -0.253806  -0.226129   0.166688 -0.286253 -0.551813  -0.070344   \n",
              "259    -0.409695  -0.226129  -0.312608  0.363633 -0.805190  -0.069171   \n",
              "260    -0.036883  -0.226129  -0.318114 -0.377994  0.050589  -0.060311   \n",
              "261    -0.096553  -0.226129   1.012065  0.229670 -0.842450  -0.069772   \n",
              "262    -0.043130  -0.226129   0.539418 -0.062943 -0.331689  -0.069004   \n",
              "263    -0.023303  -0.226129  -0.306412 -0.300517 -0.001179  -0.067980   \n",
              "264    -0.334192  -0.226129  -0.118834  0.560987 -1.186892  -0.068165   \n",
              "265    -0.392608  -0.226129  -0.459866 -0.423238 -0.550938  -0.070319   \n",
              "266    -0.005136  -0.226129   1.329151  0.060833 -0.630193  -0.069893   \n",
              "267    -0.181023  -0.226129  -0.551958 -0.434433  0.325343  -0.069464   \n",
              "268    -0.289089  -0.226129  -0.149866  0.459674 -1.101508  -0.068236   \n",
              "269    -0.346519  -0.226129  -0.265937 -0.415330 -0.521055  -0.070390   \n",
              "270    -0.105541  -0.226129   0.980835  0.245666 -0.862360  -0.069729   \n",
              "271    -0.408857  -0.226129  -0.776006 -0.491023  0.616280  -0.070948   \n",
              "272     0.013569  -0.226129   0.214614 -0.235948 -0.197723  -0.068818   \n",
              "273    -0.047445  -0.226129   0.453781 -0.076463 -0.299134  -0.068873   \n",
              "274    -0.409695  -0.226129  -0.528257 -0.427481  0.493527  -0.045147   \n",
              "275    -0.010920  -0.226129   1.309605  0.077277 -0.652767  -0.070180   \n",
              "276    -0.077402  -0.226129  -0.140703 -0.170317 -0.073141  -0.067968   \n",
              "277    -0.110164  -0.226129  -0.259484 -0.025845 -0.456796  -0.068173   \n",
              "278    -0.332960  -0.226129  -0.436090 -0.423224 -0.414640  -0.069911   \n",
              "279    -0.171054  -0.226129  -0.211434  0.072773 -0.432409  -0.067918   \n",
              "280    -0.200339  -0.226129  -0.204238  0.218848 -0.781720  -0.068205   \n",
              "281    -0.409695  -0.226129  -0.342349  0.310430 -0.713329  -0.069283   \n",
              "282    -0.280937  -0.226129  -0.581696 -0.372324  0.395694  -0.069715   \n",
              "283    -0.409695  -0.226129  -0.695115 -0.468273  0.606702  -0.061567   \n",
              "284    -0.007224  -0.226129  -0.349875 -0.390337 -0.018295  -0.068286   \n",
              "285     0.019982  -0.226129  -0.222145 -0.352252 -0.027607  -0.068273   \n",
              "286    -0.068413  -0.226129  -0.285063 -0.139139 -0.306355  -0.068158   \n",
              "287    -0.397705  -0.226129  -0.345368 -0.062292 -0.844764  -0.069711   \n",
              "288     0.012881  -0.226129   0.261415 -0.223485 -0.215952  -0.068876   \n",
              "289    -0.161844  -0.226129  -0.316103 -0.262806 -0.149014  -0.068453   \n",
              "290    -0.397105  -0.226129  -0.569895 -0.482320 -0.158300  -0.070578   \n",
              "291    -0.057489  -0.226129   0.254467 -0.107930 -0.223365  -0.068570   \n",
              "292    -0.064477  -0.226129   1.015356 -0.020902 -0.437319  -0.070333   \n",
              "293    -0.365126  -0.226129  -0.101275  0.653562 -1.329956  -0.068212   \n",
              "294    -0.409695  -0.226129  -0.612090 -0.447976  0.550388  -0.053397   \n",
              "295    -0.399819  -0.226129  -0.617489 -0.484329  0.020560  -0.070663   \n",
              "296    -0.289082  -0.226129   0.271978 -0.221640 -0.008672  -0.028270   \n",
              "297    -0.409695  -0.226129  -0.376008  0.250216 -0.609363  -0.069410   \n",
              "298    -0.409695  -0.226129  -0.334298 -0.380063  0.361970  -0.026060   \n",
              "299     0.012146  -0.226129   0.311476 -0.210154 -0.235451  -0.068939   \n",
              "300    -0.034652  -0.226129   1.189329  0.018859 -0.624518  -0.070234   \n",
              "301    -0.391922  -0.226129  -0.475287 -0.471852 -0.511364  -0.070401   \n",
              "302    -0.403047  -0.226129  -0.674114 -0.486720  0.233363  -0.070765   \n",
              "303    -0.081945  -0.226129   0.923967 -0.044722 -0.381210  -0.070365   \n",
              "304    -0.077757  -0.226129  -0.147745 -0.171429 -0.070463  -0.067958   \n",
              "305    -0.034881  -0.226129   0.703125 -0.037098 -0.393923  -0.069253   \n",
              "306    -0.099126  -0.226129  -0.272125 -0.204008 -0.051655  -0.067919   \n",
              "307    -0.049040  -0.226129   1.165687  0.017156 -0.534092  -0.063457   \n",
              "308     0.010296  -0.226129  -0.344067 -0.386325  0.003923  -0.068189   \n",
              "309    -0.049548  -0.226129   0.412059 -0.083050 -0.283274  -0.068810   \n",
              "310    -0.220532  -0.226129  -0.191867  0.273641 -0.854478  -0.068212   \n",
              "311    -0.136331  -0.226129   0.873852  0.300460 -0.930561  -0.069582   \n",
              "312    -0.136404  -0.226129   0.639058 -0.118982 -0.206286  -0.070466   \n",
              "313    -0.042400  -0.226129  -0.361536 -0.398393 -0.062902  -0.068481   \n",
              "314    -0.409695  -0.226129  -0.777732 -0.468426  0.631453  -0.070925   \n",
              "315    -0.344177  -0.226129  -0.447943 -0.402303  0.461094  -0.070852   \n",
              "316    -0.409695  -0.226129  -0.090091  0.629399 -1.286007  -0.060226   \n",
              "317    -0.288339  -0.226129  -0.443062 -0.454713 -0.374784  -0.069840   \n",
              "318    -0.409695  -0.226129  -0.103733  0.737288 -1.450348  -0.068384   \n",
              "319    -0.310763  -0.226129  -0.630112 -0.399963  0.459591  -0.070007   \n",
              "320    -0.234486  -0.226129  -0.506291 -0.329277  0.296177  -0.069261   \n",
              "321    -0.409695  -0.226129  -0.557242 -0.073992 -0.049581  -0.070094   \n",
              "322    -0.065365  -0.226129  -0.274769 -0.222790 -0.019477  -0.067844   \n",
              "323    -0.172055  -0.226129  -0.210866  0.075768 -0.437037  -0.067919   \n",
              "324     0.006726  -0.226129  -0.331097 -0.343032 -0.035610  -0.068131   \n",
              "325    -0.223691  -0.226129   0.570313  0.455926 -1.124069  -0.069166   \n",
              "326    -0.409695  -0.226129  -0.677799 -0.289657  0.322788  -0.070548   \n",
              "327    -0.325837  -0.226129  -0.208805 -0.349808  0.209281  -0.021774   \n",
              "328    -0.242366  -0.226129   0.084698 -0.263473  0.134072  -0.070663   \n",
              "329    -0.409695  -0.226129  -0.290746  0.402742 -0.872717  -0.069089   \n",
              "330     0.000489  -0.226129   1.105291  0.001230 -0.544638  -0.069929   \n",
              "331    -0.056917  -0.226129   1.054906 -0.010593 -0.461602  -0.070319   \n",
              "332    -0.093409  -0.226129  -0.277280 -0.198541 -0.006063  -0.067882   \n",
              "333    -0.293599  -0.226129   0.327409  0.580336 -1.278921  -0.068832   \n",
              "334    -0.079151  -0.226129  -0.264397 -0.197315 -0.025474  -0.067799   \n",
              "335    -0.279590  -0.226129  -0.579510 -0.371076  0.392807  -0.069702   \n",
              "336    -0.157564  -0.226129  -0.399711 -0.424765 -0.208945  -0.069117   \n",
              "337    -0.166970  -0.226129  -0.396693 -0.266711  0.151534  -0.068601   \n",
              "338    -0.409695  -0.226129  -0.331017 -0.379261  0.359745  -0.025738   \n",
              "339    -0.057807  -0.226129  -0.423311 -0.403606  0.138801  -0.068651   \n",
              "340    -0.011728  -0.226129  -0.351368 -0.391369 -0.024006  -0.068311   \n",
              "341    -0.023336  -0.226129  -0.306387 -0.300455 -0.001193  -0.067980   \n",
              "342    -0.028011  -0.226129   0.839455 -0.015575 -0.445749  -0.069460   \n",
              "343    -0.409695  -0.226129  -0.212792 -0.350358  0.279556  -0.014104   \n",
              "344     0.012799  -0.226129  -0.333572 -0.367229  0.014526  -0.068097   \n",
              "345    -0.229757  -0.226129   0.549235  0.466722 -1.137506  -0.069137   \n",
              "346    -0.312155  -0.226129   0.186072 -0.244594  0.041833  -0.024888   \n",
              "347    -0.371510  -0.226129  -0.034913 -0.303641  0.171752  -0.016187   \n",
              "348    -0.068741  -0.226129   0.031173 -0.143182 -0.138479  -0.068230   \n",
              "349    -0.392997  -0.226129  -0.457734 -0.469469 -0.456241  -0.066501   \n",
              "350    -0.134337  -0.226129  -0.503215 -0.422753  0.254664  -0.069156   \n",
              "351    -0.186786  -0.226129  -0.557975 -0.435875  0.334069  -0.069502   \n",
              "352     0.005528  -0.226129   0.762145 -0.090146 -0.410984  -0.069501   \n",
              "353    -0.225598  -0.226129  -0.491864 -0.321041  0.277136  -0.069174   \n",
              "354     0.015066  -0.226129   0.112628 -0.263105 -0.158000  -0.068691   \n",
              "355    -0.390285  -0.226129  -0.104817 -0.322319  0.212850  -0.013435   \n",
              "356    -0.258013  -0.226129  -0.383537 -0.352962 -0.298298  -0.069272   \n",
              "357    -0.406861  -0.226129  -0.224722 -0.363328  0.134548  -0.020080   \n",
              "358    -0.357855  -0.226129  -0.736585 -0.478674  0.593057  -0.070632   \n",
              "359    -0.262204  -0.226129  -0.166337  0.386720 -1.004633  -0.068227   \n",
              "360    -0.040261  -0.226129   1.207657  0.129492 -0.717760  -0.070040   \n",
              "361    -0.114187  -0.226129  -0.243712 -0.097409 -0.169410  -0.067832   \n",
              "362    -0.112630  -0.226129  -0.257973 -0.019153 -0.465682  -0.068174   \n",
              "363    -0.392195  -0.226129  -0.469144 -0.452486 -0.527129  -0.070369   \n",
              "364    -0.140668  -0.226129  -0.228681 -0.018162 -0.291879  -0.067872   \n",
              "365    -0.089069  -0.226129  -0.257970 -0.172578 -0.053245  -0.067794   \n",
              "366    -0.156439  -0.226129  -0.312313 -0.257739 -0.140624  -0.068407   \n",
              "367    -0.346705  -0.226129  -0.688459 -0.433271  0.536594  -0.070358   \n",
              "368    -0.409695  -0.226129  -0.142069  0.049201 -0.365040  -0.030568   \n",
              "369    -0.235059  -0.226129  -0.175103  0.264317 -0.728419  -0.068015   \n",
              "370    -0.055260  -0.226129   0.298707 -0.100945 -0.240183  -0.068637   \n",
              "371    -0.409695  -0.226129  -0.756361 -0.430196  0.565445  -0.070844   \n",
              "372    -0.370799  -0.226129  -0.470397 -0.473596 -0.479354  -0.070296   \n",
              "373     0.009797  -0.226129  -0.331314 -0.361683  0.013221  -0.068087   \n",
              "374    -0.405034  -0.226129  -0.255425 -0.377314  0.056701  -0.026197   \n",
              "375     0.015115  -0.226129  -0.347175 -0.385362  0.028402  -0.068170   \n",
              "376    -0.393019  -0.226129  -0.450650 -0.394186 -0.574588  -0.070270   \n",
              "377    -0.409695  -0.226129  -0.434087  0.146319 -0.429974  -0.069629   \n",
              "378    -0.026735  -0.226129  -0.303831 -0.294175 -0.002672  -0.067969   \n",
              "379    -0.131475  -0.226129   0.890724  0.291818 -0.919805  -0.069605   \n",
              "380    -0.409695  -0.226129  -0.350541  0.295775 -0.688025  -0.069314   \n",
              "381    -0.386872  -0.226129  -0.671305 -0.460522  0.598231  -0.070931   \n",
              "382    -0.025596  -0.226129   1.252974  0.040479 -0.585408  -0.066894   \n",
              "383    -0.035575  -0.226129  -0.400099 -0.398044  0.105143  -0.068504   \n",
              "384    -0.135143  -0.226129  -0.504056 -0.422954  0.255884  -0.069161   \n",
              "385    -0.328726  -0.226129  -0.367108 -0.381234  0.411464  -0.070823   \n",
              "386    -0.117556  -0.226129  -0.287596 -0.370125  0.094894  -0.049551   \n",
              "387    -0.225640  -0.226129  -0.188738  0.287503 -0.872885  -0.068214   \n",
              "388    -0.033720  -0.226129  -0.398162 -0.397579  0.102334  -0.068492   \n",
              "389    -0.056359  -0.226129  -0.366163 -0.401589 -0.080604  -0.068558   \n",
              "390    -0.332317  -0.226129  -0.123383  0.576973 -1.257267  -0.068252   \n",
              "391    -0.248321  -0.226129  -0.174843  0.349047 -0.954609  -0.068222   \n",
              "392    -0.351114  -0.226129  -0.109228  0.611629 -1.265154  -0.068190   \n",
              "393     0.013131  -0.226129   0.244418 -0.228011 -0.209332  -0.068855   \n",
              "394    -0.320529  -0.226129  -0.697614 -0.469336  0.536548  -0.070385   \n",
              "395    -0.409695  -0.226129  -0.594901 -0.141361  0.066739  -0.070236   \n",
              "396    -0.067025  -0.226129  -0.369699 -0.404032 -0.094131  -0.068617   \n",
              "397    -0.245109  -0.226129   0.207271 -0.274145 -0.554698  -0.070339   \n",
              "398     0.004908  -0.226129  -0.329983 -0.338098 -0.042162  -0.068132   \n",
              "399    -0.033669  -0.226129   0.727163 -0.033303 -0.403061  -0.069289   \n",
              "400    -0.264501  -0.226129  -0.639117 -0.455318  0.451726  -0.070015   \n",
              "401    -0.397980  -0.226129  -0.339178 -0.042780 -0.860649  -0.069678   \n",
              "402    -0.243430  -0.226129  -0.617117 -0.450046  0.419825  -0.069876   \n",
              "403     0.004925  -0.226129   0.803252 -0.079200 -0.426995  -0.069552   \n",
              "404    -0.409695  -0.226129  -0.142643  0.042791 -0.354865  -0.030240   \n",
              "405    -0.284020  -0.226129  -0.441631 -0.453724 -0.369308  -0.069816   \n",
              "406    -0.204695  -0.226129   0.281779 -0.212105  0.013070  -0.070593   \n",
              "407    -0.051167  -0.226129  -0.285450 -0.249027 -0.013300  -0.067890   \n",
              "408    -0.268000  -0.226129  -0.049410 -0.298428  0.216409  -0.070711   \n",
              "409    -0.392961  -0.226129  -0.451940 -0.398250 -0.571280  -0.070277   \n",
              "410    -0.018859  -0.226129   1.282019  0.091406 -0.670354  -0.070142   \n",
              "411    -0.409695  -0.226129  -0.134826  0.130053 -0.493380  -0.034701   \n",
              "412    -0.409116  -0.226129  -0.088990  0.745920 -1.502687  -0.068348   \n",
              "413    -0.409695  -0.226129  -0.399056  0.208985 -0.538173  -0.069497   \n",
              "414    -0.237437  -0.226129   0.522549  0.480390 -1.154518  -0.069100   \n",
              "415    -0.399538  -0.226129  -0.304176  0.067562 -0.950472  -0.069492   \n",
              "416    -0.402119  -0.226129  -0.246197  0.250336 -1.099259  -0.069184   \n",
              "417    -0.309081  -0.226129  -0.419345 -0.400837 -0.377571  -0.069707   \n",
              "418    -0.033370  -0.226129  -0.397797 -0.397492  0.101805  -0.068490   \n",
              "419    -0.314735  -0.226129  -0.423310 -0.406138 -0.386348  -0.069756   \n",
              "420    -0.166050  -0.226129   0.576187 -0.164076 -0.580926  -0.070300   \n",
              "421    -0.013623  -0.226129   1.124983  0.029503 -0.554293  -0.069895   \n",
              "422    -0.220679  -0.226129  -0.593362 -0.444354  0.385381  -0.069726   \n",
              "423    -0.299037  -0.226129  -0.044369 -0.349224 -0.536808  -0.070366   \n",
              "424    -0.182193  -0.226129  -0.330372 -0.281883 -0.180602  -0.068626   \n",
              "425    -0.090489  -0.226129  -0.457434 -0.411783  0.188281  -0.068867   \n",
              "426    -0.306917  -0.226129  -0.138944  0.508050 -1.165746  -0.068243   \n",
              "427    -0.409695  -0.226129  -0.520735 -0.008685 -0.162341  -0.069956   \n",
              "428    -0.401375  -0.226129  -0.644796 -0.485482  0.123182  -0.070712   \n",
              "429    -0.183634  -0.226129  -0.408354 -0.430735 -0.242005  -0.069261   \n",
              "430    -0.409695  -0.226129  -0.137690  0.098083 -0.442633  -0.033066   \n",
              "431    -0.180567  -0.226129  -0.206034  0.101241 -0.476404  -0.067932   \n",
              "432    -0.010973  -0.226129   1.177573  0.037805 -0.574285  -0.069975   \n",
              "433    -0.001601  -0.226129   1.247672  0.039144 -0.600095  -0.070106   \n",
              "434    -0.392570  -0.226129  -0.490359 -0.478961 -0.457203  -0.070436   \n",
              "435    -0.359829  -0.226129  -0.709763 -0.445433  0.564710  -0.070486   \n",
              "436    -0.293916  -0.226129  -0.602766 -0.384352  0.423500  -0.069842   \n",
              "437     0.003080  -0.226129  -0.328864 -0.333139 -0.048746  -0.068133   \n",
              "438    -0.403920  -0.226129  -0.205719  0.377941 -1.203135  -0.068969   \n",
              "439    -0.044603  -0.226129   0.510187 -0.067558 -0.320577  -0.068959   \n",
              "440    -0.190980  -0.226129  -0.209972  0.193451 -0.747996  -0.068201   \n",
              "441    -0.012725  -0.226129  -0.351699 -0.391597 -0.025271  -0.068316   \n",
              "442    -0.370133  -0.226129  -0.192048 -0.345487  0.233608  -0.015866   \n",
              "443    -0.041652  -0.226129  -0.292608 -0.266609 -0.009161  -0.067921   \n",
              "444    -0.170821  -0.226129  -0.541307 -0.431880  0.309898  -0.069397   \n",
              "445    -0.323162  -0.226129   0.224689  0.632947 -1.344405  -0.068692   \n",
              "446    -0.101024  -0.226129  -0.289641 -0.205598  0.010251  -0.067957   \n",
              "447    -0.087975  -0.226129   0.940506 -0.055379 -0.606828  -0.070261   \n",
              "448    -0.393856  -0.226129  -0.431825 -0.334840 -0.622899  -0.070170   \n",
              "449    -0.379897  -0.226129  -0.759599 -0.484189  0.626429  -0.070777   \n",
              "450    -0.091412  -0.226129  -0.266716 -0.196776 -0.039680  -0.067853   \n",
              "451    -0.408040  -0.226129  -0.782052 -0.489387  0.666225  -0.070971   \n",
              "452    -0.409695  -0.226129  -0.750777 -0.420208  0.548199  -0.070823   \n",
              "453    -0.302727  -0.226129  -0.231092 -0.345782  0.327955  -0.070775   \n",
              "454    -0.409695  -0.226129  -0.121887  0.274476 -0.722627  -0.042083   \n",
              "455    -0.409695  -0.226129  -0.580458 -0.115524  0.022128  -0.070181   \n",
              "456    -0.093137  -0.226129  -0.267926 -0.198394 -0.042359  -0.067867   \n",
              "457    -0.185243  -0.226129  -0.426355 -0.283644  0.190681  -0.068780   \n",
              "458    -0.070338  -0.226129  -0.000526 -0.148186 -0.126429  -0.068182   \n",
              "459    -0.126902  -0.226129  -0.331649 -0.229579  0.065691  -0.068210   \n",
              "460    -0.070460  -0.226129  -0.002951 -0.148569 -0.125507  -0.068178   \n",
              "461    -0.214860  -0.226129  -0.353277 -0.312507 -0.231311  -0.068905   \n",
              "462    -0.395806  -0.226129  -0.547111 -0.481357 -0.243925  -0.070537   \n",
              "463    -0.398916  -0.226129  -0.601656 -0.483661 -0.038941  -0.070635   \n",
              "464    -0.363058  -0.226129  -0.104549  0.660391 -1.368036  -0.068262   \n",
              "465    -0.361869  -0.226129  -0.713074 -0.447324  0.569080  -0.070506   \n",
              "466    -0.401000  -0.226129  -0.271324  0.171127 -1.034779  -0.069317   \n",
              "467    -0.409695  -0.226129  -0.624015 -0.193442  0.156662  -0.070345   \n",
              "468     0.007396  -0.226129   0.634966 -0.124013 -0.361449  -0.069342   \n",
              "469    -0.409695  -0.226129  -0.173796  0.611953 -1.233942  -0.068648   \n",
              "\n",
              "     총자본회전율_증감  sido  ownerChange_same  OC  \n",
              "0    -0.054746     3                 1   1  \n",
              "1    -0.004410     4                 1   1  \n",
              "2    -5.958044     4                 1   1  \n",
              "3     0.104751     6                 1   1  \n",
              "4     0.286898     4                 1   1  \n",
              "5    -0.044455     5                 1   1  \n",
              "6    -0.168377     6                 1   1  \n",
              "7     0.103104     2                 0   1  \n",
              "8     0.103987     4                 1   1  \n",
              "9     0.341113     1                 1   1  \n",
              "10    0.168267     5                 0   1  \n",
              "11    0.016214     4                 0   1  \n",
              "12   -0.430929     5                 1   1  \n",
              "13    0.100306     3                 1   1  \n",
              "14   -0.003440     6                 0   1  \n",
              "15    0.050217     5                 0   1  \n",
              "16    0.087882     3                 1   1  \n",
              "17   -0.054223     3                 1   1  \n",
              "18    0.337940     3                 1   1  \n",
              "19    9.861703     3                 1   1  \n",
              "20   -0.004891     3                 1   1  \n",
              "21   -0.077361     2                 1   1  \n",
              "22   -0.100760     5                 1   1  \n",
              "23    0.064899     6                 1   1  \n",
              "24   -0.008820     6                 1   1  \n",
              "25   -0.028929     2                 1   1  \n",
              "26   -0.051857     3                 1   1  \n",
              "27    0.196026     6                 1   1  \n",
              "28    0.245779     6                 1   1  \n",
              "29    0.018309     6                 1   1  \n",
              "30   -0.018790     4                 1   1  \n",
              "31   -0.049965     6                 1   1  \n",
              "32    0.052711     4                 1   1  \n",
              "33   -0.035237     2                 1   1  \n",
              "34    0.217542     6                 1   1  \n",
              "35   -0.012396     2                 1   1  \n",
              "36   -0.018043     3                 1   1  \n",
              "37    0.001408     3                 1   1  \n",
              "38    0.033352     3                 1   1  \n",
              "39    0.007737     5                 1   1  \n",
              "40    0.006031     4                 1   1  \n",
              "41   -0.014502     2                 1   1  \n",
              "42    0.149910     3                 1   1  \n",
              "43   -0.058941     3                 1   1  \n",
              "44   -0.021823     4                 1   1  \n",
              "45   -0.034429     4                 1   1  \n",
              "46   -0.004368     6                 1   1  \n",
              "47    0.057306     4                 0   1  \n",
              "48    0.006396     1                 1   1  \n",
              "49   -0.056705     5                 1   1  \n",
              "50    0.018691     3                 1   1  \n",
              "51   -0.007545     3                 1   1  \n",
              "52    0.022366     6                 1   1  \n",
              "53    0.088078     4                 1   1  \n",
              "54    0.040825     2                 1   1  \n",
              "55    0.040825     2                 1   0  \n",
              "56    0.291294     2                 0   1  \n",
              "57   -0.005972     5                 1   1  \n",
              "58    0.408327     6                 1   1  \n",
              "59   -0.002248     2                 1   1  \n",
              "60    0.002298     4                 1   1  \n",
              "61    0.054227     3                 1   1  \n",
              "62   -0.011423     6                 1   1  \n",
              "63   -0.009512     2                 1   1  \n",
              "64   -0.039920     2                 1   1  \n",
              "65   -0.038804     3                 1   1  \n",
              "66   -0.031647     2                 1   1  \n",
              "67    4.153792     1                 1   1  \n",
              "68    0.027194     3                 1   1  \n",
              "69    0.064609     3                 1   1  \n",
              "70   -0.027654     2                 1   1  \n",
              "71    0.023134     4                 1   1  \n",
              "72   -0.015800     4                 0   1  \n",
              "73    0.136064     4                 1   1  \n",
              "74    0.006986     3                 1   0  \n",
              "75    0.037584     2                 1   1  \n",
              "76    0.009398     4                 1   1  \n",
              "77   -0.037666     2                 0   1  \n",
              "78   -0.034430     4                 1   1  \n",
              "79   -0.050156     6                 1   1  \n",
              "80    0.159790     4                 1   1  \n",
              "81   -0.012003     3                 1   1  \n",
              "82    0.042482     3                 1   1  \n",
              "83    1.275452     4                 1   1  \n",
              "84   -0.020633     4                 1   1  \n",
              "85    0.032678     2                 1   1  \n",
              "86    0.067227     3                 1   1  \n",
              "87    0.115192     3                 1   1  \n",
              "88    0.033229     4                 1   1  \n",
              "89   -0.067186     3                 0   1  \n",
              "90    0.050610     6                 1   1  \n",
              "91    0.024804     4                 1   1  \n",
              "92   -0.000202     3                 1   1  \n",
              "93   -0.033823     2                 1   1  \n",
              "94   -0.028534     2                 1   1  \n",
              "95   -0.049215     6                 1   1  \n",
              "96   -0.063723     2                 1   1  \n",
              "97   -0.101343     4                 1   1  \n",
              "98   -0.003667     3                 1   1  \n",
              "99    0.066311     2                 1   1  \n",
              "100   0.066311     2                 1   1  \n",
              "101   0.015200     2                 1   1  \n",
              "102   0.133243     4                 1   1  \n",
              "103  -0.058046     4                 1   1  \n",
              "104  -0.045262     3                 1   1  \n",
              "105  -0.011699     6                 1   1  \n",
              "106  -0.016049     2                 1   1  \n",
              "107   0.114972     3                 1   1  \n",
              "108   0.005384     4                 1   1  \n",
              "109  -0.086447     3                 1   1  \n",
              "110  -0.009998     4                 1   1  \n",
              "111   0.055117     3                 1   1  \n",
              "112   0.057989     2                 1   1  \n",
              "113  -0.031261     3                 1   1  \n",
              "114  -0.003519     4                 1   1  \n",
              "115   0.065701     2                 0   1  \n",
              "116  -0.028411     2                 1   1  \n",
              "117  -0.043193     3                 1   1  \n",
              "118  -0.056308     4                 0   1  \n",
              "119   0.009598     3                 0   1  \n",
              "120  -9.108514     4                 1   0  \n",
              "121   0.130687     4                 0   1  \n",
              "122   0.023629     3                 1   1  \n",
              "123  -0.035940     4                 1   1  \n",
              "124   0.151399     4                 1   1  \n",
              "125  -0.019438     2                 1   1  \n",
              "126  -0.520294     3                 1   1  \n",
              "127  -0.123305     3                 1   1  \n",
              "128   0.007701     2                 1   1  \n",
              "129   0.077758     2                 1   1  \n",
              "130   0.075662     3                 1   1  \n",
              "131   0.089228     2                 1   1  \n",
              "132  -0.038893     6                 1   1  \n",
              "133  -0.078119     1                 1   1  \n",
              "134   0.085196     2                 0   0  \n",
              "135   0.099442     2                 1   1  \n",
              "136  -0.044319     4                 1   1  \n",
              "137  -0.016065     2                 1   1  \n",
              "138  -0.153126     3                 1   1  \n",
              "139   0.094597     2                 0   1  \n",
              "140  -0.036261     4                 1   1  \n",
              "141  -0.003170     5                 1   1  \n",
              "142  -0.027552     4                 1   1  \n",
              "143   0.001353     4                 1   1  \n",
              "144  -0.040368     4                 1   1  \n",
              "145   0.013464     3                 1   1  \n",
              "146  -0.067427     3                 1   1  \n",
              "147  -0.036635     4                 1   1  \n",
              "148   0.408327     3                 1   1  \n",
              "149   0.005493     3                 1   1  \n",
              "150   0.014024     2                 1   1  \n",
              "151   0.039533     3                 1   1  \n",
              "152  -0.084321     2                 0   1  \n",
              "153  -0.084321     2                 0   0  \n",
              "154  -0.025244     2                 1   1  \n",
              "155  -0.024364     2                 1   1  \n",
              "156   0.087284     3                 1   1  \n",
              "157  -0.031989     2                 1   1  \n",
              "158  -0.001379     2                 1   1  \n",
              "159  -0.025955     5                 1   1  \n",
              "160  -0.006726     5                 1   1  \n",
              "161   0.066329     4                 1   1  \n",
              "162   0.111968     3                 1   1  \n",
              "163  -0.048081     3                 1   1  \n",
              "164   0.273939     4                 1   1  \n",
              "165   0.019730     6                 1   1  \n",
              "166  -0.029811     2                 1   1  \n",
              "167   0.038894     3                 1   1  \n",
              "168   0.013679     3                 1   1  \n",
              "169  -0.061678     2                 1   1  \n",
              "170  -0.034985     2                 1   1  \n",
              "171   0.163274     2                 0   1  \n",
              "172  -0.074972     2                 1   1  \n",
              "173  -0.077885     2                 0   1  \n",
              "174  -0.354718     2                 1   1  \n",
              "175   0.183005     6                 0   1  \n",
              "176   0.017497     6                 1   1  \n",
              "177  -0.007911     4                 1   1  \n",
              "178  -0.009812     4                 1   1  \n",
              "179  -0.051896     4                 1   1  \n",
              "180  -0.023960     4                 1   1  \n",
              "181  -0.062610     2                 1   0  \n",
              "182   0.022550     5                 0   1  \n",
              "183   0.022550     5                 0   0  \n",
              "184   0.015293     3                 1   1  \n",
              "185  -0.025625     3                 1   1  \n",
              "186   0.067055     3                 1   1  \n",
              "187   0.015831     2                 0   1  \n",
              "188  -0.059042     2                 1   1  \n",
              "189  -0.022298     3                 1   1  \n",
              "190   0.016191     2                 1   1  \n",
              "191   0.072405     2                 0   1  \n",
              "192  -0.300773     5                 0   1  \n",
              "193  -0.017819     5                 1   1  \n",
              "194  -0.161037     3                 1   1  \n",
              "195   0.019435     4                 1   1  \n",
              "196  -0.036356     4                 1   1  \n",
              "197  -0.008333     4                 1   1  \n",
              "198  -0.171182     4                 1   1  \n",
              "199   0.001650     4                 1   1  \n",
              "200   0.025322     4                 1   1  \n",
              "201  -0.092374     4                 1   1  \n",
              "202  -0.092374     4                 1   1  \n",
              "203  -0.011078     4                 1   1  \n",
              "204  -0.056812     4                 1   1  \n",
              "205  -0.039382     5                 1   1  \n",
              "206  -0.025226     6                 1   1  \n",
              "207   0.037529     6                 0   1  \n",
              "208  -0.024978     6                 1   1  \n",
              "209  -0.012058     0                 0   1  \n",
              "210  -0.022433     6                 1   1  \n",
              "211  -0.007715     6                 1   1  \n",
              "212   0.126579     6                 1   1  \n",
              "213  -1.689950     2                 0   1  \n",
              "214   0.001079     3                 1   1  \n",
              "215  -0.009654     3                 1   1  \n",
              "216  -0.046656     2                 1   1  \n",
              "217  -0.049216     4                 1   1  \n",
              "218  -0.006632     5                 1   1  \n",
              "219   0.721041     3                 0   1  \n",
              "220   0.176241     3                 0   1  \n",
              "221   0.013249     2                 1   1  \n",
              "222  -0.011152     2                 1   1  \n",
              "223  -0.043097     1                 1   1  \n",
              "224  -0.043097     1                 1   1  \n",
              "225  -0.018548     2                 0   1  \n",
              "226   0.005523     3                 0   1  \n",
              "227   0.001456     6                 1   1  \n",
              "228   0.145468     4                 1   1  \n",
              "229   0.062924     4                 1   1  \n",
              "230  -0.036792     3                 1   1  \n",
              "231   0.003322     3                 1   1  \n",
              "232  -1.222295     3                 0   1  \n",
              "233   0.015553     3                 1   1  \n",
              "234  -0.101352     6                 0   1  \n",
              "235   0.210694     6                 1   1  \n",
              "236  -0.064651     6                 1   1  \n",
              "237  -0.022091     6                 1   1  \n",
              "238   0.011950     4                 1   1  \n",
              "239  -0.038265     6                 1   1  \n",
              "240  -0.023043     2                 1   1  \n",
              "241  -0.055591     5                 1   1  \n",
              "242   0.010390     2                 0   0  \n",
              "243  -7.494783     3                 0   0  \n",
              "244   0.012690     3                 0   0  \n",
              "245   0.041829     2                 0   0  \n",
              "246  -0.037015     2                 0   0  \n",
              "247  -0.045768     2                 0   0  \n",
              "248  -4.162989     3                 1   0  \n",
              "249  -4.470419     3                 1   0  \n",
              "250  -0.016315     3                 0   0  \n",
              "251  -0.057554     2                 0   0  \n",
              "252   0.030045     3                 0   0  \n",
              "253   0.005059     2                 0   0  \n",
              "254   0.024354     2                 1   0  \n",
              "255   0.012515     4                 0   0  \n",
              "256  -0.025617     3                 0   0  \n",
              "257   0.028979     2                 1   0  \n",
              "258   0.025035     2                 0   0  \n",
              "259   0.012139     3                 0   0  \n",
              "260  -1.201952     2                 1   0  \n",
              "261  -0.063303     2                 0   0  \n",
              "262  -0.073483     2                 0   0  \n",
              "263  -0.003445     2                 1   0  \n",
              "264  -0.009122     2                 1   0  \n",
              "265   0.081786     2                 0   0  \n",
              "266  -0.133498     2                 0   0  \n",
              "267   0.032237     3                 0   0  \n",
              "268   0.016446     2                 1   0  \n",
              "269   0.065447     2                 0   0  \n",
              "270  -0.061286     2                 0   0  \n",
              "271   0.025490     4                 0   0  \n",
              "272  -0.000567     2                 0   0  \n",
              "273  -0.072320     2                 0   0  \n",
              "274  -3.882875     4                 0   0  \n",
              "275  -0.082525     2                 0   0  \n",
              "276  -0.064246     2                 0   0  \n",
              "277   0.030481     2                 1   0  \n",
              "278   0.056978     2                 0   0  \n",
              "279  -0.043925     2                 1   0  \n",
              "280   0.023408     2                 1   0  \n",
              "281   0.012787     3                 0   0  \n",
              "282  -0.011062     3                 0   0  \n",
              "283  -1.399950     4                 0   0  \n",
              "284   0.043930     2                 0   0  \n",
              "285   0.032013     2                 0   0  \n",
              "286   0.033756     2                 1   0  \n",
              "287   0.059476     2                 0   0  \n",
              "288  -0.004058     2                 0   0  \n",
              "289  -0.025043     2                 0   0  \n",
              "290   0.066697     2                 0   0  \n",
              "291  -0.069613     2                 0   0  \n",
              "292  -0.068148     2                 0   0  \n",
              "293  -0.002522     2                 1   0  \n",
              "294  -2.635402     4                 0   0  \n",
              "295   0.057182     3                 0   0  \n",
              "296  -6.432749     3                 0   0  \n",
              "297   0.013519     3                 0   0  \n",
              "298  -6.769072     4                 0   0  \n",
              "299  -0.007792     2                 0   0  \n",
              "300  -0.070489     2                 0   0  \n",
              "301   0.084791     2                 0   0  \n",
              "302   0.045861     3                 0   0  \n",
              "303  -0.063559     2                 0   0  \n",
              "304  -0.064151     2                 0   0  \n",
              "305  -0.075706     2                 0   0  \n",
              "306  -0.055106     2                 0   0  \n",
              "307  -1.107510     2                 0   0  \n",
              "308   0.042051     2                 0   0  \n",
              "309  -0.071753     2                 0   0  \n",
              "310   0.021824     2                 1   0  \n",
              "311  -0.054374     2                 0   0  \n",
              "312  -0.049251     2                 0   0  \n",
              "313   0.047705     2                 0   0  \n",
              "314   0.022267     4                 0   0  \n",
              "315   0.005337     4                 0   0  \n",
              "316  -1.265496     3                 1   0  \n",
              "317   0.074092     2                 0   0  \n",
              "318   0.007591     3                 0   0  \n",
              "319  -0.003276     4                 0   0  \n",
              "320  -0.023188     3                 0   0  \n",
              "321   0.017466     4                 0   0  \n",
              "322  -0.044806     2                 1   0  \n",
              "323  -0.043712     2                 1   0  \n",
              "324   0.039649     2                 1   0  \n",
              "325  -0.034765     2                 0   0  \n",
              "326   0.020091     4                 0   0  \n",
              "327  -7.330065     3                 1   0  \n",
              "328  -0.021412     3                 0   0  \n",
              "329   0.011663     3                 0   0  \n",
              "330  -0.067006     2                 0   0  \n",
              "331  -0.070134     2                 0   0  \n",
              "332  -0.060016     2                 0   0  \n",
              "333  -0.019073     2                 0   0  \n",
              "334  -0.058363     2                 1   0  \n",
              "335  -0.011414     3                 0   0  \n",
              "336   0.060061     2                 0   0  \n",
              "337  -0.040813     2                 0   0  \n",
              "338  -6.817891     4                 0   0  \n",
              "339   0.037456     2                 0   0  \n",
              "340   0.044414     2                 0   0  \n",
              "341  -0.003478     2                 1   0  \n",
              "342  -0.077558     2                 0   0  \n",
              "343  -8.577145     4                 0   0  \n",
              "344   0.032055     2                 1   0  \n",
              "345  -0.033404     2                 0   0  \n",
              "346  -6.944627     3                 0   0  \n",
              "347  -8.261383     3                 0   0  \n",
              "348  -0.066580     2                 0   0  \n",
              "349  -0.515944     2                 0   0  \n",
              "350   0.034214     3                 0   0  \n",
              "351   0.031993     3                 0   0  \n",
              "352  -0.041410     2                 0   0  \n",
              "353  -0.025508     3                 0   0  \n",
              "354   0.007041     2                 0   0  \n",
              "355  -8.677915     3                 0   0  \n",
              "356   0.021054     2                 0   0  \n",
              "357  -7.649965     3                 0   0  \n",
              "358   0.024746     4                 0   0  \n",
              "359   0.018555     2                 1   0  \n",
              "360  -0.075939     2                 0   0  \n",
              "361  -0.056057     2                 1   0  \n",
              "362   0.030287     2                 1   0  \n",
              "363   0.083594     2                 0   0  \n",
              "364  -0.050408     2                 1   0  \n",
              "365  -0.061416     2                 1   0  \n",
              "366  -0.027634     2                 0   0  \n",
              "367   0.006107     4                 0   0  \n",
              "368  -5.951777     3                 1   0  \n",
              "369  -0.030271     2                 1   0  \n",
              "370  -0.070214     2                 0   0  \n",
              "371   0.021802     4                 0   0  \n",
              "372   0.082940     2                 0   0  \n",
              "373   0.029104     2                 1   0  \n",
              "374  -6.709934     3                 0   0  \n",
              "375   0.040545     2                 0   0  \n",
              "376   0.079990     2                 0   0  \n",
              "377   0.014784     4                 0   0  \n",
              "378  -0.006820     2                 1   0  \n",
              "379  -0.055464     2                 0   0  \n",
              "380   0.012965     3                 0   0  \n",
              "381   0.016554     4                 0   0  \n",
              "382  -0.587405     2                 0   0  \n",
              "383   0.038398     2                 0   0  \n",
              "384   0.034180     3                 0   0  \n",
              "385   0.001277     4                 0   0  \n",
              "386  -2.912871     2                 1   0  \n",
              "387   0.021423     2                 1   0  \n",
              "388   0.038477     2                 0   0  \n",
              "389   0.049202     2                 0   0  \n",
              "390   0.013055     2                 1   0  \n",
              "391   0.019644     2                 1   0  \n",
              "392  -0.005512     2                 1   0  \n",
              "393  -0.002790     2                 0   0  \n",
              "394   0.026327     4                 0   0  \n",
              "395   0.018286     4                 0   0  \n",
              "396   0.050347     2                 0   0  \n",
              "397   0.021245     2                 0   0  \n",
              "398   0.039507     2                 1   0  \n",
              "399  -0.076033     2                 0   0  \n",
              "400   0.028700     3                 0   0  \n",
              "401   0.058270     2                 0   0  \n",
              "402   0.029593     3                 0   0  \n",
              "403  -0.044476     2                 0   0  \n",
              "404  -6.003551     3                 1   0  \n",
              "405   0.073629     2                 0   0  \n",
              "406  -0.031309     3                 0   0  \n",
              "407  -0.030844     2                 1   0  \n",
              "408  -0.014677     3                 0   0  \n",
              "409   0.080242     2                 0   0  \n",
              "410  -0.080743     2                 0   0  \n",
              "411  -5.298727     3                 1   0  \n",
              "412   0.009522     2                 0   0  \n",
              "413   0.014021     3                 0   0  \n",
              "414  -0.031680     2                 0   0  \n",
              "415   0.051450     2                 0   0  \n",
              "416   0.040153     2                 0   0  \n",
              "417   0.045532     2                 0   0  \n",
              "418   0.038492     2                 0   0  \n",
              "419   0.048242     2                 0   0  \n",
              "420  -0.013216     2                 0   0  \n",
              "421  -0.081436     2                 0   0  \n",
              "422   0.030557     3                 0   0  \n",
              "423   0.044750     2                 0   0  \n",
              "424  -0.015289     2                 0   0  \n",
              "425   0.036072     2                 0   0  \n",
              "426   0.015048     2                 1   0  \n",
              "427   0.016671     4                 0   0  \n",
              "428   0.051723     3                 0   0  \n",
              "429   0.062858     2                 0   0  \n",
              "430  -5.556950     3                 1   0  \n",
              "431  -0.041896     2                 1   0  \n",
              "432  -0.082150     2                 0   0  \n",
              "433  -0.077627     2                 0   0  \n",
              "434   0.082599     2                 0   0  \n",
              "435   0.009533     4                 0   0  \n",
              "436  -0.007674     3                 0   0  \n",
              "437   0.039363     2                 1   0  \n",
              "438   0.032266     2                 0   0  \n",
              "439  -0.073086     2                 0   0  \n",
              "440   0.024142     2                 1   0  \n",
              "441   0.044521     2                 0   0  \n",
              "442  -8.269482     3                 1   0  \n",
              "443  -0.021489     2                 1   0  \n",
              "444   0.032669     3                 0   0  \n",
              "445  -0.012437     2                 0   0  \n",
              "446  -0.058028     2                 0   0  \n",
              "447  -0.047247     2                 0   0  \n",
              "448   0.076322     2                 0   0  \n",
              "449   0.023812     4                 0   0  \n",
              "450  -0.058804     2                 0   0  \n",
              "451   0.022115     4                 0   0  \n",
              "452   0.021680     4                 0   0  \n",
              "453  -0.005553     4                 0   0  \n",
              "454  -4.132219     3                 1   0  \n",
              "455   0.017971     4                 0   0  \n",
              "456  -0.057977     2                 0   0  \n",
              "457  -0.036043     2                 0   0  \n",
              "458  -0.066150     2                 0   0  \n",
              "459  -0.051273     2                 0   0  \n",
              "460  -0.066117     2                 0   0  \n",
              "461   0.000369     2                 0   0  \n",
              "462   0.071253     2                 0   0  \n",
              "463   0.060348     3                 0   0  \n",
              "464   0.010644     2                 1   0  \n",
              "465   0.010065     4                 0   0  \n",
              "466   0.045049     2                 0   0  \n",
              "467   0.018920     4                 0   0  \n",
              "468  -0.031923     2                 0   0  \n",
              "469   0.009116     3                 0   0  "
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = setup(\n",
        "    data = train,\n",
        "    target = \"OC\",\n",
        "    fold = 15\n",
        ")"
      ],
      "metadata": {
        "id": "riFLuXnY38VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top5 = compare_models(sort='F1', n_select=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551,
          "referenced_widgets": [
            "fb0c890df92444fb9e9beb3571503d1a",
            "468ec3a09514479596b91151e3d75a32",
            "26f1d49fea5e4decac2f1e7d772b783e"
          ]
        },
        "id": "oomAP4_A4EJT",
        "outputId": "09e0b54c-b1d5-42e8-d4c1-7ef9bb96c580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-84c6957e-058e-41d7-9ead-fae6b3e66469\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "      <th>TT (Sec)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>rf</th>\n",
              "      <td>Random Forest Classifier</td>\n",
              "      <td>0.9899</td>\n",
              "      <td>0.9984</td>\n",
              "      <td>0.9833</td>\n",
              "      <td>0.9879</td>\n",
              "      <td>0.9844</td>\n",
              "      <td>0.9770</td>\n",
              "      <td>0.9782</td>\n",
              "      <td>0.5367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>et</th>\n",
              "      <td>Extra Trees Classifier</td>\n",
              "      <td>0.9899</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9833</td>\n",
              "      <td>0.9879</td>\n",
              "      <td>0.9844</td>\n",
              "      <td>0.9770</td>\n",
              "      <td>0.9782</td>\n",
              "      <td>0.4727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>catboost</th>\n",
              "      <td>CatBoost Classifier</td>\n",
              "      <td>0.9899</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>0.9833</td>\n",
              "      <td>0.9879</td>\n",
              "      <td>0.9844</td>\n",
              "      <td>0.9770</td>\n",
              "      <td>0.9782</td>\n",
              "      <td>11.1687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xgboost</th>\n",
              "      <td>Extreme Gradient Boosting</td>\n",
              "      <td>0.9874</td>\n",
              "      <td>0.9949</td>\n",
              "      <td>0.9750</td>\n",
              "      <td>0.9879</td>\n",
              "      <td>0.9800</td>\n",
              "      <td>0.9708</td>\n",
              "      <td>0.9723</td>\n",
              "      <td>2.9960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gbc</th>\n",
              "      <td>Gradient Boosting Classifier</td>\n",
              "      <td>0.9848</td>\n",
              "      <td>0.9941</td>\n",
              "      <td>0.9667</td>\n",
              "      <td>0.9879</td>\n",
              "      <td>0.9756</td>\n",
              "      <td>0.9645</td>\n",
              "      <td>0.9663</td>\n",
              "      <td>0.2727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ada</th>\n",
              "      <td>Ada Boost Classifier</td>\n",
              "      <td>0.9848</td>\n",
              "      <td>0.9984</td>\n",
              "      <td>0.9667</td>\n",
              "      <td>0.9879</td>\n",
              "      <td>0.9749</td>\n",
              "      <td>0.9641</td>\n",
              "      <td>0.9664</td>\n",
              "      <td>0.1327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lightgbm</th>\n",
              "      <td>Light Gradient Boosting Machine</td>\n",
              "      <td>0.9848</td>\n",
              "      <td>0.9981</td>\n",
              "      <td>0.9667</td>\n",
              "      <td>0.9879</td>\n",
              "      <td>0.9749</td>\n",
              "      <td>0.9641</td>\n",
              "      <td>0.9664</td>\n",
              "      <td>0.1253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dt</th>\n",
              "      <td>Decision Tree Classifier</td>\n",
              "      <td>0.9798</td>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.9509</td>\n",
              "      <td>0.9867</td>\n",
              "      <td>0.9666</td>\n",
              "      <td>0.9521</td>\n",
              "      <td>0.9541</td>\n",
              "      <td>0.0167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>svm</th>\n",
              "      <td>SVM - Linear Kernel</td>\n",
              "      <td>0.9719</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.9933</td>\n",
              "      <td>0.9487</td>\n",
              "      <td>0.9299</td>\n",
              "      <td>0.9352</td>\n",
              "      <td>0.0133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lr</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.9540</td>\n",
              "      <td>0.9750</td>\n",
              "      <td>0.8519</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9157</td>\n",
              "      <td>0.8850</td>\n",
              "      <td>0.8937</td>\n",
              "      <td>0.2033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lda</th>\n",
              "      <td>Linear Discriminant Analysis</td>\n",
              "      <td>0.9209</td>\n",
              "      <td>0.9181</td>\n",
              "      <td>0.7454</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8499</td>\n",
              "      <td>0.7985</td>\n",
              "      <td>0.8175</td>\n",
              "      <td>0.0147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>knn</th>\n",
              "      <td>K Neighbors Classifier</td>\n",
              "      <td>0.9210</td>\n",
              "      <td>0.9697</td>\n",
              "      <td>0.7463</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8491</td>\n",
              "      <td>0.7985</td>\n",
              "      <td>0.8178</td>\n",
              "      <td>0.1227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ridge</th>\n",
              "      <td>Ridge Classifier</td>\n",
              "      <td>0.9183</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.7370</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8419</td>\n",
              "      <td>0.7901</td>\n",
              "      <td>0.8110</td>\n",
              "      <td>0.0120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>nb</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>0.8981</td>\n",
              "      <td>0.9585</td>\n",
              "      <td>0.6731</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7937</td>\n",
              "      <td>0.7327</td>\n",
              "      <td>0.7641</td>\n",
              "      <td>0.0140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>qda</th>\n",
              "      <td>Quadratic Discriminant Analysis</td>\n",
              "      <td>0.7270</td>\n",
              "      <td>0.8130</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.5472</td>\n",
              "      <td>0.7027</td>\n",
              "      <td>0.4959</td>\n",
              "      <td>0.5743</td>\n",
              "      <td>0.0147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dummy</th>\n",
              "      <td>Dummy Classifier</td>\n",
              "      <td>0.6889</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84c6957e-058e-41d7-9ead-fae6b3e66469')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-84c6957e-058e-41d7-9ead-fae6b3e66469 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-84c6957e-058e-41d7-9ead-fae6b3e66469');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                    Model  Accuracy     AUC  Recall   Prec.  \\\n",
              "rf               Random Forest Classifier    0.9899  0.9984  0.9833  0.9879   \n",
              "et                 Extra Trees Classifier    0.9899  1.0000  0.9833  0.9879   \n",
              "catboost              CatBoost Classifier    0.9899  0.9959  0.9833  0.9879   \n",
              "xgboost         Extreme Gradient Boosting    0.9874  0.9949  0.9750  0.9879   \n",
              "gbc          Gradient Boosting Classifier    0.9848  0.9941  0.9667  0.9879   \n",
              "ada                  Ada Boost Classifier    0.9848  0.9984  0.9667  0.9879   \n",
              "lightgbm  Light Gradient Boosting Machine    0.9848  0.9981  0.9667  0.9879   \n",
              "dt               Decision Tree Classifier    0.9798  0.9718  0.9509  0.9867   \n",
              "svm                   SVM - Linear Kernel    0.9719  0.0000  0.9167  0.9933   \n",
              "lr                    Logistic Regression    0.9540  0.9750  0.8519  1.0000   \n",
              "lda          Linear Discriminant Analysis    0.9209  0.9181  0.7454  1.0000   \n",
              "knn                K Neighbors Classifier    0.9210  0.9697  0.7463  1.0000   \n",
              "ridge                    Ridge Classifier    0.9183  0.0000  0.7370  1.0000   \n",
              "nb                            Naive Bayes    0.8981  0.9585  0.6731  1.0000   \n",
              "qda       Quadratic Discriminant Analysis    0.7270  0.8130  1.0000  0.5472   \n",
              "dummy                    Dummy Classifier    0.6889  0.5000  0.0000  0.0000   \n",
              "\n",
              "              F1   Kappa     MCC  TT (Sec)  \n",
              "rf        0.9844  0.9770  0.9782    0.5367  \n",
              "et        0.9844  0.9770  0.9782    0.4727  \n",
              "catboost  0.9844  0.9770  0.9782   11.1687  \n",
              "xgboost   0.9800  0.9708  0.9723    2.9960  \n",
              "gbc       0.9756  0.9645  0.9663    0.2727  \n",
              "ada       0.9749  0.9641  0.9664    0.1327  \n",
              "lightgbm  0.9749  0.9641  0.9664    0.1253  \n",
              "dt        0.9666  0.9521  0.9541    0.0167  \n",
              "svm       0.9487  0.9299  0.9352    0.0133  \n",
              "lr        0.9157  0.8850  0.8937    0.2033  \n",
              "lda       0.8499  0.7985  0.8175    0.0147  \n",
              "knn       0.8491  0.7985  0.8178    0.1227  \n",
              "ridge     0.8419  0.7901  0.8110    0.0120  \n",
              "nb        0.7937  0.7327  0.7641    0.0140  \n",
              "qda       0.7027  0.4959  0.5743    0.0147  \n",
              "dummy     0.0000  0.0000  0.0000    0.0273  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = create_model('rf', fold = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "152217d3b8bd46198ecefad410d084ad",
            "22e4e2e9d620432dabd952e1905b50e1",
            "5eee8fdd5ad242f5a43376c827415b2b"
          ]
        },
        "id": "05AzHiqT8Meu",
        "outputId": "ceacd5a8-94f8-4126-ce68-118be94c152e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4048d3dc-2c2d-4093-bddb-8cd81d83a56b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.9747</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9259</td>\n",
              "      <td>0.9615</td>\n",
              "      <td>0.9427</td>\n",
              "      <td>0.9443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.9872</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9583</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9787</td>\n",
              "      <td>0.9696</td>\n",
              "      <td>0.9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.9872</td>\n",
              "      <td>0.9730</td>\n",
              "      <td>0.9583</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9787</td>\n",
              "      <td>0.9696</td>\n",
              "      <td>0.9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.9898</td>\n",
              "      <td>0.9946</td>\n",
              "      <td>0.9833</td>\n",
              "      <td>0.9852</td>\n",
              "      <td>0.9838</td>\n",
              "      <td>0.9764</td>\n",
              "      <td>0.9769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SD</th>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0108</td>\n",
              "      <td>0.0204</td>\n",
              "      <td>0.0296</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>0.0216</td>\n",
              "      <td>0.0211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4048d3dc-2c2d-4093-bddb-8cd81d83a56b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4048d3dc-2c2d-4093-bddb-8cd81d83a56b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4048d3dc-2c2d-4093-bddb-8cd81d83a56b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
              "0       0.9747  1.0000  1.0000  0.9259  0.9615  0.9427  0.9443\n",
              "1       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "2       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "3       0.9872  1.0000  0.9583  1.0000  0.9787  0.9696  0.9700\n",
              "4       0.9872  0.9730  0.9583  1.0000  0.9787  0.9696  0.9700\n",
              "Mean    0.9898  0.9946  0.9833  0.9852  0.9838  0.9764  0.9769\n",
              "SD      0.0095  0.0108  0.0204  0.0296  0.0146  0.0216  0.0211"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = finalize_model(model_rf)"
      ],
      "metadata": {
        "id": "349AGtmJCR5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = predict_model(final_model, data = submission) \n",
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "_pzl9NYRCR8G",
        "outputId": "6bed9c23-1eb8-4a8d-f4b6-28a22ba35d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8d8d0100-16b4-4d8d-b0fb-50594097c781\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>openYear</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>liquidAsset1</th>\n",
              "      <th>quickAsset1</th>\n",
              "      <th>inventoryAsset1</th>\n",
              "      <th>liquidLiabilities1</th>\n",
              "      <th>shortLoan1</th>\n",
              "      <th>netAsset1</th>\n",
              "      <th>surplus1</th>\n",
              "      <th>ctax2</th>\n",
              "      <th>profit2</th>\n",
              "      <th>liquidAsset2</th>\n",
              "      <th>receivableS2</th>\n",
              "      <th>inventoryAsset2</th>\n",
              "      <th>tanAsset2</th>\n",
              "      <th>NCLiabilities2</th>\n",
              "      <th>longLoan2</th>\n",
              "      <th>netAsset2</th>\n",
              "      <th>surplus2</th>\n",
              "      <th>employee1</th>\n",
              "      <th>employee2</th>\n",
              "      <th>평균_총자본경상이익율</th>\n",
              "      <th>총자본경상이익율_증감</th>\n",
              "      <th>평균_당좌비율</th>\n",
              "      <th>당좌비율_증감</th>\n",
              "      <th>평균_총자본회전율</th>\n",
              "      <th>sido</th>\n",
              "      <th>Label</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.726079</td>\n",
              "      <td>2.422635</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>3.447613</td>\n",
              "      <td>3.057431</td>\n",
              "      <td>4.594441</td>\n",
              "      <td>3.758111</td>\n",
              "      <td>5.393930</td>\n",
              "      <td>-0.023786</td>\n",
              "      <td>0.207897</td>\n",
              "      <td>3.244247</td>\n",
              "      <td>3.134705</td>\n",
              "      <td>5.047697</td>\n",
              "      <td>7.351420</td>\n",
              "      <td>11.683434</td>\n",
              "      <td>-0.038517</td>\n",
              "      <td>0.830613</td>\n",
              "      <td>0.563204</td>\n",
              "      <td>0.189515</td>\n",
              "      <td>3.294154</td>\n",
              "      <td>0.323490</td>\n",
              "      <td>4.404966</td>\n",
              "      <td>2.001131</td>\n",
              "      <td>-0.082846</td>\n",
              "      <td>0.294008</td>\n",
              "      <td>0.841496</td>\n",
              "      <td>1.521282</td>\n",
              "      <td>3.628717</td>\n",
              "      <td>3.905871</td>\n",
              "      <td>0.068107</td>\n",
              "      <td>0.122859</td>\n",
              "      <td>-0.443315</td>\n",
              "      <td>-0.210372</td>\n",
              "      <td>-0.048298</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.298761</td>\n",
              "      <td>0.963042</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>1.359442</td>\n",
              "      <td>1.246483</td>\n",
              "      <td>-0.268227</td>\n",
              "      <td>1.223775</td>\n",
              "      <td>5.941602</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.163014</td>\n",
              "      <td>0.268622</td>\n",
              "      <td>0.223901</td>\n",
              "      <td>1.287827</td>\n",
              "      <td>5.547936</td>\n",
              "      <td>6.924538</td>\n",
              "      <td>0.576283</td>\n",
              "      <td>0.998891</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>-4.304481</td>\n",
              "      <td>0.262281</td>\n",
              "      <td>-0.366264</td>\n",
              "      <td>1.611775</td>\n",
              "      <td>3.362036</td>\n",
              "      <td>2.442302</td>\n",
              "      <td>3.113450</td>\n",
              "      <td>-0.219084</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.578938</td>\n",
              "      <td>1.668051</td>\n",
              "      <td>-1.304361</td>\n",
              "      <td>0.077525</td>\n",
              "      <td>-0.537117</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>-0.052162</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.738499</td>\n",
              "      <td>0.427281</td>\n",
              "      <td>0.037002</td>\n",
              "      <td>0.759143</td>\n",
              "      <td>0.658739</td>\n",
              "      <td>1.487940</td>\n",
              "      <td>0.703028</td>\n",
              "      <td>2.100202</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-2.121192</td>\n",
              "      <td>1.276429</td>\n",
              "      <td>1.137318</td>\n",
              "      <td>4.277088</td>\n",
              "      <td>3.499308</td>\n",
              "      <td>6.448233</td>\n",
              "      <td>-0.498831</td>\n",
              "      <td>1.655361</td>\n",
              "      <td>-0.239331</td>\n",
              "      <td>-1.712526</td>\n",
              "      <td>1.512966</td>\n",
              "      <td>3.994684</td>\n",
              "      <td>2.889273</td>\n",
              "      <td>1.019648</td>\n",
              "      <td>-0.664527</td>\n",
              "      <td>-0.677570</td>\n",
              "      <td>-0.545905</td>\n",
              "      <td>2.974916</td>\n",
              "      <td>-0.256575</td>\n",
              "      <td>-0.258779</td>\n",
              "      <td>-2097.515887</td>\n",
              "      <td>-17.958029</td>\n",
              "      <td>-0.451839</td>\n",
              "      <td>-0.348341</td>\n",
              "      <td>-3.454428</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.638093</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.698939</td>\n",
              "      <td>-0.705488</td>\n",
              "      <td>5.209016</td>\n",
              "      <td>1.293159</td>\n",
              "      <td>-0.662740</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>2.215229</td>\n",
              "      <td>0.681575</td>\n",
              "      <td>0.609525</td>\n",
              "      <td>2.243891</td>\n",
              "      <td>3.163233</td>\n",
              "      <td>5.458064</td>\n",
              "      <td>2.939640</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>2.230144</td>\n",
              "      <td>1.280661</td>\n",
              "      <td>-0.464590</td>\n",
              "      <td>2.444958</td>\n",
              "      <td>1.532146</td>\n",
              "      <td>-0.349171</td>\n",
              "      <td>-0.677570</td>\n",
              "      <td>2.830250</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>4.066091</td>\n",
              "      <td>4.371504</td>\n",
              "      <td>0.089354</td>\n",
              "      <td>0.092636</td>\n",
              "      <td>-0.364381</td>\n",
              "      <td>-1.401750</td>\n",
              "      <td>-0.072953</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.051132</td>\n",
              "      <td>1.668977</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>2.367727</td>\n",
              "      <td>1.974966</td>\n",
              "      <td>1.410636</td>\n",
              "      <td>-0.495324</td>\n",
              "      <td>8.115302</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>2.704340</td>\n",
              "      <td>0.563637</td>\n",
              "      <td>0.494255</td>\n",
              "      <td>2.092506</td>\n",
              "      <td>3.013570</td>\n",
              "      <td>4.958393</td>\n",
              "      <td>1.159555</td>\n",
              "      <td>1.493700</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>3.754351</td>\n",
              "      <td>0.782577</td>\n",
              "      <td>-0.352276</td>\n",
              "      <td>3.170114</td>\n",
              "      <td>3.031794</td>\n",
              "      <td>3.824296</td>\n",
              "      <td>3.744690</td>\n",
              "      <td>1.674647</td>\n",
              "      <td>2.718465</td>\n",
              "      <td>1.957560</td>\n",
              "      <td>1.765348</td>\n",
              "      <td>0.293342</td>\n",
              "      <td>0.092857</td>\n",
              "      <td>-0.385672</td>\n",
              "      <td>-1.374690</td>\n",
              "      <td>-0.066336</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>0.384685</td>\n",
              "      <td>-0.171818</td>\n",
              "      <td>0.468952</td>\n",
              "      <td>0.732869</td>\n",
              "      <td>0.138297</td>\n",
              "      <td>0.636969</td>\n",
              "      <td>0.762689</td>\n",
              "      <td>1.343518</td>\n",
              "      <td>1.251549</td>\n",
              "      <td>2.113343</td>\n",
              "      <td>2.185145</td>\n",
              "      <td>-0.093313</td>\n",
              "      <td>1.206764</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>0.637504</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>1.887742</td>\n",
              "      <td>1.345246</td>\n",
              "      <td>2.064718</td>\n",
              "      <td>1.499445</td>\n",
              "      <td>-0.215969</td>\n",
              "      <td>0.022079</td>\n",
              "      <td>-0.325579</td>\n",
              "      <td>-0.677570</td>\n",
              "      <td>0.648652</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>2.760499</td>\n",
              "      <td>-0.104052</td>\n",
              "      <td>0.204536</td>\n",
              "      <td>0.092740</td>\n",
              "      <td>-0.189332</td>\n",
              "      <td>-0.027353</td>\n",
              "      <td>-0.067785</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>0.201877</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.412656</td>\n",
              "      <td>0.091085</td>\n",
              "      <td>-0.310900</td>\n",
              "      <td>-0.476904</td>\n",
              "      <td>-0.591269</td>\n",
              "      <td>2.449725</td>\n",
              "      <td>-0.183629</td>\n",
              "      <td>-0.051238</td>\n",
              "      <td>-0.031118</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.420852</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>0.199759</td>\n",
              "      <td>0.738868</td>\n",
              "      <td>0.339050</td>\n",
              "      <td>0.324360</td>\n",
              "      <td>0.119950</td>\n",
              "      <td>0.292196</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.283154</td>\n",
              "      <td>-0.442714</td>\n",
              "      <td>-0.317113</td>\n",
              "      <td>0.193665</td>\n",
              "      <td>1.168090</td>\n",
              "      <td>-0.033467</td>\n",
              "      <td>0.041810</td>\n",
              "      <td>0.039064</td>\n",
              "      <td>0.079371</td>\n",
              "      <td>0.167586</td>\n",
              "      <td>-0.843643</td>\n",
              "      <td>-0.065932</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>0.014253</td>\n",
              "      <td>-0.063629</td>\n",
              "      <td>0.026532</td>\n",
              "      <td>0.027459</td>\n",
              "      <td>-0.315734</td>\n",
              "      <td>0.251598</td>\n",
              "      <td>2.226555</td>\n",
              "      <td>-0.258164</td>\n",
              "      <td>-0.078741</td>\n",
              "      <td>-0.601226</td>\n",
              "      <td>-0.598463</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.701846</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>-0.498831</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.228253</td>\n",
              "      <td>-0.143877</td>\n",
              "      <td>-0.594047</td>\n",
              "      <td>-0.464590</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.707179</td>\n",
              "      <td>-0.664527</td>\n",
              "      <td>-0.677570</td>\n",
              "      <td>-0.545905</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.256575</td>\n",
              "      <td>-0.258779</td>\n",
              "      <td>-2097.515887</td>\n",
              "      <td>-17.958029</td>\n",
              "      <td>-279.615417</td>\n",
              "      <td>-1688.454070</td>\n",
              "      <td>-3.454428</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>1.602877</td>\n",
              "      <td>4.847823</td>\n",
              "      <td>-0.400756</td>\n",
              "      <td>-0.528699</td>\n",
              "      <td>-0.249456</td>\n",
              "      <td>0.606826</td>\n",
              "      <td>2.195036</td>\n",
              "      <td>-0.204679</td>\n",
              "      <td>1.010194</td>\n",
              "      <td>0.924695</td>\n",
              "      <td>0.779639</td>\n",
              "      <td>4.154964</td>\n",
              "      <td>0.684792</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>-0.410196</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>0.774575</td>\n",
              "      <td>0.627066</td>\n",
              "      <td>-0.454677</td>\n",
              "      <td>3.773922</td>\n",
              "      <td>-0.217963</td>\n",
              "      <td>2.874614</td>\n",
              "      <td>4.632178</td>\n",
              "      <td>-0.563592</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.474491</td>\n",
              "      <td>1.473458</td>\n",
              "      <td>-4.916354</td>\n",
              "      <td>0.074339</td>\n",
              "      <td>-0.326372</td>\n",
              "      <td>0.224167</td>\n",
              "      <td>-0.397375</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>-0.951061</td>\n",
              "      <td>-0.137729</td>\n",
              "      <td>-0.132341</td>\n",
              "      <td>-0.148872</td>\n",
              "      <td>-0.263730</td>\n",
              "      <td>-0.296197</td>\n",
              "      <td>0.132156</td>\n",
              "      <td>0.776825</td>\n",
              "      <td>-0.228422</td>\n",
              "      <td>0.065198</td>\n",
              "      <td>-0.601226</td>\n",
              "      <td>-0.598463</td>\n",
              "      <td>-0.487229</td>\n",
              "      <td>-0.701846</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>-0.498831</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.235038</td>\n",
              "      <td>-0.219458</td>\n",
              "      <td>-0.594047</td>\n",
              "      <td>-0.464590</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.707179</td>\n",
              "      <td>-0.664527</td>\n",
              "      <td>-0.677570</td>\n",
              "      <td>-0.545905</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.256575</td>\n",
              "      <td>-0.258779</td>\n",
              "      <td>-2097.515887</td>\n",
              "      <td>-17.958029</td>\n",
              "      <td>-279.615417</td>\n",
              "      <td>-1688.454070</td>\n",
              "      <td>-3.454428</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.90</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d8d0100-16b4-4d8d-b0fb-50594097c781')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d8d0100-16b4-4d8d-b0fb-50594097c781 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d8d0100-16b4-4d8d-b0fb-50594097c781');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     openYear  revenue1  salescost1      sga1  ...  평균_총자본회전율  sido  Label  Score\n",
              "0   -0.726079  2.422635   -0.276402  3.447613  ...  -0.048298     4      1   0.99\n",
              "1    1.298761  0.963042   -0.276402  1.359442  ...  -0.052162     0      1   0.95\n",
              "2   -1.738499  0.427281    0.037002  0.759143  ...  -3.454428     2      1   0.96\n",
              "3    0.623815 -0.638093   -0.276402 -0.698939  ...  -0.072953     4      1   0.92\n",
              "4   -0.051132  1.668977   -0.276402  2.367727  ...  -0.066336     2      1   0.92\n",
              "..        ...       ...         ...       ...  ...        ...   ...    ...    ...\n",
              "122 -0.388606  0.384685   -0.171818  0.468952  ...  -0.067785     2      0   0.89\n",
              "123 -0.388606  0.201877   -0.276402  0.412656  ...  -0.065932     6      1   0.96\n",
              "124 -0.388606  0.014253   -0.063629  0.026532  ...  -3.454428     6      1   0.93\n",
              "125 -0.388606  1.602877    4.847823 -0.400756  ...  -0.397375     0      1   0.90\n",
              "126 -0.951061 -0.137729   -0.132341 -0.148872  ...  -3.454428     2      1   0.90\n",
              "\n",
              "[127 rows x 37 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_et = create_model('et', fold = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "05eaa5d0b9f94a80ac8102e0770ceb61",
            "03a15d446ac047a79c77d96bb83b2482",
            "2b8598dfc3084a3cad96f537900ff000"
          ]
        },
        "id": "WQIwcwszCR-v",
        "outputId": "3d526235-575c-430f-c13a-897910552d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7725eb4e-8189-40e8-9e83-1ba2ee75e9d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.9747</td>\n",
              "      <td>0.9985</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9259</td>\n",
              "      <td>0.9615</td>\n",
              "      <td>0.9427</td>\n",
              "      <td>0.9443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.9872</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9583</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9787</td>\n",
              "      <td>0.9696</td>\n",
              "      <td>0.9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.9872</td>\n",
              "      <td>0.9958</td>\n",
              "      <td>0.9583</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9787</td>\n",
              "      <td>0.9696</td>\n",
              "      <td>0.9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.9898</td>\n",
              "      <td>0.9989</td>\n",
              "      <td>0.9833</td>\n",
              "      <td>0.9852</td>\n",
              "      <td>0.9838</td>\n",
              "      <td>0.9764</td>\n",
              "      <td>0.9769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SD</th>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0204</td>\n",
              "      <td>0.0296</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>0.0216</td>\n",
              "      <td>0.0211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7725eb4e-8189-40e8-9e83-1ba2ee75e9d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7725eb4e-8189-40e8-9e83-1ba2ee75e9d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7725eb4e-8189-40e8-9e83-1ba2ee75e9d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
              "0       0.9747  0.9985  1.0000  0.9259  0.9615  0.9427  0.9443\n",
              "1       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "2       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "3       0.9872  1.0000  0.9583  1.0000  0.9787  0.9696  0.9700\n",
              "4       0.9872  0.9958  0.9583  1.0000  0.9787  0.9696  0.9700\n",
              "Mean    0.9898  0.9989  0.9833  0.9852  0.9838  0.9764  0.9769\n",
              "SD      0.0095  0.0017  0.0204  0.0296  0.0146  0.0216  0.0211"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_et = tune_model(model_et, optimize = 'F1', n_iter = 10, choose_better = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582,
          "referenced_widgets": [
            "33e25d9767bc48e0acdb5e571affcb03",
            "31c10a1fba6d42c2a1d567b05c9242b4",
            "2ec706554de445648e8395eb6838a50f"
          ]
        },
        "id": "NdHVs3oJEEYB",
        "outputId": "9f57cdb5-b221-4def-d4ad-e958fef4aed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-042f0010-7afd-48ae-a945-53dedb2f2fd4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.9630</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8889</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9412</td>\n",
              "      <td>0.9143</td>\n",
              "      <td>0.9177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.9615</td>\n",
              "      <td>0.9931</td>\n",
              "      <td>0.8750</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9333</td>\n",
              "      <td>0.9065</td>\n",
              "      <td>0.9105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.9615</td>\n",
              "      <td>0.9722</td>\n",
              "      <td>0.8750</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9333</td>\n",
              "      <td>0.9065</td>\n",
              "      <td>0.9105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.9924</td>\n",
              "      <td>0.9977</td>\n",
              "      <td>0.9759</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.9872</td>\n",
              "      <td>0.9818</td>\n",
              "      <td>0.9826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SD</th>\n",
              "      <td>0.0152</td>\n",
              "      <td>0.0070</td>\n",
              "      <td>0.0482</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0257</td>\n",
              "      <td>0.0364</td>\n",
              "      <td>0.0349</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-042f0010-7afd-48ae-a945-53dedb2f2fd4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-042f0010-7afd-48ae-a945-53dedb2f2fd4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-042f0010-7afd-48ae-a945-53dedb2f2fd4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Accuracy     AUC  Recall  Prec.      F1   Kappa     MCC\n",
              "0       0.9630  1.0000  0.8889    1.0  0.9412  0.9143  0.9177\n",
              "1       1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "2       1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "3       1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "4       1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "5       1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "6       1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "7       1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "8       1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "9       1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "10      1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "11      0.9615  0.9931  0.8750    1.0  0.9333  0.9065  0.9105\n",
              "12      1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "13      0.9615  0.9722  0.8750    1.0  0.9333  0.9065  0.9105\n",
              "14      1.0000  1.0000  1.0000    1.0  1.0000  1.0000  1.0000\n",
              "Mean    0.9924  0.9977  0.9759    1.0  0.9872  0.9818  0.9826\n",
              "SD      0.0152  0.0070  0.0482    0.0  0.0257  0.0364  0.0349"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = finalize_model(tuned_et)"
      ],
      "metadata": {
        "id": "xkMJScVoEEas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = predict_model(final_model, data = submission) \n",
        "prediction[prediction['Label']==0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "gPgjXumAEEc4",
        "outputId": "b91823d8-743b-4f11-a94d-001fc736d052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a941015c-1a15-4d30-af27-6cde3652319b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>openYear</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>liquidAsset1</th>\n",
              "      <th>quickAsset1</th>\n",
              "      <th>inventoryAsset1</th>\n",
              "      <th>liquidLiabilities1</th>\n",
              "      <th>shortLoan1</th>\n",
              "      <th>netAsset1</th>\n",
              "      <th>surplus1</th>\n",
              "      <th>ctax2</th>\n",
              "      <th>profit2</th>\n",
              "      <th>liquidAsset2</th>\n",
              "      <th>receivableS2</th>\n",
              "      <th>inventoryAsset2</th>\n",
              "      <th>tanAsset2</th>\n",
              "      <th>NCLiabilities2</th>\n",
              "      <th>longLoan2</th>\n",
              "      <th>netAsset2</th>\n",
              "      <th>surplus2</th>\n",
              "      <th>employee1</th>\n",
              "      <th>employee2</th>\n",
              "      <th>평균_총자본경상이익율</th>\n",
              "      <th>총자본경상이익율_증감</th>\n",
              "      <th>평균_당좌비율</th>\n",
              "      <th>당좌비율_증감</th>\n",
              "      <th>평균_총자본회전율</th>\n",
              "      <th>sido</th>\n",
              "      <th>Label</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.398832</td>\n",
              "      <td>-0.425230</td>\n",
              "      <td>-0.247701</td>\n",
              "      <td>-0.443565</td>\n",
              "      <td>-0.403335</td>\n",
              "      <td>-0.332352</td>\n",
              "      <td>-0.207866</td>\n",
              "      <td>-0.283931</td>\n",
              "      <td>-0.199832</td>\n",
              "      <td>-0.217328</td>\n",
              "      <td>-0.387344</td>\n",
              "      <td>-0.388323</td>\n",
              "      <td>-0.238702</td>\n",
              "      <td>-0.251419</td>\n",
              "      <td>0.120992</td>\n",
              "      <td>-0.430533</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>0.049041</td>\n",
              "      <td>-0.594047</td>\n",
              "      <td>-0.464590</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.707179</td>\n",
              "      <td>-0.664527</td>\n",
              "      <td>-0.677570</td>\n",
              "      <td>-0.545905</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.333753</td>\n",
              "      <td>-0.243130</td>\n",
              "      <td>-2097.515887</td>\n",
              "      <td>-17.958029</td>\n",
              "      <td>-279.615417</td>\n",
              "      <td>-1688.454070</td>\n",
              "      <td>-3.454428</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.6568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.026281</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>0.362045</td>\n",
              "      <td>0.231367</td>\n",
              "      <td>0.189178</td>\n",
              "      <td>1.787854</td>\n",
              "      <td>2.262586</td>\n",
              "      <td>0.188468</td>\n",
              "      <td>-5.269798</td>\n",
              "      <td>-0.436714</td>\n",
              "      <td>-0.436203</td>\n",
              "      <td>-0.310887</td>\n",
              "      <td>-0.178199</td>\n",
              "      <td>-0.068857</td>\n",
              "      <td>-0.403334</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>-0.261484</td>\n",
              "      <td>-0.594047</td>\n",
              "      <td>-0.464590</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.707179</td>\n",
              "      <td>-0.664527</td>\n",
              "      <td>-0.677570</td>\n",
              "      <td>-0.545905</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>1.063230</td>\n",
              "      <td>1.174619</td>\n",
              "      <td>-2097.515887</td>\n",
              "      <td>-17.958029</td>\n",
              "      <td>-279.615417</td>\n",
              "      <td>-1688.454070</td>\n",
              "      <td>-3.454428</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>1.186270</td>\n",
              "      <td>-0.355442</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.337916</td>\n",
              "      <td>-0.268290</td>\n",
              "      <td>-0.265663</td>\n",
              "      <td>-0.227760</td>\n",
              "      <td>0.195350</td>\n",
              "      <td>-0.265659</td>\n",
              "      <td>-0.058279</td>\n",
              "      <td>-0.409348</td>\n",
              "      <td>-0.419201</td>\n",
              "      <td>-0.044940</td>\n",
              "      <td>-0.427123</td>\n",
              "      <td>-0.315132</td>\n",
              "      <td>-0.185584</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>-0.016837</td>\n",
              "      <td>-0.594047</td>\n",
              "      <td>-0.464590</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.707179</td>\n",
              "      <td>-0.664527</td>\n",
              "      <td>-0.677570</td>\n",
              "      <td>-0.545905</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.046523</td>\n",
              "      <td>-0.041587</td>\n",
              "      <td>-2097.515887</td>\n",
              "      <td>-17.958029</td>\n",
              "      <td>-279.615417</td>\n",
              "      <td>-1688.454070</td>\n",
              "      <td>-3.454428</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.6518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.387723</td>\n",
              "      <td>-0.232192</td>\n",
              "      <td>-0.386241</td>\n",
              "      <td>-0.336253</td>\n",
              "      <td>-0.307873</td>\n",
              "      <td>-0.363561</td>\n",
              "      <td>-0.148480</td>\n",
              "      <td>-0.312219</td>\n",
              "      <td>-0.204980</td>\n",
              "      <td>-0.293576</td>\n",
              "      <td>-0.283810</td>\n",
              "      <td>-0.423099</td>\n",
              "      <td>-0.557538</td>\n",
              "      <td>-0.364133</td>\n",
              "      <td>-0.156504</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.280518</td>\n",
              "      <td>-0.231403</td>\n",
              "      <td>-0.315243</td>\n",
              "      <td>0.420935</td>\n",
              "      <td>-0.415915</td>\n",
              "      <td>-0.235762</td>\n",
              "      <td>-0.107699</td>\n",
              "      <td>0.258446</td>\n",
              "      <td>-0.144955</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.098747</td>\n",
              "      <td>-0.097185</td>\n",
              "      <td>-0.034385</td>\n",
              "      <td>0.095358</td>\n",
              "      <td>0.730232</td>\n",
              "      <td>-1.340343</td>\n",
              "      <td>-0.069014</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.6808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>0.384685</td>\n",
              "      <td>-0.171818</td>\n",
              "      <td>0.468952</td>\n",
              "      <td>0.732869</td>\n",
              "      <td>0.138297</td>\n",
              "      <td>0.636969</td>\n",
              "      <td>0.762689</td>\n",
              "      <td>1.343518</td>\n",
              "      <td>1.251549</td>\n",
              "      <td>2.113343</td>\n",
              "      <td>2.185145</td>\n",
              "      <td>-0.093313</td>\n",
              "      <td>1.206764</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>0.637504</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>1.887742</td>\n",
              "      <td>1.345246</td>\n",
              "      <td>2.064718</td>\n",
              "      <td>1.499445</td>\n",
              "      <td>-0.215969</td>\n",
              "      <td>0.022079</td>\n",
              "      <td>-0.325579</td>\n",
              "      <td>-0.677570</td>\n",
              "      <td>0.648652</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>2.760499</td>\n",
              "      <td>-0.104052</td>\n",
              "      <td>0.204536</td>\n",
              "      <td>0.092740</td>\n",
              "      <td>-0.189332</td>\n",
              "      <td>-0.027353</td>\n",
              "      <td>-0.067785</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9401</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a941015c-1a15-4d30-af27-6cde3652319b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a941015c-1a15-4d30-af27-6cde3652319b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a941015c-1a15-4d30-af27-6cde3652319b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     openYear  revenue1  salescost1      sga1  ...  평균_총자본회전율  sido  Label   Score\n",
              "40   0.398832 -0.425230   -0.247701 -0.443565  ...  -3.454428     2      0  0.6568\n",
              "48   0.736306 -0.026281   -0.276402  0.362045  ...  -3.454428     3      0  0.5381\n",
              "60   1.186270 -0.355442   -0.276402 -0.337916  ...  -3.454428     3      0  0.6518\n",
              "62   0.623815 -0.387723   -0.232192 -0.386241  ...  -0.069014     2      0  0.6808\n",
              "122 -0.388606  0.384685   -0.171818  0.468952  ...  -0.067785     2      0  0.9401\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cat = create_model('catboost', fold = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "74562fa093d3433b86884ba9f6210131",
            "6a0cdc4a4c4b4c90857d97a7f51e2091",
            "34f581ca93ac40b9a64a8a7ec8693d9c"
          ]
        },
        "id": "xx1dBl42EEfe",
        "outputId": "7d2f21df-f690-41d8-e4bd-f0ed3eb9547b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-87188288-7995-4b4d-9808-065beb12d4d3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.9747</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9259</td>\n",
              "      <td>0.9615</td>\n",
              "      <td>0.9427</td>\n",
              "      <td>0.9443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.9873</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9600</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9796</td>\n",
              "      <td>0.9704</td>\n",
              "      <td>0.9708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.9872</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9583</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9787</td>\n",
              "      <td>0.9696</td>\n",
              "      <td>0.9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.9872</td>\n",
              "      <td>0.9823</td>\n",
              "      <td>0.9583</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9787</td>\n",
              "      <td>0.9696</td>\n",
              "      <td>0.9700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.9873</td>\n",
              "      <td>0.9959</td>\n",
              "      <td>0.9753</td>\n",
              "      <td>0.9852</td>\n",
              "      <td>0.9797</td>\n",
              "      <td>0.9704</td>\n",
              "      <td>0.9710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SD</th>\n",
              "      <td>0.0080</td>\n",
              "      <td>0.0069</td>\n",
              "      <td>0.0201</td>\n",
              "      <td>0.0296</td>\n",
              "      <td>0.0122</td>\n",
              "      <td>0.0181</td>\n",
              "      <td>0.0177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87188288-7995-4b4d-9808-065beb12d4d3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87188288-7995-4b4d-9808-065beb12d4d3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87188288-7995-4b4d-9808-065beb12d4d3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
              "0       0.9747  0.9970  1.0000  0.9259  0.9615  0.9427  0.9443\n",
              "1       0.9873  1.0000  0.9600  1.0000  0.9796  0.9704  0.9708\n",
              "2       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "3       0.9872  1.0000  0.9583  1.0000  0.9787  0.9696  0.9700\n",
              "4       0.9872  0.9823  0.9583  1.0000  0.9787  0.9696  0.9700\n",
              "Mean    0.9873  0.9959  0.9753  0.9852  0.9797  0.9704  0.9710\n",
              "SD      0.0080  0.0069  0.0201  0.0296  0.0122  0.0181  0.0177"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_cat = tune_model(model_cat, optimize = 'F1', n_iter = 10, choose_better = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582,
          "referenced_widgets": [
            "b2c64be81e7a4ebd94724ade2f43a8fa",
            "279c82940f6243b99fc8c6a4c04cab47",
            "3f67fdc1864042298d370b44008ebdf3"
          ]
        },
        "id": "OX5XX8DTFQ_A",
        "outputId": "13081898-4d7e-4219-ccdd-9f1d3bab68cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6b396039-a192-4b6e-aedb-d75510224946\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Prec.</th>\n",
              "      <th>F1</th>\n",
              "      <th>Kappa</th>\n",
              "      <th>MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.9259</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8182</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>0.8421</td>\n",
              "      <td>0.8528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.9615</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8750</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9333</td>\n",
              "      <td>0.9065</td>\n",
              "      <td>0.9105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.9615</td>\n",
              "      <td>0.9514</td>\n",
              "      <td>0.8750</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.9333</td>\n",
              "      <td>0.9065</td>\n",
              "      <td>0.9105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mean</th>\n",
              "      <td>0.9899</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>0.9833</td>\n",
              "      <td>0.9879</td>\n",
              "      <td>0.9844</td>\n",
              "      <td>0.9770</td>\n",
              "      <td>0.9782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SD</th>\n",
              "      <td>0.0215</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0425</td>\n",
              "      <td>0.0454</td>\n",
              "      <td>0.0319</td>\n",
              "      <td>0.0480</td>\n",
              "      <td>0.0452</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b396039-a192-4b6e-aedb-d75510224946')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b396039-a192-4b6e-aedb-d75510224946 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b396039-a192-4b6e-aedb-d75510224946');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Accuracy     AUC  Recall   Prec.      F1   Kappa     MCC\n",
              "0       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "1       0.9259  1.0000  1.0000  0.8182  0.9000  0.8421  0.8528\n",
              "2       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "3       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "4       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "5       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "6       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "7       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "8       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "9       1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "10      1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "11      0.9615  1.0000  0.8750  1.0000  0.9333  0.9065  0.9105\n",
              "12      1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "13      0.9615  0.9514  0.8750  1.0000  0.9333  0.9065  0.9105\n",
              "14      1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
              "Mean    0.9899  0.9968  0.9833  0.9879  0.9844  0.9770  0.9782\n",
              "SD      0.0215  0.0121  0.0425  0.0454  0.0319  0.0480  0.0452"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = finalize_model(tuned_cat)"
      ],
      "metadata": {
        "id": "l2m61aiOFRBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = predict_model(final_model, data = submission) \n",
        "prediction[prediction['Label']==0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "2c7NT5b2FREZ",
        "outputId": "e9ea68e0-1d59-44c8-ad25-75664e53bd0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0b0a11ce-314e-4eeb-acfa-8d12353833a2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>openYear</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>liquidAsset1</th>\n",
              "      <th>quickAsset1</th>\n",
              "      <th>inventoryAsset1</th>\n",
              "      <th>liquidLiabilities1</th>\n",
              "      <th>shortLoan1</th>\n",
              "      <th>netAsset1</th>\n",
              "      <th>surplus1</th>\n",
              "      <th>ctax2</th>\n",
              "      <th>profit2</th>\n",
              "      <th>liquidAsset2</th>\n",
              "      <th>receivableS2</th>\n",
              "      <th>inventoryAsset2</th>\n",
              "      <th>tanAsset2</th>\n",
              "      <th>NCLiabilities2</th>\n",
              "      <th>longLoan2</th>\n",
              "      <th>netAsset2</th>\n",
              "      <th>surplus2</th>\n",
              "      <th>employee1</th>\n",
              "      <th>employee2</th>\n",
              "      <th>평균_총자본경상이익율</th>\n",
              "      <th>총자본경상이익율_증감</th>\n",
              "      <th>평균_당좌비율</th>\n",
              "      <th>당좌비율_증감</th>\n",
              "      <th>평균_총자본회전율</th>\n",
              "      <th>sido</th>\n",
              "      <th>Label</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.217055</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.158617</td>\n",
              "      <td>-0.131943</td>\n",
              "      <td>-0.276812</td>\n",
              "      <td>-0.099442</td>\n",
              "      <td>0.811327</td>\n",
              "      <td>-0.331399</td>\n",
              "      <td>-0.022218</td>\n",
              "      <td>-0.419081</td>\n",
              "      <td>-0.416838</td>\n",
              "      <td>-0.338740</td>\n",
              "      <td>0.132575</td>\n",
              "      <td>0.193053</td>\n",
              "      <td>0.012209</td>\n",
              "      <td>0.318108</td>\n",
              "      <td>-0.229866</td>\n",
              "      <td>-0.075417</td>\n",
              "      <td>-0.510676</td>\n",
              "      <td>-0.464590</td>\n",
              "      <td>-0.420621</td>\n",
              "      <td>0.030850</td>\n",
              "      <td>0.292867</td>\n",
              "      <td>0.890358</td>\n",
              "      <td>-0.336559</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.110148</td>\n",
              "      <td>-0.340426</td>\n",
              "      <td>0.079058</td>\n",
              "      <td>0.085071</td>\n",
              "      <td>-0.517232</td>\n",
              "      <td>0.186346</td>\n",
              "      <td>-0.066654</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.6900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>1.186270</td>\n",
              "      <td>-0.355442</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.337916</td>\n",
              "      <td>-0.268290</td>\n",
              "      <td>-0.265663</td>\n",
              "      <td>-0.227760</td>\n",
              "      <td>0.195350</td>\n",
              "      <td>-0.265659</td>\n",
              "      <td>-0.058279</td>\n",
              "      <td>-0.409348</td>\n",
              "      <td>-0.419201</td>\n",
              "      <td>-0.044940</td>\n",
              "      <td>-0.427123</td>\n",
              "      <td>-0.315132</td>\n",
              "      <td>-0.185584</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>-0.016837</td>\n",
              "      <td>-0.594047</td>\n",
              "      <td>-0.464590</td>\n",
              "      <td>-0.474324</td>\n",
              "      <td>-0.707179</td>\n",
              "      <td>-0.664527</td>\n",
              "      <td>-0.677570</td>\n",
              "      <td>-0.545905</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.046523</td>\n",
              "      <td>-0.041587</td>\n",
              "      <td>-2097.515887</td>\n",
              "      <td>-17.958029</td>\n",
              "      <td>-279.615417</td>\n",
              "      <td>-1688.454070</td>\n",
              "      <td>-3.454428</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>0.961288</td>\n",
              "      <td>-0.004090</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.003480</td>\n",
              "      <td>0.087246</td>\n",
              "      <td>-0.207069</td>\n",
              "      <td>0.972569</td>\n",
              "      <td>0.020162</td>\n",
              "      <td>0.792581</td>\n",
              "      <td>0.627505</td>\n",
              "      <td>-0.101191</td>\n",
              "      <td>-0.089229</td>\n",
              "      <td>-0.331309</td>\n",
              "      <td>-0.115520</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>-0.238389</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>0.458366</td>\n",
              "      <td>0.340676</td>\n",
              "      <td>-0.227937</td>\n",
              "      <td>0.598449</td>\n",
              "      <td>-0.313057</td>\n",
              "      <td>0.177437</td>\n",
              "      <td>0.532876</td>\n",
              "      <td>0.858008</td>\n",
              "      <td>-0.323325</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.345154</td>\n",
              "      <td>0.410147</td>\n",
              "      <td>0.521021</td>\n",
              "      <td>0.096552</td>\n",
              "      <td>-0.380483</td>\n",
              "      <td>0.877343</td>\n",
              "      <td>-0.057747</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>0.384685</td>\n",
              "      <td>-0.171818</td>\n",
              "      <td>0.468952</td>\n",
              "      <td>0.732869</td>\n",
              "      <td>0.138297</td>\n",
              "      <td>0.636969</td>\n",
              "      <td>0.762689</td>\n",
              "      <td>1.343518</td>\n",
              "      <td>1.251549</td>\n",
              "      <td>2.113343</td>\n",
              "      <td>2.185145</td>\n",
              "      <td>-0.093313</td>\n",
              "      <td>1.206764</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>0.637504</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>1.887742</td>\n",
              "      <td>1.345246</td>\n",
              "      <td>2.064718</td>\n",
              "      <td>1.499445</td>\n",
              "      <td>-0.215969</td>\n",
              "      <td>0.022079</td>\n",
              "      <td>-0.325579</td>\n",
              "      <td>-0.677570</td>\n",
              "      <td>0.648652</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>2.760499</td>\n",
              "      <td>-0.104052</td>\n",
              "      <td>0.204536</td>\n",
              "      <td>0.092740</td>\n",
              "      <td>-0.189332</td>\n",
              "      <td>-0.027353</td>\n",
              "      <td>-0.067785</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9989</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b0a11ce-314e-4eeb-acfa-8d12353833a2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0b0a11ce-314e-4eeb-acfa-8d12353833a2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0b0a11ce-314e-4eeb-acfa-8d12353833a2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     openYear  revenue1  salescost1      sga1  ...  평균_총자본회전율  sido  Label   Score\n",
              "36   0.736306 -0.217055   -0.276402 -0.158617  ...  -0.066654     1      0  0.6900\n",
              "60   1.186270 -0.355442   -0.276402 -0.337916  ...  -3.454428     3      0  0.8816\n",
              "105  0.961288 -0.004090   -0.276402 -0.003480  ...  -0.057747     3      0  0.8768\n",
              "122 -0.388606  0.384685   -0.171818  0.468952  ...  -0.067785     2      0  0.9989\n",
              "\n",
              "[4 rows x 37 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### optuna "
      ],
      "metadata": {
        "id": "4WDA74MfKV4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DH-FYRHKigc",
        "outputId": "95293ea2-c923-446b-f23f-4a96265cb53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 92 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 184 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 235 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 256 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 276 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 296 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 308 kB 14.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.5)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.6-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.31)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 57.1 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.0-py3-none-any.whl (150 kB)\n",
            "\u001b[K     |████████████████████████████████| 150 kB 62.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.1.1)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=e3219b4998cbc371cf572b81bb8c535f730dcf268ce40542f6e946c92c04bf11\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.1.6 alembic-1.7.6 autopage-0.5.0 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.0 colorlog-6.6.0 optuna-2.10.0 pbr-5.8.1 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna import Trial, visualization\n",
        "from optuna.samplers import TPESampler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "1B5la3TrKiiv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "635cff56-1896-4657-b7c8-95c2bb537b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-363e9d9dbb78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPESampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data4.drop([\"OC\"], axis = 1)\n",
        "y = data4[[\"OC\"]]"
      ],
      "metadata": {
        "id": "1AW79JAiOACW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMRXCLBrPRpf",
        "outputId": "db711b9c-6c1e-4a57-fef9-4c5c8ec5ced6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sido                              0\n",
              "openYear                          0\n",
              "revenue1                          0\n",
              "salescost1                        0\n",
              "sga1                              0\n",
              "                                 ..\n",
              "당좌비율_증감                           0\n",
              "평균_총자본회전율                         0\n",
              "총자본회전율_증감                         0\n",
              "bedCount_sort_general hospital    0\n",
              "bedCount_sort_hospital            0\n",
              "Length: 65, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_cat = X[['sido','bedCount_sort_general hospital',\t'bedCount_sort_hospital',\t'ownerChange_same']]\n",
        "X_num = X.drop(['sido','bedCount_sort_general hospital',\t'bedCount_sort_hospital',\t'ownerChange_same','instkind_general_hospital',\t'instkind_hospital',\t'instkind_nursing_hospital',\t'instkind_others'], axis=1)"
      ],
      "metadata": {
        "id": "gLkwocOdOFAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_num)\n",
        "\n",
        "X_stand = scaler.transform(X_num)"
      ],
      "metadata": {
        "id": "J7EXzKLfOWpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_stand.reset_index(drop = True, inplace = True)\n",
        "X_cat.reset_index(drop = True, inplace = True)"
      ],
      "metadata": {
        "id": "JgXxm-6EPjOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_stand = pd.DataFrame(data = X_stand, columns = X_num.columns)\n",
        "X_stand1 = pd.concat([X_stand, X_cat],axis=1)"
      ],
      "metadata": {
        "id": "Rni6snewOWsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_stand1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "QQyJkahQTFG3",
        "outputId": "0db678fa-b489-4740-ffbb-fcafed67e089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8856d04e-8fe7-4af2-aacb-91f2083e1e42\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>openYear</th>\n",
              "      <th>revenue1</th>\n",
              "      <th>salescost1</th>\n",
              "      <th>sga1</th>\n",
              "      <th>salary1</th>\n",
              "      <th>noi1</th>\n",
              "      <th>noe1</th>\n",
              "      <th>interest1</th>\n",
              "      <th>ctax1</th>\n",
              "      <th>profit1</th>\n",
              "      <th>liquidAsset1</th>\n",
              "      <th>quickAsset1</th>\n",
              "      <th>receivableS1</th>\n",
              "      <th>inventoryAsset1</th>\n",
              "      <th>nonCAsset1</th>\n",
              "      <th>tanAsset1</th>\n",
              "      <th>OnonCAsset1</th>\n",
              "      <th>receivableL1</th>\n",
              "      <th>debt1</th>\n",
              "      <th>liquidLiabilities1</th>\n",
              "      <th>shortLoan1</th>\n",
              "      <th>NCLiabilities1</th>\n",
              "      <th>longLoan1</th>\n",
              "      <th>netAsset1</th>\n",
              "      <th>surplus1</th>\n",
              "      <th>revenue2</th>\n",
              "      <th>salescost2</th>\n",
              "      <th>sga2</th>\n",
              "      <th>salary2</th>\n",
              "      <th>noi2</th>\n",
              "      <th>noe2</th>\n",
              "      <th>interest2</th>\n",
              "      <th>ctax2</th>\n",
              "      <th>profit2</th>\n",
              "      <th>liquidAsset2</th>\n",
              "      <th>quickAsset2</th>\n",
              "      <th>receivableS2</th>\n",
              "      <th>inventoryAsset2</th>\n",
              "      <th>nonCAsset2</th>\n",
              "      <th>tanAsset2</th>\n",
              "      <th>OnonCAsset2</th>\n",
              "      <th>receivableL2</th>\n",
              "      <th>debt2</th>\n",
              "      <th>liquidLiabilities2</th>\n",
              "      <th>shortLoan2</th>\n",
              "      <th>NCLiabilities2</th>\n",
              "      <th>longLoan2</th>\n",
              "      <th>netAsset2</th>\n",
              "      <th>surplus2</th>\n",
              "      <th>employee1</th>\n",
              "      <th>employee2</th>\n",
              "      <th>평균_총자본경상이익율</th>\n",
              "      <th>총자본경상이익율_증감</th>\n",
              "      <th>평균_당좌비율</th>\n",
              "      <th>당좌비율_증감</th>\n",
              "      <th>평균_총자본회전율</th>\n",
              "      <th>총자본회전율_증감</th>\n",
              "      <th>sido</th>\n",
              "      <th>bedCount_sort_general hospital</th>\n",
              "      <th>bedCount_sort_hospital</th>\n",
              "      <th>ownerChange_same</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.286341</td>\n",
              "      <td>-0.444920</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.448775</td>\n",
              "      <td>-0.468084</td>\n",
              "      <td>-0.329000</td>\n",
              "      <td>-0.482102</td>\n",
              "      <td>-0.617597</td>\n",
              "      <td>-0.205325</td>\n",
              "      <td>-0.036654</td>\n",
              "      <td>-0.450947</td>\n",
              "      <td>-0.445741</td>\n",
              "      <td>-0.202794</td>\n",
              "      <td>-0.432738</td>\n",
              "      <td>-0.568254</td>\n",
              "      <td>-0.567295</td>\n",
              "      <td>-0.253380</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.692452</td>\n",
              "      <td>-0.671966</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>-0.583419</td>\n",
              "      <td>-0.544036</td>\n",
              "      <td>-0.290859</td>\n",
              "      <td>0.005048</td>\n",
              "      <td>-0.433691</td>\n",
              "      <td>-0.284501</td>\n",
              "      <td>-0.429894</td>\n",
              "      <td>-0.448467</td>\n",
              "      <td>-0.420704</td>\n",
              "      <td>-0.492888</td>\n",
              "      <td>-0.603845</td>\n",
              "      <td>-0.225121</td>\n",
              "      <td>-0.080600</td>\n",
              "      <td>-0.464881</td>\n",
              "      <td>-0.459875</td>\n",
              "      <td>-0.131148</td>\n",
              "      <td>-0.423697</td>\n",
              "      <td>-0.559528</td>\n",
              "      <td>-0.558825</td>\n",
              "      <td>-0.256366</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.684708</td>\n",
              "      <td>-0.588184</td>\n",
              "      <td>-0.572006</td>\n",
              "      <td>-0.598458</td>\n",
              "      <td>-0.596778</td>\n",
              "      <td>-0.319985</td>\n",
              "      <td>0.022217</td>\n",
              "      <td>-0.490424</td>\n",
              "      <td>-0.465522</td>\n",
              "      <td>0.116702</td>\n",
              "      <td>0.096008</td>\n",
              "      <td>0.632457</td>\n",
              "      <td>0.449676</td>\n",
              "      <td>-0.067667</td>\n",
              "      <td>-0.054746</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.501097</td>\n",
              "      <td>2.682905</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>3.764669</td>\n",
              "      <td>3.004798</td>\n",
              "      <td>0.232901</td>\n",
              "      <td>0.597703</td>\n",
              "      <td>3.419262</td>\n",
              "      <td>0.367563</td>\n",
              "      <td>0.558400</td>\n",
              "      <td>1.334067</td>\n",
              "      <td>1.167249</td>\n",
              "      <td>-0.503919</td>\n",
              "      <td>4.976478</td>\n",
              "      <td>1.664306</td>\n",
              "      <td>1.580197</td>\n",
              "      <td>3.487949</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.374171</td>\n",
              "      <td>3.452640</td>\n",
              "      <td>5.823489</td>\n",
              "      <td>1.075875</td>\n",
              "      <td>0.831505</td>\n",
              "      <td>0.514328</td>\n",
              "      <td>0.797258</td>\n",
              "      <td>2.673216</td>\n",
              "      <td>-0.284501</td>\n",
              "      <td>3.827541</td>\n",
              "      <td>3.084495</td>\n",
              "      <td>0.402715</td>\n",
              "      <td>0.615016</td>\n",
              "      <td>3.823953</td>\n",
              "      <td>0.565147</td>\n",
              "      <td>0.468205</td>\n",
              "      <td>1.137013</td>\n",
              "      <td>0.998356</td>\n",
              "      <td>-0.464590</td>\n",
              "      <td>4.124054</td>\n",
              "      <td>1.563348</td>\n",
              "      <td>1.459804</td>\n",
              "      <td>3.705636</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.117699</td>\n",
              "      <td>1.773377</td>\n",
              "      <td>2.630713</td>\n",
              "      <td>1.890977</td>\n",
              "      <td>2.450539</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>1.286116</td>\n",
              "      <td>3.432879</td>\n",
              "      <td>3.697378</td>\n",
              "      <td>0.086642</td>\n",
              "      <td>0.096172</td>\n",
              "      <td>-0.432617</td>\n",
              "      <td>-0.637886</td>\n",
              "      <td>-0.055363</td>\n",
              "      <td>-0.004410</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>1.608211</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>2.310763</td>\n",
              "      <td>2.149743</td>\n",
              "      <td>-0.218240</td>\n",
              "      <td>0.736132</td>\n",
              "      <td>3.795674</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.163469</td>\n",
              "      <td>0.336194</td>\n",
              "      <td>0.300603</td>\n",
              "      <td>2.122170</td>\n",
              "      <td>1.122079</td>\n",
              "      <td>1.691097</td>\n",
              "      <td>1.877916</td>\n",
              "      <td>-0.175744</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.928498</td>\n",
              "      <td>3.588785</td>\n",
              "      <td>5.193187</td>\n",
              "      <td>1.866315</td>\n",
              "      <td>2.076491</td>\n",
              "      <td>-0.492521</td>\n",
              "      <td>0.839587</td>\n",
              "      <td>1.740777</td>\n",
              "      <td>-0.284501</td>\n",
              "      <td>2.582601</td>\n",
              "      <td>2.285668</td>\n",
              "      <td>0.618915</td>\n",
              "      <td>0.755994</td>\n",
              "      <td>4.078127</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>-0.233073</td>\n",
              "      <td>0.169402</td>\n",
              "      <td>0.126287</td>\n",
              "      <td>1.678057</td>\n",
              "      <td>1.173996</td>\n",
              "      <td>1.961598</td>\n",
              "      <td>2.129604</td>\n",
              "      <td>0.419844</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.153760</td>\n",
              "      <td>3.608832</td>\n",
              "      <td>5.539854</td>\n",
              "      <td>1.967827</td>\n",
              "      <td>2.245043</td>\n",
              "      <td>-0.545425</td>\n",
              "      <td>1.537099</td>\n",
              "      <td>0.449602</td>\n",
              "      <td>0.458795</td>\n",
              "      <td>7.093530</td>\n",
              "      <td>0.080859</td>\n",
              "      <td>-0.522229</td>\n",
              "      <td>0.476809</td>\n",
              "      <td>15.519711</td>\n",
              "      <td>-5.958044</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-2.525937</td>\n",
              "      <td>0.394847</td>\n",
              "      <td>0.272113</td>\n",
              "      <td>0.347744</td>\n",
              "      <td>0.587141</td>\n",
              "      <td>0.003159</td>\n",
              "      <td>0.346430</td>\n",
              "      <td>2.546376</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.524769</td>\n",
              "      <td>1.091585</td>\n",
              "      <td>1.121564</td>\n",
              "      <td>-0.412180</td>\n",
              "      <td>0.133545</td>\n",
              "      <td>0.783293</td>\n",
              "      <td>0.881219</td>\n",
              "      <td>0.067749</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.032077</td>\n",
              "      <td>2.318304</td>\n",
              "      <td>4.311830</td>\n",
              "      <td>-0.190194</td>\n",
              "      <td>0.128046</td>\n",
              "      <td>0.555297</td>\n",
              "      <td>0.854295</td>\n",
              "      <td>0.417374</td>\n",
              "      <td>0.365882</td>\n",
              "      <td>0.357353</td>\n",
              "      <td>0.573079</td>\n",
              "      <td>-0.051749</td>\n",
              "      <td>0.336283</td>\n",
              "      <td>2.541006</td>\n",
              "      <td>0.004479</td>\n",
              "      <td>0.158587</td>\n",
              "      <td>1.124673</td>\n",
              "      <td>1.172198</td>\n",
              "      <td>-0.409523</td>\n",
              "      <td>-0.198369</td>\n",
              "      <td>0.869795</td>\n",
              "      <td>0.970115</td>\n",
              "      <td>0.105405</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.745661</td>\n",
              "      <td>1.249403</td>\n",
              "      <td>3.650705</td>\n",
              "      <td>0.118031</td>\n",
              "      <td>0.613034</td>\n",
              "      <td>1.083252</td>\n",
              "      <td>1.566155</td>\n",
              "      <td>0.547521</td>\n",
              "      <td>0.799333</td>\n",
              "      <td>0.040079</td>\n",
              "      <td>0.120388</td>\n",
              "      <td>-0.378928</td>\n",
              "      <td>-0.705713</td>\n",
              "      <td>-0.068386</td>\n",
              "      <td>0.104751</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.963482</td>\n",
              "      <td>2.309701</td>\n",
              "      <td>2.282292</td>\n",
              "      <td>1.882984</td>\n",
              "      <td>2.425431</td>\n",
              "      <td>1.286628</td>\n",
              "      <td>2.252866</td>\n",
              "      <td>0.926513</td>\n",
              "      <td>1.781553</td>\n",
              "      <td>0.163683</td>\n",
              "      <td>1.291771</td>\n",
              "      <td>1.219979</td>\n",
              "      <td>3.313117</td>\n",
              "      <td>2.693899</td>\n",
              "      <td>1.412487</td>\n",
              "      <td>1.553750</td>\n",
              "      <td>-0.358995</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.706887</td>\n",
              "      <td>3.092185</td>\n",
              "      <td>3.603464</td>\n",
              "      <td>1.906199</td>\n",
              "      <td>-0.414702</td>\n",
              "      <td>-0.172511</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>2.307606</td>\n",
              "      <td>2.388449</td>\n",
              "      <td>2.045321</td>\n",
              "      <td>2.535129</td>\n",
              "      <td>2.760546</td>\n",
              "      <td>0.252957</td>\n",
              "      <td>0.970040</td>\n",
              "      <td>0.538137</td>\n",
              "      <td>0.723686</td>\n",
              "      <td>0.756789</td>\n",
              "      <td>0.665464</td>\n",
              "      <td>2.670332</td>\n",
              "      <td>2.738065</td>\n",
              "      <td>1.674385</td>\n",
              "      <td>1.820745</td>\n",
              "      <td>-0.396213</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.983609</td>\n",
              "      <td>0.958016</td>\n",
              "      <td>-0.572006</td>\n",
              "      <td>2.387471</td>\n",
              "      <td>2.636983</td>\n",
              "      <td>0.680417</td>\n",
              "      <td>1.566741</td>\n",
              "      <td>2.297014</td>\n",
              "      <td>2.488122</td>\n",
              "      <td>0.140507</td>\n",
              "      <td>0.101035</td>\n",
              "      <td>-0.409102</td>\n",
              "      <td>-0.709298</td>\n",
              "      <td>-0.041466</td>\n",
              "      <td>0.286898</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>0.848797</td>\n",
              "      <td>-0.440993</td>\n",
              "      <td>-0.246525</td>\n",
              "      <td>-0.480682</td>\n",
              "      <td>-0.445313</td>\n",
              "      <td>-0.342530</td>\n",
              "      <td>-0.262913</td>\n",
              "      <td>-0.366468</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.065045</td>\n",
              "      <td>-0.534316</td>\n",
              "      <td>-0.530431</td>\n",
              "      <td>-0.262032</td>\n",
              "      <td>-0.463760</td>\n",
              "      <td>-0.296181</td>\n",
              "      <td>-0.270900</td>\n",
              "      <td>-0.340703</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.379062</td>\n",
              "      <td>-0.270630</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>-0.397613</td>\n",
              "      <td>-0.211413</td>\n",
              "      <td>-0.281039</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.452472</td>\n",
              "      <td>-0.257878</td>\n",
              "      <td>-0.488868</td>\n",
              "      <td>-0.458692</td>\n",
              "      <td>-0.447557</td>\n",
              "      <td>-0.264093</td>\n",
              "      <td>-0.124344</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>-0.080307</td>\n",
              "      <td>-0.555255</td>\n",
              "      <td>-0.552885</td>\n",
              "      <td>-0.319020</td>\n",
              "      <td>-0.431463</td>\n",
              "      <td>-0.278492</td>\n",
              "      <td>-0.252290</td>\n",
              "      <td>-0.365013</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.358717</td>\n",
              "      <td>-0.184654</td>\n",
              "      <td>-0.572006</td>\n",
              "      <td>-0.421781</td>\n",
              "      <td>-0.278684</td>\n",
              "      <td>-0.318390</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.405561</td>\n",
              "      <td>-0.423823</td>\n",
              "      <td>0.153321</td>\n",
              "      <td>0.104193</td>\n",
              "      <td>-0.539233</td>\n",
              "      <td>1.525804</td>\n",
              "      <td>-0.067994</td>\n",
              "      <td>-0.022091</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>0.623815</td>\n",
              "      <td>-0.084819</td>\n",
              "      <td>-0.210959</td>\n",
              "      <td>-0.097616</td>\n",
              "      <td>-0.041157</td>\n",
              "      <td>-0.289917</td>\n",
              "      <td>0.503857</td>\n",
              "      <td>0.029199</td>\n",
              "      <td>1.468136</td>\n",
              "      <td>0.186066</td>\n",
              "      <td>-0.093036</td>\n",
              "      <td>-0.083141</td>\n",
              "      <td>1.103136</td>\n",
              "      <td>-0.276255</td>\n",
              "      <td>0.074209</td>\n",
              "      <td>0.133380</td>\n",
              "      <td>-0.290702</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.146948</td>\n",
              "      <td>0.326127</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>-0.501174</td>\n",
              "      <td>-0.613989</td>\n",
              "      <td>0.198979</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.107379</td>\n",
              "      <td>-0.207376</td>\n",
              "      <td>-0.096162</td>\n",
              "      <td>-0.031388</td>\n",
              "      <td>-0.342880</td>\n",
              "      <td>-0.101288</td>\n",
              "      <td>0.009331</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>0.640763</td>\n",
              "      <td>-0.078617</td>\n",
              "      <td>-0.063881</td>\n",
              "      <td>0.669563</td>\n",
              "      <td>-0.355015</td>\n",
              "      <td>0.036353</td>\n",
              "      <td>0.092412</td>\n",
              "      <td>-0.344485</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.197469</td>\n",
              "      <td>-0.464674</td>\n",
              "      <td>-0.474277</td>\n",
              "      <td>0.085989</td>\n",
              "      <td>0.542600</td>\n",
              "      <td>0.235991</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>0.279875</td>\n",
              "      <td>0.340649</td>\n",
              "      <td>0.123018</td>\n",
              "      <td>0.085907</td>\n",
              "      <td>-0.087410</td>\n",
              "      <td>-1.560450</td>\n",
              "      <td>-0.068782</td>\n",
              "      <td>0.011950</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>0.736306</td>\n",
              "      <td>-0.386159</td>\n",
              "      <td>-0.276402</td>\n",
              "      <td>-0.390501</td>\n",
              "      <td>-0.395732</td>\n",
              "      <td>-0.275133</td>\n",
              "      <td>-0.162807</td>\n",
              "      <td>-0.480441</td>\n",
              "      <td>-0.129625</td>\n",
              "      <td>-0.018631</td>\n",
              "      <td>-0.041428</td>\n",
              "      <td>-0.023323</td>\n",
              "      <td>0.157173</td>\n",
              "      <td>-0.432161</td>\n",
              "      <td>-0.415888</td>\n",
              "      <td>-0.392494</td>\n",
              "      <td>-0.394550</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.413176</td>\n",
              "      <td>-0.559698</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>-0.220341</td>\n",
              "      <td>-0.255258</td>\n",
              "      <td>-0.163708</td>\n",
              "      <td>-0.160814</td>\n",
              "      <td>-0.371220</td>\n",
              "      <td>-0.284501</td>\n",
              "      <td>-0.368434</td>\n",
              "      <td>-0.377559</td>\n",
              "      <td>-0.329981</td>\n",
              "      <td>-0.192822</td>\n",
              "      <td>-0.468548</td>\n",
              "      <td>-0.096092</td>\n",
              "      <td>-0.046821</td>\n",
              "      <td>-0.127716</td>\n",
              "      <td>-0.112199</td>\n",
              "      <td>0.017477</td>\n",
              "      <td>-0.410469</td>\n",
              "      <td>-0.402593</td>\n",
              "      <td>-0.378740</td>\n",
              "      <td>-0.409695</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.443346</td>\n",
              "      <td>-0.519495</td>\n",
              "      <td>-0.572006</td>\n",
              "      <td>-0.265984</td>\n",
              "      <td>-0.304683</td>\n",
              "      <td>-0.171758</td>\n",
              "      <td>-0.226129</td>\n",
              "      <td>-0.490424</td>\n",
              "      <td>-0.451622</td>\n",
              "      <td>0.066266</td>\n",
              "      <td>0.095016</td>\n",
              "      <td>0.561579</td>\n",
              "      <td>-0.253529</td>\n",
              "      <td>-0.068755</td>\n",
              "      <td>-0.038265</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>0.061359</td>\n",
              "      <td>-0.289323</td>\n",
              "      <td>-0.225206</td>\n",
              "      <td>-0.326130</td>\n",
              "      <td>-0.300695</td>\n",
              "      <td>-0.298708</td>\n",
              "      <td>0.150253</td>\n",
              "      <td>-0.023005</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>0.287556</td>\n",
              "      <td>-0.189194</td>\n",
              "      <td>-0.174512</td>\n",
              "      <td>0.647342</td>\n",
              "      <td>-0.461534</td>\n",
              "      <td>-0.052285</td>\n",
              "      <td>-0.008447</td>\n",
              "      <td>-0.276598</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.101468</td>\n",
              "      <td>-0.476251</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>0.218592</td>\n",
              "      <td>0.567830</td>\n",
              "      <td>-0.062051</td>\n",
              "      <td>-0.004969</td>\n",
              "      <td>-0.296078</td>\n",
              "      <td>-0.229113</td>\n",
              "      <td>-0.241197</td>\n",
              "      <td>-0.230571</td>\n",
              "      <td>-0.288197</td>\n",
              "      <td>-0.343047</td>\n",
              "      <td>0.031254</td>\n",
              "      <td>-0.095569</td>\n",
              "      <td>-0.489876</td>\n",
              "      <td>-0.197203</td>\n",
              "      <td>-0.182536</td>\n",
              "      <td>0.785061</td>\n",
              "      <td>-0.443701</td>\n",
              "      <td>-0.003260</td>\n",
              "      <td>0.045367</td>\n",
              "      <td>-0.328924</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.028361</td>\n",
              "      <td>-0.251389</td>\n",
              "      <td>-0.572006</td>\n",
              "      <td>0.174174</td>\n",
              "      <td>0.550886</td>\n",
              "      <td>-0.081911</td>\n",
              "      <td>-0.067567</td>\n",
              "      <td>0.364738</td>\n",
              "      <td>0.069609</td>\n",
              "      <td>0.003546</td>\n",
              "      <td>0.041117</td>\n",
              "      <td>-0.181250</td>\n",
              "      <td>1.334954</td>\n",
              "      <td>-0.068572</td>\n",
              "      <td>-0.023043</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>-0.388606</td>\n",
              "      <td>-0.576673</td>\n",
              "      <td>-0.176455</td>\n",
              "      <td>-0.667088</td>\n",
              "      <td>-0.690092</td>\n",
              "      <td>-0.344335</td>\n",
              "      <td>-0.431492</td>\n",
              "      <td>-0.411866</td>\n",
              "      <td>-0.332867</td>\n",
              "      <td>-0.274337</td>\n",
              "      <td>-0.502927</td>\n",
              "      <td>-0.503588</td>\n",
              "      <td>-0.494015</td>\n",
              "      <td>-0.332624</td>\n",
              "      <td>-0.143743</td>\n",
              "      <td>-0.103937</td>\n",
              "      <td>-0.309001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.344027</td>\n",
              "      <td>-0.228747</td>\n",
              "      <td>-0.588183</td>\n",
              "      <td>-0.374440</td>\n",
              "      <td>-0.177533</td>\n",
              "      <td>-0.097382</td>\n",
              "      <td>0.534097</td>\n",
              "      <td>-0.572688</td>\n",
              "      <td>-0.170827</td>\n",
              "      <td>-0.676943</td>\n",
              "      <td>-0.695151</td>\n",
              "      <td>-0.441965</td>\n",
              "      <td>-0.438977</td>\n",
              "      <td>-0.327762</td>\n",
              "      <td>-0.299407</td>\n",
              "      <td>-0.217395</td>\n",
              "      <td>-0.516725</td>\n",
              "      <td>-0.518353</td>\n",
              "      <td>-0.454750</td>\n",
              "      <td>-0.309497</td>\n",
              "      <td>-0.110133</td>\n",
              "      <td>-0.069247</td>\n",
              "      <td>-0.318337</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.337020</td>\n",
              "      <td>-0.180402</td>\n",
              "      <td>-0.572006</td>\n",
              "      <td>-0.390209</td>\n",
              "      <td>-0.223891</td>\n",
              "      <td>-0.068397</td>\n",
              "      <td>0.998666</td>\n",
              "      <td>-0.797238</td>\n",
              "      <td>-0.806059</td>\n",
              "      <td>-0.048791</td>\n",
              "      <td>0.061608</td>\n",
              "      <td>-0.523936</td>\n",
              "      <td>0.430761</td>\n",
              "      <td>-0.072082</td>\n",
              "      <td>-0.055591</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>242 rows × 61 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8856d04e-8fe7-4af2-aacb-91f2083e1e42')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8856d04e-8fe7-4af2-aacb-91f2083e1e42 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8856d04e-8fe7-4af2-aacb-91f2083e1e42');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     openYear  revenue1  ...  bedCount_sort_hospital  ownerChange_same\n",
              "0    0.286341 -0.444920  ...                       0                 1\n",
              "1   -0.501097  2.682905  ...                       0                 1\n",
              "2    0.061359  1.608211  ...                       0                 1\n",
              "3   -2.525937  0.394847  ...                       0                 1\n",
              "4   -1.963482  2.309701  ...                       0                 1\n",
              "..        ...       ...  ...                     ...               ...\n",
              "237  0.848797 -0.440993  ...                       0                 1\n",
              "238  0.623815 -0.084819  ...                       0                 1\n",
              "239  0.736306 -0.386159  ...                       0                 1\n",
              "240  0.061359 -0.289323  ...                       0                 1\n",
              "241 -0.388606 -0.576673  ...                       0                 1\n",
              "\n",
              "[242 rows x 61 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# standard\n",
        "smote = SMOTE(random_state = 42)\n",
        "X_over, y_over = smote.fit_resample(X_stand1, y)\n",
        "\n",
        "print('After OverSampling, shape of X_train: {}'.format(X_over.shape))\n",
        "print('After OverSampling, shape of y_train: {} \\n'.format(y_over.shape))\n",
        "print('')\n",
        "print(\"After OverSampling, '1': {}\".format(sum(y_over[\"OC\"]==1)))\n",
        "print(\"After OverSampling, '0': {}\".format(sum(y_over[\"OC\"]==0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8vDQftLOWuk",
        "outputId": "e3252ce3-e724-4fcf-d409-30994fecebfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After OverSampling, shape of X_train: (470, 61)\n",
            "After OverSampling, shape of y_train: (470, 1) \n",
            "\n",
            "\n",
            "After OverSampling, '1': 235\n",
            "After OverSampling, '0': 235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=10,random_state=42)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3,random_state=1)\n",
        "rfe = RFECV(model,cv=cv)\n",
        "rfe = rfe.fit(X_over, y_over['OC'])\n",
        "\n",
        "print('Selected features; %s' % list(X_over.columns[rfe.support_]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZOMZKPHQAD7",
        "outputId": "14e2d753-27d8-4d59-bce9-d097e128589f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features; ['openYear', 'salescost1', 'interest1', 'ctax1', 'profit1', 'quickAsset1', 'inventoryAsset1', 'OnonCAsset1', 'liquidLiabilities1', 'NCLiabilities1', 'netAsset1', 'surplus1', 'inventoryAsset2', 'nonCAsset2', 'tanAsset2', 'OnonCAsset2', 'surplus2', 'employee1', '평균_당좌비율', '당좌비율_증감', '평균_총자본회전율', '총자본회전율_증감', 'sido', 'ownerChange_same']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_over = X_over[['openYear', 'salescost1', 'interest1', 'ctax1', 'profit1', 'quickAsset1', 'inventoryAsset1', 'OnonCAsset1', 'liquidLiabilities1', 'NCLiabilities1', 'netAsset1', 'surplus1', 'inventoryAsset2', 'nonCAsset2', 'tanAsset2', 'OnonCAsset2', 'surplus2', 'employee1', '평균_당좌비율', '당좌비율_증감', '평균_총자본회전율', '총자본회전율_증감', 'sido', 'ownerChange_same']]"
      ],
      "metadata": {
        "id": "rgAmCPGsQxsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def RF_objective(trial):\n",
        "    max_depth = trial.suggest_int('max_depth', 1, 10)\n",
        "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 1000)\n",
        "    n_estimators =  trial.suggest_int('n_estimators', 100, 500)\n",
        "   \n",
        "    model = RandomForestClassifier(max_depth = max_depth, max_leaf_nodes = max_leaf_nodes,n_estimators = n_estimators,n_jobs=2,random_state=25)\n",
        "\n",
        "    \n",
        "    model.fit(X_over, y_over)    \n",
        "    score = cross_val_score(model, X_over, y_over, cv=5, scoring=\"f1\")\n",
        "    f1_mean = score.mean()\n",
        "\n",
        "    return f1_mean"
      ],
      "metadata": {
        "id": "VKitfmZy9D7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF_study = optuna.create_study(direction='maximize')\n",
        "RF_study.optimize(RF_objective, n_trials=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YdPpB6JKZxo",
        "outputId": "750eea58-87d5-40cf-997b-c4f35924fae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-01 06:56:15,084]\u001b[0m A new study created in memory with name: no-name-17b59b0d-7382-41bc-be2d-e55437ba18f3\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:56:21,565]\u001b[0m Trial 0 finished with value: 0.9562272261009989 and parameters: {'max_depth': 3, 'max_leaf_nodes': 623, 'n_estimators': 155}. Best is trial 0 with value: 0.9562272261009989.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:56:28,570]\u001b[0m Trial 1 finished with value: 0.9723921006529702 and parameters: {'max_depth': 10, 'max_leaf_nodes': 761, 'n_estimators': 276}. Best is trial 1 with value: 0.9723921006529702.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:56:36,137]\u001b[0m Trial 2 finished with value: 0.9104635400081881 and parameters: {'max_depth': 1, 'max_leaf_nodes': 941, 'n_estimators': 297}. Best is trial 1 with value: 0.9723921006529702.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:56:44,848]\u001b[0m Trial 3 finished with value: 0.970096618357488 and parameters: {'max_depth': 10, 'max_leaf_nodes': 422, 'n_estimators': 361}. Best is trial 1 with value: 0.9723921006529702.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:56:54,661]\u001b[0m Trial 4 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 25, 'n_estimators': 494}. Best is trial 4 with value: 0.9738600592177029.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:01,160]\u001b[0m Trial 5 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 172, 'n_estimators': 439}. Best is trial 5 with value: 0.9739973275773461.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:05,057]\u001b[0m Trial 6 finished with value: 0.970096618357488 and parameters: {'max_depth': 9, 'max_leaf_nodes': 681, 'n_estimators': 232}. Best is trial 5 with value: 0.9739973275773461.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:09,126]\u001b[0m Trial 7 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 219, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:12,247]\u001b[0m Trial 8 finished with value: 0.9672666526242966 and parameters: {'max_depth': 4, 'max_leaf_nodes': 532, 'n_estimators': 189}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:15,974]\u001b[0m Trial 9 finished with value: 0.9445900468988544 and parameters: {'max_depth': 2, 'max_leaf_nodes': 72, 'n_estimators': 223}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:18,684]\u001b[0m Trial 10 finished with value: 0.970096618357488 and parameters: {'max_depth': 7, 'max_leaf_nodes': 306, 'n_estimators': 123}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:25,531]\u001b[0m Trial 11 finished with value: 0.970096618357488 and parameters: {'max_depth': 7, 'max_leaf_nodes': 214, 'n_estimators': 417}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:31,061]\u001b[0m Trial 12 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 229, 'n_estimators': 374}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:37,896]\u001b[0m Trial 13 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 373, 'n_estimators': 499}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:44,346]\u001b[0m Trial 14 finished with value: 0.970096618357488 and parameters: {'max_depth': 8, 'max_leaf_nodes': 148, 'n_estimators': 433}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:49,647]\u001b[0m Trial 15 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 309, 'n_estimators': 347}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:53,688]\u001b[0m Trial 16 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 134, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:57:57,592]\u001b[0m Trial 17 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 9, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:02,388]\u001b[0m Trial 18 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 481, 'n_estimators': 318}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:05,742]\u001b[0m Trial 19 finished with value: 0.9723921006529702 and parameters: {'max_depth': 8, 'max_leaf_nodes': 112, 'n_estimators': 184}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:07,846]\u001b[0m Trial 20 finished with value: 0.8919670534124483 and parameters: {'max_depth': 1, 'max_leaf_nodes': 283, 'n_estimators': 103}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:11,952]\u001b[0m Trial 21 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 163, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:16,580]\u001b[0m Trial 22 finished with value: 0.970096618357488 and parameters: {'max_depth': 7, 'max_leaf_nodes': 181, 'n_estimators': 281}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:21,463]\u001b[0m Trial 23 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 385, 'n_estimators': 322}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:27,455]\u001b[0m Trial 24 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 218, 'n_estimators': 408}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:32,723]\u001b[0m Trial 25 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 261, 'n_estimators': 380}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:37,034]\u001b[0m Trial 26 finished with value: 0.970096618357488 and parameters: {'max_depth': 8, 'max_leaf_nodes': 344, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:43,604]\u001b[0m Trial 27 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 83, 'n_estimators': 454}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:50,011]\u001b[0m Trial 28 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 72, 'n_estimators': 462}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:53,218]\u001b[0m Trial 29 finished with value: 0.9445900468988544 and parameters: {'max_depth': 2, 'max_leaf_nodes': 81, 'n_estimators': 197}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:58:58,343]\u001b[0m Trial 30 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 516, 'n_estimators': 329}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:59:04,243]\u001b[0m Trial 31 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 598, 'n_estimators': 401}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:59:09,893]\u001b[0m Trial 32 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 631, 'n_estimators': 391}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:59:15,584]\u001b[0m Trial 33 finished with value: 0.970096618357488 and parameters: {'max_depth': 7, 'max_leaf_nodes': 821, 'n_estimators': 324}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:59:20,296]\u001b[0m Trial 34 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 717, 'n_estimators': 291}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:59:24,672]\u001b[0m Trial 35 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 746, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:59:31,496]\u001b[0m Trial 36 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 870, 'n_estimators': 466}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:59:36,387]\u001b[0m Trial 37 finished with value: 0.944540109320827 and parameters: {'max_depth': 2, 'max_leaf_nodes': 624, 'n_estimators': 336}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:59:41,136]\u001b[0m Trial 38 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 603, 'n_estimators': 299}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:59:46,551]\u001b[0m Trial 39 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 727, 'n_estimators': 378}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:59:51,662]\u001b[0m Trial 40 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 459, 'n_estimators': 352}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 06:59:57,676]\u001b[0m Trial 41 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 658, 'n_estimators': 412}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:04,365]\u001b[0m Trial 42 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 987, 'n_estimators': 467}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:08,343]\u001b[0m Trial 43 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 574, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:11,936]\u001b[0m Trial 44 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 674, 'n_estimators': 211}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:16,080]\u001b[0m Trial 45 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 532, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:20,824]\u001b[0m Trial 46 finished with value: 0.9718467899429374 and parameters: {'max_depth': 6, 'max_leaf_nodes': 535, 'n_estimators': 304}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:25,473]\u001b[0m Trial 47 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 805, 'n_estimators': 289}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:28,586]\u001b[0m Trial 48 finished with value: 0.9562272261009989 and parameters: {'max_depth': 3, 'max_leaf_nodes': 999, 'n_estimators': 162}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:33,121]\u001b[0m Trial 49 finished with value: 0.9421930431535361 and parameters: {'max_depth': 2, 'max_leaf_nodes': 810, 'n_estimators': 289}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:36,969]\u001b[0m Trial 50 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 896, 'n_estimators': 221}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:40,999]\u001b[0m Trial 51 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 560, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:44,961]\u001b[0m Trial 52 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 984, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:48,785]\u001b[0m Trial 53 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 708, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:52,650]\u001b[0m Trial 54 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 714, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:00:57,447]\u001b[0m Trial 55 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 835, 'n_estimators': 307}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:01,204]\u001b[0m Trial 56 finished with value: 0.9562755524045846 and parameters: {'max_depth': 3, 'max_leaf_nodes': 779, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:04,726]\u001b[0m Trial 57 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 573, 'n_estimators': 207}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:08,543]\u001b[0m Trial 58 finished with value: 0.9421930431535361 and parameters: {'max_depth': 2, 'max_leaf_nodes': 418, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:12,059]\u001b[0m Trial 59 finished with value: 0.9723921006529702 and parameters: {'max_depth': 10, 'max_leaf_nodes': 697, 'n_estimators': 181}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:16,348]\u001b[0m Trial 60 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 713, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:20,209]\u001b[0m Trial 61 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 778, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:24,065]\u001b[0m Trial 62 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 769, 'n_estimators': 232}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:28,117]\u001b[0m Trial 63 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 572, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:32,874]\u001b[0m Trial 64 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 904, 'n_estimators': 290}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:36,698]\u001b[0m Trial 65 finished with value: 0.9562755524045846 and parameters: {'max_depth': 3, 'max_leaf_nodes': 784, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:40,678]\u001b[0m Trial 66 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 576, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:44,521]\u001b[0m Trial 67 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 479, 'n_estimators': 217}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:47,853]\u001b[0m Trial 68 finished with value: 0.9672666526242966 and parameters: {'max_depth': 4, 'max_leaf_nodes': 650, 'n_estimators': 199}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:52,037]\u001b[0m Trial 69 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 726, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:55,196]\u001b[0m Trial 70 finished with value: 0.9650693641717485 and parameters: {'max_depth': 4, 'max_leaf_nodes': 779, 'n_estimators': 162}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:01:59,243]\u001b[0m Trial 71 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 676, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:03,229]\u001b[0m Trial 72 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 569, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:07,191]\u001b[0m Trial 73 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 591, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:11,122]\u001b[0m Trial 74 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 860, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:15,720]\u001b[0m Trial 75 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 678, 'n_estimators': 285}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:19,600]\u001b[0m Trial 76 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 640, 'n_estimators': 242}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:24,523]\u001b[0m Trial 77 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 436, 'n_estimators': 310}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:28,872]\u001b[0m Trial 78 finished with value: 0.970096618357488 and parameters: {'max_depth': 9, 'max_leaf_nodes': 841, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:32,773]\u001b[0m Trial 79 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 744, 'n_estimators': 225}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:36,738]\u001b[0m Trial 80 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 507, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:40,660]\u001b[0m Trial 81 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 697, 'n_estimators': 242}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:44,855]\u001b[0m Trial 82 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 566, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:48,368]\u001b[0m Trial 83 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 705, 'n_estimators': 209}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:53,062]\u001b[0m Trial 84 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 574, 'n_estimators': 291}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:02:57,470]\u001b[0m Trial 85 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 756, 'n_estimators': 293}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:01,568]\u001b[0m Trial 86 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 920, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:05,752]\u001b[0m Trial 87 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 562, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:08,987]\u001b[0m Trial 88 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 961, 'n_estimators': 177}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:12,274]\u001b[0m Trial 89 finished with value: 0.9538801599337081 and parameters: {'max_depth': 3, 'max_leaf_nodes': 810, 'n_estimators': 197}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:16,690]\u001b[0m Trial 90 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 614, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:21,076]\u001b[0m Trial 91 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 623, 'n_estimators': 278}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:25,445]\u001b[0m Trial 92 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 614, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:30,293]\u001b[0m Trial 93 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 658, 'n_estimators': 317}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:34,074]\u001b[0m Trial 94 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 737, 'n_estimators': 228}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:37,823]\u001b[0m Trial 95 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 547, 'n_estimators': 254}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:42,945]\u001b[0m Trial 96 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 624, 'n_estimators': 337}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:46,666]\u001b[0m Trial 97 finished with value: 0.9445900468988544 and parameters: {'max_depth': 2, 'max_leaf_nodes': 793, 'n_estimators': 218}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:50,652]\u001b[0m Trial 98 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 767, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:54,574]\u001b[0m Trial 99 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 904, 'n_estimators': 235}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:03:59,327]\u001b[0m Trial 100 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 764, 'n_estimators': 301}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:03,390]\u001b[0m Trial 101 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 868, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:07,499]\u001b[0m Trial 102 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 662, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:11,368]\u001b[0m Trial 103 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 484, 'n_estimators': 227}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:15,936]\u001b[0m Trial 104 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 585, 'n_estimators': 296}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:20,351]\u001b[0m Trial 105 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 602, 'n_estimators': 280}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:25,176]\u001b[0m Trial 106 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 864, 'n_estimators': 310}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:29,442]\u001b[0m Trial 107 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 931, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:33,207]\u001b[0m Trial 108 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 852, 'n_estimators': 217}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:37,203]\u001b[0m Trial 109 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 524, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:40,576]\u001b[0m Trial 110 finished with value: 0.9538801599337081 and parameters: {'max_depth': 3, 'max_leaf_nodes': 691, 'n_estimators': 202}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:44,597]\u001b[0m Trial 111 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 674, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:48,487]\u001b[0m Trial 112 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 673, 'n_estimators': 254}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:52,600]\u001b[0m Trial 113 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 657, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:04:56,736]\u001b[0m Trial 114 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 654, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:01,051]\u001b[0m Trial 115 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 880, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:05,084]\u001b[0m Trial 116 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 710, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:09,014]\u001b[0m Trial 117 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 521, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:12,957]\u001b[0m Trial 118 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 526, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:17,493]\u001b[0m Trial 119 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 565, 'n_estimators': 285}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:21,908]\u001b[0m Trial 120 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 546, 'n_estimators': 292}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:25,706]\u001b[0m Trial 121 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 500, 'n_estimators': 232}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:29,518]\u001b[0m Trial 122 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 820, 'n_estimators': 233}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:33,205]\u001b[0m Trial 123 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 800, 'n_estimators': 214}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:37,396]\u001b[0m Trial 124 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 830, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:41,321]\u001b[0m Trial 125 finished with value: 0.9540782639520368 and parameters: {'max_depth': 3, 'max_leaf_nodes': 556, 'n_estimators': 226}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:45,237]\u001b[0m Trial 126 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 449, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:48,388]\u001b[0m Trial 127 finished with value: 0.9672666526242966 and parameters: {'max_depth': 4, 'max_leaf_nodes': 593, 'n_estimators': 190}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:52,364]\u001b[0m Trial 128 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 367, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:54,621]\u001b[0m Trial 129 finished with value: 0.9558904451136987 and parameters: {'max_depth': 3, 'max_leaf_nodes': 778, 'n_estimators': 124}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:05:58,932]\u001b[0m Trial 130 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 887, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:03,446]\u001b[0m Trial 131 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 926, 'n_estimators': 283}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:07,257]\u001b[0m Trial 132 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 723, 'n_estimators': 236}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:11,240]\u001b[0m Trial 133 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 919, 'n_estimators': 242}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:15,130]\u001b[0m Trial 134 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 570, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:19,622]\u001b[0m Trial 135 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 614, 'n_estimators': 281}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:24,010]\u001b[0m Trial 136 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 631, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:27,813]\u001b[0m Trial 137 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 744, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:31,857]\u001b[0m Trial 138 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 430, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:35,600]\u001b[0m Trial 139 finished with value: 0.9027591346068391 and parameters: {'max_depth': 1, 'max_leaf_nodes': 481, 'n_estimators': 224}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:39,219]\u001b[0m Trial 140 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 399, 'n_estimators': 211}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:43,102]\u001b[0m Trial 141 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 459, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:46,890]\u001b[0m Trial 142 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 453, 'n_estimators': 230}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:51,308]\u001b[0m Trial 143 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 606, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:55,909]\u001b[0m Trial 144 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 672, 'n_estimators': 299}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:06:59,825]\u001b[0m Trial 145 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 951, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:03,925]\u001b[0m Trial 146 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 648, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:07,990]\u001b[0m Trial 147 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 656, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:12,418]\u001b[0m Trial 148 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 548, 'n_estimators': 292}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:16,835]\u001b[0m Trial 149 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 582, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:20,926]\u001b[0m Trial 150 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 610, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:25,003]\u001b[0m Trial 151 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 877, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:28,954]\u001b[0m Trial 152 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 527, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:32,925]\u001b[0m Trial 153 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 839, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:36,898]\u001b[0m Trial 154 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 504, 'n_estimators': 240}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:40,791]\u001b[0m Trial 155 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 522, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:45,052]\u001b[0m Trial 156 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 490, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:49,660]\u001b[0m Trial 157 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 823, 'n_estimators': 286}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:53,425]\u001b[0m Trial 158 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 739, 'n_estimators': 220}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:07:57,427]\u001b[0m Trial 159 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 447, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:01,262]\u001b[0m Trial 160 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 468, 'n_estimators': 232}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:05,170]\u001b[0m Trial 161 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 690, 'n_estimators': 236}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:08,961]\u001b[0m Trial 162 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 557, 'n_estimators': 220}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:13,195]\u001b[0m Trial 163 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 805, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:17,565]\u001b[0m Trial 164 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 628, 'n_estimators': 274}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:21,779]\u001b[0m Trial 165 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 951, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:26,211]\u001b[0m Trial 166 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 626, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:30,208]\u001b[0m Trial 167 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 402, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:34,221]\u001b[0m Trial 168 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 422, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:38,869]\u001b[0m Trial 169 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 923, 'n_estimators': 285}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:43,456]\u001b[0m Trial 170 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 765, 'n_estimators': 312}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:47,881]\u001b[0m Trial 171 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 680, 'n_estimators': 292}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:51,807]\u001b[0m Trial 172 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 589, 'n_estimators': 242}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:08:56,385]\u001b[0m Trial 173 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 668, 'n_estimators': 283}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:00,413]\u001b[0m Trial 174 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 906, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:04,543]\u001b[0m Trial 175 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 513, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:08,451]\u001b[0m Trial 176 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 887, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:12,262]\u001b[0m Trial 177 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 542, 'n_estimators': 233}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:16,015]\u001b[0m Trial 178 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 751, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:20,006]\u001b[0m Trial 179 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 325, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:24,193]\u001b[0m Trial 180 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 645, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:28,240]\u001b[0m Trial 181 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 858, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:32,125]\u001b[0m Trial 182 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 819, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:36,204]\u001b[0m Trial 183 finished with value: 0.970096618357488 and parameters: {'max_depth': 7, 'max_leaf_nodes': 500, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:40,731]\u001b[0m Trial 184 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 576, 'n_estimators': 296}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:44,549]\u001b[0m Trial 185 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 719, 'n_estimators': 223}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:48,090]\u001b[0m Trial 186 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 707, 'n_estimators': 209}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:52,475]\u001b[0m Trial 187 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 476, 'n_estimators': 276}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:09:56,328]\u001b[0m Trial 188 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 463, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:00,346]\u001b[0m Trial 189 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 432, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:04,210]\u001b[0m Trial 190 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 431, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:08,193]\u001b[0m Trial 191 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 379, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:12,452]\u001b[0m Trial 192 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 974, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:16,440]\u001b[0m Trial 193 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 963, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:20,245]\u001b[0m Trial 194 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 596, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:24,144]\u001b[0m Trial 195 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 832, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:28,162]\u001b[0m Trial 196 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 828, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:32,098]\u001b[0m Trial 197 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 411, 'n_estimators': 227}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:36,729]\u001b[0m Trial 198 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 497, 'n_estimators': 303}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:40,711]\u001b[0m Trial 199 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 541, 'n_estimators': 243}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:44,666]\u001b[0m Trial 200 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 517, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:48,927]\u001b[0m Trial 201 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 640, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:52,948]\u001b[0m Trial 202 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 643, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:10:57,399]\u001b[0m Trial 203 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 450, 'n_estimators': 293}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:01,851]\u001b[0m Trial 204 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 563, 'n_estimators': 282}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:06,403]\u001b[0m Trial 205 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 913, 'n_estimators': 285}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:10,895]\u001b[0m Trial 206 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 451, 'n_estimators': 284}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:15,355]\u001b[0m Trial 207 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 623, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:19,328]\u001b[0m Trial 208 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 846, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:23,267]\u001b[0m Trial 209 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 854, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:27,150]\u001b[0m Trial 210 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 873, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:31,167]\u001b[0m Trial 211 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 957, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:35,107]\u001b[0m Trial 212 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 875, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:39,408]\u001b[0m Trial 213 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 581, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:43,766]\u001b[0m Trial 214 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 894, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:47,645]\u001b[0m Trial 215 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 939, 'n_estimators': 236}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:51,430]\u001b[0m Trial 216 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 928, 'n_estimators': 230}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:55,296]\u001b[0m Trial 217 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 919, 'n_estimators': 236}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:11:59,418]\u001b[0m Trial 218 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 926, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:03,895]\u001b[0m Trial 219 finished with value: 0.970096618357488 and parameters: {'max_depth': 9, 'max_leaf_nodes': 619, 'n_estimators': 274}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:08,039]\u001b[0m Trial 220 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 664, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:12,738]\u001b[0m Trial 221 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 453, 'n_estimators': 288}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:17,297]\u001b[0m Trial 222 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 848, 'n_estimators': 298}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:21,689]\u001b[0m Trial 223 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 680, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:25,533]\u001b[0m Trial 224 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 469, 'n_estimators': 223}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:30,201]\u001b[0m Trial 225 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 605, 'n_estimators': 291}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:34,527]\u001b[0m Trial 226 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 623, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:38,378]\u001b[0m Trial 227 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 484, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:42,060]\u001b[0m Trial 228 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 482, 'n_estimators': 228}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:46,565]\u001b[0m Trial 229 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 596, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:50,734]\u001b[0m Trial 230 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 354, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:54,585]\u001b[0m Trial 231 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 535, 'n_estimators': 233}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:12:58,928]\u001b[0m Trial 232 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 629, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:02,806]\u001b[0m Trial 233 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 791, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:06,674]\u001b[0m Trial 234 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 791, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:12,900]\u001b[0m Trial 235 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 513, 'n_estimators': 431}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:16,894]\u001b[0m Trial 236 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 821, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:21,086]\u001b[0m Trial 237 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 393, 'n_estimators': 270}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:25,247]\u001b[0m Trial 238 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 636, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:29,896]\u001b[0m Trial 239 finished with value: 0.9718467899429374 and parameters: {'max_depth': 5, 'max_leaf_nodes': 431, 'n_estimators': 287}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:33,897]\u001b[0m Trial 240 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 538, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:38,410]\u001b[0m Trial 241 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 570, 'n_estimators': 283}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:43,052]\u001b[0m Trial 242 finished with value: 0.9718467899429374 and parameters: {'max_depth': 5, 'max_leaf_nodes': 569, 'n_estimators': 288}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:47,057]\u001b[0m Trial 243 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 527, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:51,003]\u001b[0m Trial 244 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 416, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:55,103]\u001b[0m Trial 245 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 406, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:13:59,033]\u001b[0m Trial 246 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 488, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:02,943]\u001b[0m Trial 247 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 859, 'n_estimators': 237}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:06,922]\u001b[0m Trial 248 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 875, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:11,129]\u001b[0m Trial 249 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 891, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:15,287]\u001b[0m Trial 250 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 513, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:19,306]\u001b[0m Trial 251 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 829, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:23,623]\u001b[0m Trial 252 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 459, 'n_estimators': 273}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:27,348]\u001b[0m Trial 253 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 501, 'n_estimators': 228}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:31,249]\u001b[0m Trial 254 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 968, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:36,062]\u001b[0m Trial 255 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 979, 'n_estimators': 307}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:39,973]\u001b[0m Trial 256 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 938, 'n_estimators': 235}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:43,905]\u001b[0m Trial 257 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 817, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:50,731]\u001b[0m Trial 258 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 440, 'n_estimators': 477}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:55,195]\u001b[0m Trial 259 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 476, 'n_estimators': 292}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:14:59,753]\u001b[0m Trial 260 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 397, 'n_estimators': 296}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:03,759]\u001b[0m Trial 261 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 651, 'n_estimators': 243}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:08,302]\u001b[0m Trial 262 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 685, 'n_estimators': 281}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:12,762]\u001b[0m Trial 263 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 597, 'n_estimators': 280}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:16,478]\u001b[0m Trial 264 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 905, 'n_estimators': 217}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:21,025]\u001b[0m Trial 265 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 454, 'n_estimators': 284}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:24,819]\u001b[0m Trial 266 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 746, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:28,674]\u001b[0m Trial 267 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 848, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:32,603]\u001b[0m Trial 268 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 556, 'n_estimators': 223}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:36,537]\u001b[0m Trial 269 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 422, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:40,625]\u001b[0m Trial 270 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 998, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:44,647]\u001b[0m Trial 271 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 976, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:48,640]\u001b[0m Trial 272 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 868, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:52,967]\u001b[0m Trial 273 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 471, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:15:57,163]\u001b[0m Trial 274 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 797, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:01,082]\u001b[0m Trial 275 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 947, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:05,076]\u001b[0m Trial 276 finished with value: 0.9562755524045846 and parameters: {'max_depth': 3, 'max_leaf_nodes': 955, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:09,226]\u001b[0m Trial 277 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 381, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:13,102]\u001b[0m Trial 278 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 556, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:16,882]\u001b[0m Trial 279 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 809, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:20,693]\u001b[0m Trial 280 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 804, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:24,576]\u001b[0m Trial 281 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 536, 'n_estimators': 225}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:28,805]\u001b[0m Trial 282 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 265, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:32,832]\u001b[0m Trial 283 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 371, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:36,601]\u001b[0m Trial 284 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 521, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:40,513]\u001b[0m Trial 285 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 426, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:44,646]\u001b[0m Trial 286 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 431, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:49,019]\u001b[0m Trial 287 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 543, 'n_estimators': 278}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:52,852]\u001b[0m Trial 288 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 889, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:16:57,425]\u001b[0m Trial 289 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 968, 'n_estimators': 286}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:02,145]\u001b[0m Trial 290 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 916, 'n_estimators': 286}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:06,743]\u001b[0m Trial 291 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 650, 'n_estimators': 300}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:11,162]\u001b[0m Trial 292 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 673, 'n_estimators': 280}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:15,397]\u001b[0m Trial 293 finished with value: 0.9723921006529702 and parameters: {'max_depth': 10, 'max_leaf_nodes': 390, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:19,784]\u001b[0m Trial 294 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 614, 'n_estimators': 278}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:23,994]\u001b[0m Trial 295 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 836, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:27,759]\u001b[0m Trial 296 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 837, 'n_estimators': 230}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:31,636]\u001b[0m Trial 297 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 352, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:35,693]\u001b[0m Trial 298 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 409, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:39,691]\u001b[0m Trial 299 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 594, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:43,651]\u001b[0m Trial 300 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 474, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:47,822]\u001b[0m Trial 301 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 641, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:51,742]\u001b[0m Trial 302 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 448, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:55,833]\u001b[0m Trial 303 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 872, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:17:59,651]\u001b[0m Trial 304 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 884, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:03,522]\u001b[0m Trial 305 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 319, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:07,461]\u001b[0m Trial 306 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 506, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:11,304]\u001b[0m Trial 307 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 446, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:15,294]\u001b[0m Trial 308 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 860, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:19,420]\u001b[0m Trial 309 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 496, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:23,338]\u001b[0m Trial 310 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 847, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:27,123]\u001b[0m Trial 311 finished with value: 0.9540782639520368 and parameters: {'max_depth': 3, 'max_leaf_nodes': 930, 'n_estimators': 221}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:30,822]\u001b[0m Trial 312 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 918, 'n_estimators': 217}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:35,243]\u001b[0m Trial 313 finished with value: 0.970096618357488 and parameters: {'max_depth': 8, 'max_leaf_nodes': 493, 'n_estimators': 273}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:40,072]\u001b[0m Trial 314 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 593, 'n_estimators': 319}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:43,951]\u001b[0m Trial 315 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 520, 'n_estimators': 235}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:48,579]\u001b[0m Trial 316 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 827, 'n_estimators': 300}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:52,003]\u001b[0m Trial 317 finished with value: 0.9672666526242966 and parameters: {'max_depth': 4, 'max_leaf_nodes': 932, 'n_estimators': 203}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:18:56,246]\u001b[0m Trial 318 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 691, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:01,768]\u001b[0m Trial 319 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 421, 'n_estimators': 370}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:05,960]\u001b[0m Trial 320 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 421, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:09,642]\u001b[0m Trial 321 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 460, 'n_estimators': 213}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:13,871]\u001b[0m Trial 322 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 720, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:17,899]\u001b[0m Trial 323 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 662, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:21,847]\u001b[0m Trial 324 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 555, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:26,515]\u001b[0m Trial 325 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 630, 'n_estimators': 291}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:30,942]\u001b[0m Trial 326 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 606, 'n_estimators': 293}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:34,781]\u001b[0m Trial 327 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 371, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:39,180]\u001b[0m Trial 328 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 866, 'n_estimators': 276}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:43,102]\u001b[0m Trial 329 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 844, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:47,457]\u001b[0m Trial 330 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 857, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:51,500]\u001b[0m Trial 331 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 992, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:55,753]\u001b[0m Trial 332 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 899, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:19:57,946]\u001b[0m Trial 333 finished with value: 0.9650693641717485 and parameters: {'max_depth': 4, 'max_leaf_nodes': 891, 'n_estimators': 104}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:01,891]\u001b[0m Trial 334 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 372, 'n_estimators': 227}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:05,830]\u001b[0m Trial 335 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 934, 'n_estimators': 237}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:10,358]\u001b[0m Trial 336 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 955, 'n_estimators': 284}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:14,229]\u001b[0m Trial 337 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 905, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:18,164]\u001b[0m Trial 338 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 438, 'n_estimators': 243}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:22,058]\u001b[0m Trial 339 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 574, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:25,942]\u001b[0m Trial 340 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 575, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:29,784]\u001b[0m Trial 341 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 465, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:33,813]\u001b[0m Trial 342 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 482, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:38,170]\u001b[0m Trial 343 finished with value: 0.9421930431535361 and parameters: {'max_depth': 2, 'max_leaf_nodes': 945, 'n_estimators': 295}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:42,739]\u001b[0m Trial 344 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 823, 'n_estimators': 288}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:47,397]\u001b[0m Trial 345 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 903, 'n_estimators': 307}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:51,338]\u001b[0m Trial 346 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 540, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:55,160]\u001b[0m Trial 347 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 505, 'n_estimators': 233}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:20:59,039]\u001b[0m Trial 348 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 535, 'n_estimators': 224}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:02,997]\u001b[0m Trial 349 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 431, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:07,623]\u001b[0m Trial 350 finished with value: 0.970096618357488 and parameters: {'max_depth': 7, 'max_leaf_nodes': 922, 'n_estimators': 284}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:11,997]\u001b[0m Trial 351 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 430, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:16,606]\u001b[0m Trial 352 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 17, 'n_estimators': 288}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:21,274]\u001b[0m Trial 353 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 876, 'n_estimators': 303}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:25,062]\u001b[0m Trial 354 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 492, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:28,873]\u001b[0m Trial 355 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 938, 'n_estimators': 221}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:32,666]\u001b[0m Trial 356 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 514, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:37,178]\u001b[0m Trial 357 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 635, 'n_estimators': 284}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:41,368]\u001b[0m Trial 358 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 793, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:45,329]\u001b[0m Trial 359 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 917, 'n_estimators': 227}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:49,281]\u001b[0m Trial 360 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 350, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:53,312]\u001b[0m Trial 361 finished with value: 0.9027591346068391 and parameters: {'max_depth': 1, 'max_leaf_nodes': 881, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:21:57,842]\u001b[0m Trial 362 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 1000, 'n_estimators': 280}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:02,113]\u001b[0m Trial 363 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 853, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:06,689]\u001b[0m Trial 364 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 897, 'n_estimators': 289}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:10,931]\u001b[0m Trial 365 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 845, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:15,406]\u001b[0m Trial 366 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 905, 'n_estimators': 294}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:19,320]\u001b[0m Trial 367 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 870, 'n_estimators': 243}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:23,452]\u001b[0m Trial 368 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 879, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:27,823]\u001b[0m Trial 369 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 472, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:31,739]\u001b[0m Trial 370 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 793, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:35,450]\u001b[0m Trial 371 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 942, 'n_estimators': 214}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:39,304]\u001b[0m Trial 372 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 771, 'n_estimators': 235}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:43,357]\u001b[0m Trial 373 finished with value: 0.970096618357488 and parameters: {'max_depth': 7, 'max_leaf_nodes': 810, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:47,492]\u001b[0m Trial 374 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 754, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:52,248]\u001b[0m Trial 375 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 700, 'n_estimators': 302}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:22:57,007]\u001b[0m Trial 376 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 622, 'n_estimators': 309}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:01,022]\u001b[0m Trial 377 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 307, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:05,481]\u001b[0m Trial 378 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 448, 'n_estimators': 281}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:09,999]\u001b[0m Trial 379 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 194, 'n_estimators': 295}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:14,768]\u001b[0m Trial 380 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 594, 'n_estimators': 314}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:19,845]\u001b[0m Trial 381 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 654, 'n_estimators': 331}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:24,314]\u001b[0m Trial 382 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 610, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:28,717]\u001b[0m Trial 383 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 890, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:33,262]\u001b[0m Trial 384 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 392, 'n_estimators': 286}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:37,670]\u001b[0m Trial 385 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 970, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:41,625]\u001b[0m Trial 386 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 828, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:47,146]\u001b[0m Trial 387 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 913, 'n_estimators': 392}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:51,123]\u001b[0m Trial 388 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 402, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:54,987]\u001b[0m Trial 389 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 872, 'n_estimators': 235}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:23:58,936]\u001b[0m Trial 390 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 863, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:02,833]\u001b[0m Trial 391 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 891, 'n_estimators': 225}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:06,827]\u001b[0m Trial 392 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 462, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:10,771]\u001b[0m Trial 393 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 782, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:15,149]\u001b[0m Trial 394 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 846, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:19,175]\u001b[0m Trial 395 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 736, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:23,137]\u001b[0m Trial 396 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 662, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:27,443]\u001b[0m Trial 397 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 265, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:31,636]\u001b[0m Trial 398 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 212, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:35,433]\u001b[0m Trial 399 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 557, 'n_estimators': 219}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:39,264]\u001b[0m Trial 400 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 92, 'n_estimators': 233}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:43,248]\u001b[0m Trial 401 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 583, 'n_estimators': 242}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:47,483]\u001b[0m Trial 402 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 109, 'n_estimators': 270}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:51,934]\u001b[0m Trial 403 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 622, 'n_estimators': 292}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:24:56,334]\u001b[0m Trial 404 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 634, 'n_estimators': 276}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:00,601]\u001b[0m Trial 405 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 639, 'n_estimators': 273}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:05,244]\u001b[0m Trial 406 finished with value: 0.970096618357488 and parameters: {'max_depth': 8, 'max_leaf_nodes': 601, 'n_estimators': 283}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:09,271]\u001b[0m Trial 407 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 930, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:13,686]\u001b[0m Trial 408 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 452, 'n_estimators': 292}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:17,985]\u001b[0m Trial 409 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 759, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:22,136]\u001b[0m Trial 410 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 768, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:26,199]\u001b[0m Trial 411 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 791, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:30,039]\u001b[0m Trial 412 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 400, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:33,971]\u001b[0m Trial 413 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 493, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:37,792]\u001b[0m Trial 414 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 528, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:40,695]\u001b[0m Trial 415 finished with value: 0.9718467899429374 and parameters: {'max_depth': 6, 'max_leaf_nodes': 698, 'n_estimators': 147}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:44,802]\u001b[0m Trial 416 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 980, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:48,510]\u001b[0m Trial 417 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 816, 'n_estimators': 229}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:52,390]\u001b[0m Trial 418 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 808, 'n_estimators': 237}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:25:56,276]\u001b[0m Trial 419 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 539, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:00,069]\u001b[0m Trial 420 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 531, 'n_estimators': 254}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:04,097]\u001b[0m Trial 421 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 239, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:08,644]\u001b[0m Trial 422 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 673, 'n_estimators': 284}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:12,140]\u001b[0m Trial 423 finished with value: 0.9561756422291904 and parameters: {'max_depth': 3, 'max_leaf_nodes': 334, 'n_estimators': 210}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:16,426]\u001b[0m Trial 424 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 648, 'n_estimators': 274}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:20,445]\u001b[0m Trial 425 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 519, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:24,451]\u001b[0m Trial 426 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 531, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:28,588]\u001b[0m Trial 427 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 504, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:32,657]\u001b[0m Trial 428 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 412, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:36,826]\u001b[0m Trial 429 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 406, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:40,930]\u001b[0m Trial 430 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 391, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:44,846]\u001b[0m Trial 431 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 488, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:49,362]\u001b[0m Trial 432 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 971, 'n_estimators': 285}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:53,975]\u001b[0m Trial 433 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 456, 'n_estimators': 302}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:26:57,860]\u001b[0m Trial 434 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 832, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:02,089]\u001b[0m Trial 435 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 841, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:06,239]\u001b[0m Trial 436 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 561, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:10,153]\u001b[0m Trial 437 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 474, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:14,160]\u001b[0m Trial 438 finished with value: 0.9562755524045846 and parameters: {'max_depth': 3, 'max_leaf_nodes': 479, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:17,963]\u001b[0m Trial 439 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 828, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:22,635]\u001b[0m Trial 440 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 302, 'n_estimators': 284}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:26,520]\u001b[0m Trial 441 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 285, 'n_estimators': 240}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:30,423]\u001b[0m Trial 442 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 686, 'n_estimators': 240}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:34,378]\u001b[0m Trial 443 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 468, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:38,251]\u001b[0m Trial 444 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 466, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:42,232]\u001b[0m Trial 445 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 334, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:46,190]\u001b[0m Trial 446 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 348, 'n_estimators': 227}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:50,652]\u001b[0m Trial 447 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 908, 'n_estimators': 296}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:54,455]\u001b[0m Trial 448 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 863, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:27:58,255]\u001b[0m Trial 449 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 858, 'n_estimators': 223}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:01,955]\u001b[0m Trial 450 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 846, 'n_estimators': 228}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:05,943]\u001b[0m Trial 451 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 981, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:10,025]\u001b[0m Trial 452 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 993, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:14,049]\u001b[0m Trial 453 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 881, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:17,926]\u001b[0m Trial 454 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 856, 'n_estimators': 235}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:21,936]\u001b[0m Trial 455 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 958, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:27,127]\u001b[0m Trial 456 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 266, 'n_estimators': 351}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:31,589]\u001b[0m Trial 457 finished with value: 0.962628796368179 and parameters: {'max_depth': 3, 'max_leaf_nodes': 561, 'n_estimators': 298}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:35,852]\u001b[0m Trial 458 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 788, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:39,778]\u001b[0m Trial 459 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 823, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:43,751]\u001b[0m Trial 460 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 438, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:47,391]\u001b[0m Trial 461 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 710, 'n_estimators': 209}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:51,397]\u001b[0m Trial 462 finished with value: 0.970096618357488 and parameters: {'max_depth': 9, 'max_leaf_nodes': 982, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:55,108]\u001b[0m Trial 463 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 930, 'n_estimators': 216}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:28:59,038]\u001b[0m Trial 464 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 948, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:03,282]\u001b[0m Trial 465 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 53, 'n_estimators': 270}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:07,407]\u001b[0m Trial 466 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 804, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:11,286]\u001b[0m Trial 467 finished with value: 0.9445900468988544 and parameters: {'max_depth': 2, 'max_leaf_nodes': 788, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:15,117]\u001b[0m Trial 468 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 184, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:19,546]\u001b[0m Trial 469 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 130, 'n_estimators': 274}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:25,711]\u001b[0m Trial 470 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 615, 'n_estimators': 418}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:31,631]\u001b[0m Trial 471 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 159, 'n_estimators': 405}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:36,067]\u001b[0m Trial 472 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 675, 'n_estimators': 278}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:39,905]\u001b[0m Trial 473 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 433, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:43,809]\u001b[0m Trial 474 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 959, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:48,457]\u001b[0m Trial 475 finished with value: 0.9718467899429374 and parameters: {'max_depth': 5, 'max_leaf_nodes': 759, 'n_estimators': 287}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:52,291]\u001b[0m Trial 476 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 878, 'n_estimators': 222}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:29:56,731]\u001b[0m Trial 477 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 588, 'n_estimators': 292}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:01,410]\u001b[0m Trial 478 finished with value: 0.9718467899429374 and parameters: {'max_depth': 5, 'max_leaf_nodes': 727, 'n_estimators': 287}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:05,753]\u001b[0m Trial 479 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 945, 'n_estimators': 274}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:10,229]\u001b[0m Trial 480 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 935, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:14,828]\u001b[0m Trial 481 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 741, 'n_estimators': 290}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:18,757]\u001b[0m Trial 482 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 502, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:23,142]\u001b[0m Trial 483 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 550, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:27,745]\u001b[0m Trial 484 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 806, 'n_estimators': 301}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:31,703]\u001b[0m Trial 485 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 779, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:36,175]\u001b[0m Trial 486 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 632, 'n_estimators': 281}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:40,726]\u001b[0m Trial 487 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 546, 'n_estimators': 287}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:45,192]\u001b[0m Trial 488 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 809, 'n_estimators': 281}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:49,590]\u001b[0m Trial 489 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 805, 'n_estimators': 280}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:54,100]\u001b[0m Trial 490 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 834, 'n_estimators': 294}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:30:57,809]\u001b[0m Trial 491 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 868, 'n_estimators': 229}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:02,216]\u001b[0m Trial 492 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 418, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:06,710]\u001b[0m Trial 493 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 376, 'n_estimators': 282}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:09,832]\u001b[0m Trial 494 finished with value: 0.9538801599337081 and parameters: {'max_depth': 3, 'max_leaf_nodes': 424, 'n_estimators': 193}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:14,699]\u001b[0m Trial 495 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 739, 'n_estimators': 309}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:18,549]\u001b[0m Trial 496 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 578, 'n_estimators': 237}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:22,736]\u001b[0m Trial 497 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 962, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:26,924]\u001b[0m Trial 498 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 773, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:33,204]\u001b[0m Trial 499 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 363, 'n_estimators': 448}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:37,267]\u001b[0m Trial 500 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 534, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:41,181]\u001b[0m Trial 501 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 238, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:45,318]\u001b[0m Trial 502 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 503, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:50,726]\u001b[0m Trial 503 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 178, 'n_estimators': 367}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:54,646]\u001b[0m Trial 504 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 832, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:31:59,061]\u001b[0m Trial 505 finished with value: 0.9723921006529702 and parameters: {'max_depth': 10, 'max_leaf_nodes': 443, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:03,023]\u001b[0m Trial 506 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 489, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:07,695]\u001b[0m Trial 507 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 884, 'n_estimators': 290}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:12,435]\u001b[0m Trial 508 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 894, 'n_estimators': 320}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:16,381]\u001b[0m Trial 509 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 535, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:20,245]\u001b[0m Trial 510 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 330, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:24,677]\u001b[0m Trial 511 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 904, 'n_estimators': 278}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:29,132]\u001b[0m Trial 512 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 917, 'n_estimators': 282}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:33,723]\u001b[0m Trial 513 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 913, 'n_estimators': 302}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:37,801]\u001b[0m Trial 514 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 390, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:41,755]\u001b[0m Trial 515 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 506, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:46,150]\u001b[0m Trial 516 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 461, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:50,281]\u001b[0m Trial 517 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 356, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:54,163]\u001b[0m Trial 518 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 859, 'n_estimators': 236}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:32:58,323]\u001b[0m Trial 519 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 480, 'n_estimators': 243}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:02,148]\u001b[0m Trial 520 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 331, 'n_estimators': 254}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:05,868]\u001b[0m Trial 521 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 865, 'n_estimators': 230}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:09,695]\u001b[0m Trial 522 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 854, 'n_estimators': 222}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:13,547]\u001b[0m Trial 523 finished with value: 0.9718467899429374 and parameters: {'max_depth': 6, 'max_leaf_nodes': 850, 'n_estimators': 219}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:17,370]\u001b[0m Trial 524 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 300, 'n_estimators': 235}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:21,427]\u001b[0m Trial 525 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 430, 'n_estimators': 242}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:25,363]\u001b[0m Trial 526 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 427, 'n_estimators': 240}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:29,417]\u001b[0m Trial 527 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 615, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:33,360]\u001b[0m Trial 528 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 604, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:37,340]\u001b[0m Trial 529 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 997, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:40,937]\u001b[0m Trial 530 finished with value: 0.9123448390003495 and parameters: {'max_depth': 1, 'max_leaf_nodes': 985, 'n_estimators': 254}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:44,776]\u001b[0m Trial 531 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 265, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:48,836]\u001b[0m Trial 532 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 365, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:52,642]\u001b[0m Trial 533 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 699, 'n_estimators': 232}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:33:56,580]\u001b[0m Trial 534 finished with value: 0.9562755524045846 and parameters: {'max_depth': 3, 'max_leaf_nodes': 307, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:00,430]\u001b[0m Trial 535 finished with value: 0.9540782639520368 and parameters: {'max_depth': 3, 'max_leaf_nodes': 114, 'n_estimators': 225}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:04,149]\u001b[0m Trial 536 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 74, 'n_estimators': 228}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:08,464]\u001b[0m Trial 537 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 924, 'n_estimators': 274}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:12,970]\u001b[0m Trial 538 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 932, 'n_estimators': 296}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:16,670]\u001b[0m Trial 539 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 903, 'n_estimators': 216}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:20,790]\u001b[0m Trial 540 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 470, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:25,098]\u001b[0m Trial 541 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 33, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:29,408]\u001b[0m Trial 542 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 46, 'n_estimators': 274}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:34,081]\u001b[0m Trial 543 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 623, 'n_estimators': 291}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:38,531]\u001b[0m Trial 544 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 896, 'n_estimators': 292}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:42,505]\u001b[0m Trial 545 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 842, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:46,559]\u001b[0m Trial 546 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 345, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:50,511]\u001b[0m Trial 547 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 850, 'n_estimators': 242}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:54,400]\u001b[0m Trial 548 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 875, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:34:58,285]\u001b[0m Trial 549 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 887, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:02,243]\u001b[0m Trial 550 finished with value: 0.9562755524045846 and parameters: {'max_depth': 3, 'max_leaf_nodes': 320, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:06,359]\u001b[0m Trial 551 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 641, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:10,465]\u001b[0m Trial 552 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 886, 'n_estimators': 243}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:14,204]\u001b[0m Trial 553 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 666, 'n_estimators': 230}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:18,132]\u001b[0m Trial 554 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 658, 'n_estimators': 237}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:22,394]\u001b[0m Trial 555 finished with value: 0.9445900468988544 and parameters: {'max_depth': 2, 'max_leaf_nodes': 559, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:26,470]\u001b[0m Trial 556 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 364, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:30,285]\u001b[0m Trial 557 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 515, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:34,168]\u001b[0m Trial 558 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 492, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:38,130]\u001b[0m Trial 559 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 867, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:44,780]\u001b[0m Trial 560 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 515, 'n_estimators': 484}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:49,414]\u001b[0m Trial 561 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 778, 'n_estimators': 305}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:53,595]\u001b[0m Trial 562 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 523, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:35:58,054]\u001b[0m Trial 563 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 587, 'n_estimators': 284}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:02,448]\u001b[0m Trial 564 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 759, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:06,266]\u001b[0m Trial 565 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 490, 'n_estimators': 236}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:10,216]\u001b[0m Trial 566 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 289, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:14,372]\u001b[0m Trial 567 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 399, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:18,546]\u001b[0m Trial 568 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 402, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:23,126]\u001b[0m Trial 569 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 479, 'n_estimators': 289}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:27,609]\u001b[0m Trial 570 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 515, 'n_estimators': 286}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:32,002]\u001b[0m Trial 571 finished with value: 0.970096618357488 and parameters: {'max_depth': 9, 'max_leaf_nodes': 777, 'n_estimators': 273}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:36,577]\u001b[0m Trial 572 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 910, 'n_estimators': 302}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:40,701]\u001b[0m Trial 573 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 964, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:44,464]\u001b[0m Trial 574 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 490, 'n_estimators': 230}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:48,314]\u001b[0m Trial 575 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 808, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:52,074]\u001b[0m Trial 576 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 497, 'n_estimators': 221}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:55,914]\u001b[0m Trial 577 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 342, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:36:59,812]\u001b[0m Trial 578 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 546, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:03,216]\u001b[0m Trial 579 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 963, 'n_estimators': 202}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:07,256]\u001b[0m Trial 580 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 822, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:11,947]\u001b[0m Trial 581 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 446, 'n_estimators': 289}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:16,365]\u001b[0m Trial 582 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 124, 'n_estimators': 294}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:20,356]\u001b[0m Trial 583 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 788, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:24,279]\u001b[0m Trial 584 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 814, 'n_estimators': 254}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:28,255]\u001b[0m Trial 585 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 279, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:32,794]\u001b[0m Trial 586 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 686, 'n_estimators': 285}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:37,330]\u001b[0m Trial 587 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 249, 'n_estimators': 283}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:41,834]\u001b[0m Trial 588 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 580, 'n_estimators': 283}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:45,392]\u001b[0m Trial 589 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 459, 'n_estimators': 210}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:49,394]\u001b[0m Trial 590 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 603, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:54,041]\u001b[0m Trial 591 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 452, 'n_estimators': 299}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:37:57,963]\u001b[0m Trial 592 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 475, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:01,869]\u001b[0m Trial 593 finished with value: 0.9540782639520368 and parameters: {'max_depth': 3, 'max_leaf_nodes': 23, 'n_estimators': 226}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:05,623]\u001b[0m Trial 594 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 845, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:09,349]\u001b[0m Trial 595 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 374, 'n_estimators': 218}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:13,050]\u001b[0m Trial 596 finished with value: 0.9560885491320275 and parameters: {'max_depth': 3, 'max_leaf_nodes': 835, 'n_estimators': 229}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:16,759]\u001b[0m Trial 597 finished with value: 0.9052011370488415 and parameters: {'max_depth': 7, 'max_leaf_nodes': 2, 'n_estimators': 236}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:20,508]\u001b[0m Trial 598 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 438, 'n_estimators': 232}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:25,111]\u001b[0m Trial 599 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 858, 'n_estimators': 278}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:29,022]\u001b[0m Trial 600 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 576, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:36,016]\u001b[0m Trial 601 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 217, 'n_estimators': 499}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:39,775]\u001b[0m Trial 602 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 562, 'n_estimators': 230}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:43,682]\u001b[0m Trial 603 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 880, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:47,526]\u001b[0m Trial 604 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 868, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:51,438]\u001b[0m Trial 605 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 272, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:55,235]\u001b[0m Trial 606 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 292, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:38:59,144]\u001b[0m Trial 607 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 894, 'n_estimators': 225}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:03,060]\u001b[0m Trial 608 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 985, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:06,928]\u001b[0m Trial 609 finished with value: 0.9718467899429374 and parameters: {'max_depth': 6, 'max_leaf_nodes': 462, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:10,862]\u001b[0m Trial 610 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 418, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:14,517]\u001b[0m Trial 611 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 315, 'n_estimators': 215}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:18,504]\u001b[0m Trial 612 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 379, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:22,330]\u001b[0m Trial 613 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 445, 'n_estimators': 224}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:26,247]\u001b[0m Trial 614 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 364, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:30,057]\u001b[0m Trial 615 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 446, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:34,137]\u001b[0m Trial 616 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 964, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:38,472]\u001b[0m Trial 617 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 427, 'n_estimators': 276}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:42,290]\u001b[0m Trial 618 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 475, 'n_estimators': 233}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:45,025]\u001b[0m Trial 619 finished with value: 0.9650693641717485 and parameters: {'max_depth': 4, 'max_leaf_nodes': 505, 'n_estimators': 141}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:49,282]\u001b[0m Trial 620 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 950, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:53,871]\u001b[0m Trial 621 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 417, 'n_estimators': 287}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:39:58,617]\u001b[0m Trial 622 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 150, 'n_estimators': 313}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:01,680]\u001b[0m Trial 623 finished with value: 0.9538801599337081 and parameters: {'max_depth': 3, 'max_leaf_nodes': 636, 'n_estimators': 170}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:05,571]\u001b[0m Trial 624 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 483, 'n_estimators': 224}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:09,651]\u001b[0m Trial 625 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 605, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:13,401]\u001b[0m Trial 626 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 317, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:17,251]\u001b[0m Trial 627 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 669, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:21,196]\u001b[0m Trial 628 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 681, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:25,179]\u001b[0m Trial 629 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 295, 'n_estimators': 242}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:29,166]\u001b[0m Trial 630 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 247, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:32,684]\u001b[0m Trial 631 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 389, 'n_estimators': 207}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:36,889]\u001b[0m Trial 632 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 306, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:41,269]\u001b[0m Trial 633 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 957, 'n_estimators': 273}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:45,674]\u001b[0m Trial 634 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 948, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:49,789]\u001b[0m Trial 635 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 742, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:53,877]\u001b[0m Trial 636 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 998, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:40:58,370]\u001b[0m Trial 637 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 829, 'n_estimators': 281}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:02,880]\u001b[0m Trial 638 finished with value: 0.9718467899429374 and parameters: {'max_depth': 5, 'max_leaf_nodes': 932, 'n_estimators': 293}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:07,110]\u001b[0m Trial 639 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 59, 'n_estimators': 270}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:11,662]\u001b[0m Trial 640 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 589, 'n_estimators': 298}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:16,179]\u001b[0m Trial 641 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 792, 'n_estimators': 281}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:20,910]\u001b[0m Trial 642 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 800, 'n_estimators': 307}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:24,914]\u001b[0m Trial 643 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 571, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:28,955]\u001b[0m Trial 644 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 828, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:32,952]\u001b[0m Trial 645 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 568, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:36,989]\u001b[0m Trial 646 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 531, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:40,706]\u001b[0m Trial 647 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 568, 'n_estimators': 229}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:44,650]\u001b[0m Trial 648 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 537, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:49,150]\u001b[0m Trial 649 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 768, 'n_estimators': 285}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:53,590]\u001b[0m Trial 650 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 774, 'n_estimators': 284}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:41:58,202]\u001b[0m Trial 651 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 996, 'n_estimators': 298}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:02,147]\u001b[0m Trial 652 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 530, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:06,132]\u001b[0m Trial 653 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 417, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:10,036]\u001b[0m Trial 654 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 383, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:14,338]\u001b[0m Trial 655 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 71, 'n_estimators': 274}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:18,705]\u001b[0m Trial 656 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 113, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:23,092]\u001b[0m Trial 657 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 863, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:27,346]\u001b[0m Trial 658 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 871, 'n_estimators': 273}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:31,719]\u001b[0m Trial 659 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 98, 'n_estimators': 276}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:36,163]\u001b[0m Trial 660 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 196, 'n_estimators': 292}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:40,777]\u001b[0m Trial 661 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 886, 'n_estimators': 290}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:45,236]\u001b[0m Trial 662 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 726, 'n_estimators': 295}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:48,981]\u001b[0m Trial 663 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 448, 'n_estimators': 218}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:53,027]\u001b[0m Trial 664 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 822, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:42:57,532]\u001b[0m Trial 665 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 69, 'n_estimators': 281}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:02,018]\u001b[0m Trial 666 finished with value: 0.970096618357488 and parameters: {'max_depth': 8, 'max_leaf_nodes': 131, 'n_estimators': 278}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:05,822]\u001b[0m Trial 667 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 481, 'n_estimators': 233}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:09,603]\u001b[0m Trial 668 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 478, 'n_estimators': 232}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:13,791]\u001b[0m Trial 669 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 971, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:17,659]\u001b[0m Trial 670 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 979, 'n_estimators': 254}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:21,986]\u001b[0m Trial 671 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 779, 'n_estimators': 278}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:25,880]\u001b[0m Trial 672 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 436, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:30,127]\u001b[0m Trial 673 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 140, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:34,977]\u001b[0m Trial 674 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 766, 'n_estimators': 324}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:38,628]\u001b[0m Trial 675 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 399, 'n_estimators': 213}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:42,749]\u001b[0m Trial 676 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 992, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:46,835]\u001b[0m Trial 677 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 808, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:51,051]\u001b[0m Trial 678 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 753, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:55,022]\u001b[0m Trial 679 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 765, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:43:58,996]\u001b[0m Trial 680 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 794, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:02,894]\u001b[0m Trial 681 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 80, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:06,980]\u001b[0m Trial 682 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 820, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:10,689]\u001b[0m Trial 683 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 17, 'n_estimators': 229}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:14,696]\u001b[0m Trial 684 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 105, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:18,902]\u001b[0m Trial 685 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 796, 'n_estimators': 270}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:22,938]\u001b[0m Trial 686 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 762, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:27,498]\u001b[0m Trial 687 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 727, 'n_estimators': 286}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:32,122]\u001b[0m Trial 688 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 752, 'n_estimators': 290}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:36,471]\u001b[0m Trial 689 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 636, 'n_estimators': 276}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:40,615]\u001b[0m Trial 690 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 527, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:44,989]\u001b[0m Trial 691 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 836, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:48,777]\u001b[0m Trial 692 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 230, 'n_estimators': 221}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:53,330]\u001b[0m Trial 693 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 608, 'n_estimators': 283}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:44:57,245]\u001b[0m Trial 694 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 403, 'n_estimators': 242}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:01,322]\u001b[0m Trial 695 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 524, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:05,492]\u001b[0m Trial 696 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 509, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:09,436]\u001b[0m Trial 697 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 511, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:13,375]\u001b[0m Trial 698 finished with value: 0.9718467899429374 and parameters: {'max_depth': 6, 'max_leaf_nodes': 979, 'n_estimators': 235}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:17,245]\u001b[0m Trial 699 finished with value: 0.9540782639520368 and parameters: {'max_depth': 3, 'max_leaf_nodes': 565, 'n_estimators': 225}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:19,613]\u001b[0m Trial 700 finished with value: 0.9650693641717485 and parameters: {'max_depth': 4, 'max_leaf_nodes': 801, 'n_estimators': 112}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:23,614]\u001b[0m Trial 701 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 842, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:28,199]\u001b[0m Trial 702 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 469, 'n_estimators': 288}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:32,426]\u001b[0m Trial 703 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 542, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:36,590]\u001b[0m Trial 704 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 529, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:41,401]\u001b[0m Trial 705 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 500, 'n_estimators': 312}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:45,307]\u001b[0m Trial 706 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 433, 'n_estimators': 236}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:49,413]\u001b[0m Trial 707 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 35, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:53,381]\u001b[0m Trial 708 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 64, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:45:57,544]\u001b[0m Trial 709 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 508, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:01,637]\u001b[0m Trial 710 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 541, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:06,268]\u001b[0m Trial 711 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 355, 'n_estimators': 303}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:10,814]\u001b[0m Trial 712 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 615, 'n_estimators': 297}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:15,534]\u001b[0m Trial 713 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 650, 'n_estimators': 309}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:19,219]\u001b[0m Trial 714 finished with value: 0.9560885491320275 and parameters: {'max_depth': 3, 'max_leaf_nodes': 450, 'n_estimators': 229}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:23,524]\u001b[0m Trial 715 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 624, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:27,459]\u001b[0m Trial 716 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 518, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:31,485]\u001b[0m Trial 717 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 497, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:35,436]\u001b[0m Trial 718 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 521, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:39,239]\u001b[0m Trial 719 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 465, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:42,903]\u001b[0m Trial 720 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 416, 'n_estimators': 215}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:46,799]\u001b[0m Trial 721 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 299, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:51,106]\u001b[0m Trial 722 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 637, 'n_estimators': 273}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:55,679]\u001b[0m Trial 723 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 19, 'n_estimators': 289}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:46:59,903]\u001b[0m Trial 724 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 527, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:03,648]\u001b[0m Trial 725 finished with value: 0.9027591346068391 and parameters: {'max_depth': 1, 'max_leaf_nodes': 846, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:07,384]\u001b[0m Trial 726 finished with value: 0.9421930431535361 and parameters: {'max_depth': 2, 'max_leaf_nodes': 555, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:11,383]\u001b[0m Trial 727 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 543, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:15,780]\u001b[0m Trial 728 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 145, 'n_estimators': 281}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:20,297]\u001b[0m Trial 729 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 91, 'n_estimators': 285}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:24,757]\u001b[0m Trial 730 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 54, 'n_estimators': 293}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:28,779]\u001b[0m Trial 731 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 278, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:32,658]\u001b[0m Trial 732 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 822, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:36,661]\u001b[0m Trial 733 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 667, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:40,803]\u001b[0m Trial 734 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 660, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:44,730]\u001b[0m Trial 735 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 997, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:48,800]\u001b[0m Trial 736 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 973, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:52,718]\u001b[0m Trial 737 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 971, 'n_estimators': 243}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:47:57,104]\u001b[0m Trial 738 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 786, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:01,043]\u001b[0m Trial 739 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 249, 'n_estimators': 227}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:05,040]\u001b[0m Trial 740 finished with value: 0.970096618357488 and parameters: {'max_depth': 9, 'max_leaf_nodes': 345, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:09,354]\u001b[0m Trial 741 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 726, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:13,743]\u001b[0m Trial 742 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 749, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:17,629]\u001b[0m Trial 743 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 790, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:21,519]\u001b[0m Trial 744 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 495, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:25,474]\u001b[0m Trial 745 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 826, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:29,530]\u001b[0m Trial 746 finished with value: 0.9723921006529702 and parameters: {'max_depth': 10, 'max_leaf_nodes': 974, 'n_estimators': 240}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:34,580]\u001b[0m Trial 747 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 202, 'n_estimators': 343}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:38,470]\u001b[0m Trial 748 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 326, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:42,373]\u001b[0m Trial 749 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 378, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:46,337]\u001b[0m Trial 750 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 343, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:50,334]\u001b[0m Trial 751 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 355, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:54,438]\u001b[0m Trial 752 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 357, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:48:58,819]\u001b[0m Trial 753 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 380, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:03,394]\u001b[0m Trial 754 finished with value: 0.970096618357488 and parameters: {'max_depth': 7, 'max_leaf_nodes': 367, 'n_estimators': 280}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:07,187]\u001b[0m Trial 755 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 867, 'n_estimators': 233}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:11,125]\u001b[0m Trial 756 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 184, 'n_estimators': 243}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:16,671]\u001b[0m Trial 757 finished with value: 0.9606301599288976 and parameters: {'max_depth': 3, 'max_leaf_nodes': 288, 'n_estimators': 390}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:21,235]\u001b[0m Trial 758 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 93, 'n_estimators': 300}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:25,037]\u001b[0m Trial 759 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 852, 'n_estimators': 254}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:28,824]\u001b[0m Trial 760 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 867, 'n_estimators': 254}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:32,691]\u001b[0m Trial 761 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 859, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:36,606]\u001b[0m Trial 762 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 851, 'n_estimators': 257}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:40,586]\u001b[0m Trial 763 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 884, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:44,587]\u001b[0m Trial 764 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 42, 'n_estimators': 250}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:48,743]\u001b[0m Trial 765 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 940, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:55,051]\u001b[0m Trial 766 finished with value: 0.9718467899429374 and parameters: {'max_depth': 5, 'max_leaf_nodes': 958, 'n_estimators': 445}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:49:58,236]\u001b[0m Trial 767 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 420, 'n_estimators': 184}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:02,745]\u001b[0m Trial 768 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 248, 'n_estimators': 284}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:09,515]\u001b[0m Trial 769 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 945, 'n_estimators': 477}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:13,779]\u001b[0m Trial 770 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 954, 'n_estimators': 270}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:17,620]\u001b[0m Trial 771 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 323, 'n_estimators': 237}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:21,830]\u001b[0m Trial 772 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 551, 'n_estimators': 270}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:28,180]\u001b[0m Trial 773 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 950, 'n_estimators': 436}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:32,138]\u001b[0m Trial 774 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 897, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:35,611]\u001b[0m Trial 775 finished with value: 0.9672666526242966 and parameters: {'max_depth': 4, 'max_leaf_nodes': 576, 'n_estimators': 204}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:39,690]\u001b[0m Trial 776 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 539, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:44,077]\u001b[0m Trial 777 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 946, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:47,958]\u001b[0m Trial 778 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 649, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:54,089]\u001b[0m Trial 779 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 935, 'n_estimators': 429}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:50:57,916]\u001b[0m Trial 780 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 588, 'n_estimators': 220}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:01,805]\u001b[0m Trial 781 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 248, 'n_estimators': 225}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:05,685]\u001b[0m Trial 782 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 793, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:09,581]\u001b[0m Trial 783 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 772, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:13,518]\u001b[0m Trial 784 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 485, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:18,805]\u001b[0m Trial 785 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 825, 'n_estimators': 363}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:24,154]\u001b[0m Trial 786 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 812, 'n_estimators': 380}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:28,784]\u001b[0m Trial 787 finished with value: 0.9718467899429374 and parameters: {'max_depth': 5, 'max_leaf_nodes': 810, 'n_estimators': 288}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:32,595]\u001b[0m Trial 788 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 874, 'n_estimators': 235}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:38,777]\u001b[0m Trial 789 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 584, 'n_estimators': 418}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:45,253]\u001b[0m Trial 790 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 809, 'n_estimators': 456}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:49,149]\u001b[0m Trial 791 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 333, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:53,160]\u001b[0m Trial 792 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 281, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:51:57,075]\u001b[0m Trial 793 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 288, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:01,053]\u001b[0m Trial 794 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 311, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:05,620]\u001b[0m Trial 795 finished with value: 0.9718467899429374 and parameters: {'max_depth': 5, 'max_leaf_nodes': 261, 'n_estimators': 295}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:10,008]\u001b[0m Trial 796 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 165, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:13,764]\u001b[0m Trial 797 finished with value: 0.9562755524045846 and parameters: {'max_depth': 3, 'max_leaf_nodes': 917, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:18,459]\u001b[0m Trial 798 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 594, 'n_estimators': 302}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:22,402]\u001b[0m Trial 799 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 391, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:26,268]\u001b[0m Trial 800 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 394, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:30,940]\u001b[0m Trial 801 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 558, 'n_estimators': 291}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:34,718]\u001b[0m Trial 802 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 415, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:39,213]\u001b[0m Trial 803 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 587, 'n_estimators': 283}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:43,028]\u001b[0m Trial 804 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 412, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:47,616]\u001b[0m Trial 805 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 972, 'n_estimators': 282}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:51,865]\u001b[0m Trial 806 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 438, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:52:56,424]\u001b[0m Trial 807 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 996, 'n_estimators': 296}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:01,069]\u001b[0m Trial 808 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 427, 'n_estimators': 303}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:05,463]\u001b[0m Trial 809 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 453, 'n_estimators': 284}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:09,308]\u001b[0m Trial 810 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 700, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:13,760]\u001b[0m Trial 811 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 1000, 'n_estimators': 282}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:18,231]\u001b[0m Trial 812 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 563, 'n_estimators': 280}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:22,305]\u001b[0m Trial 813 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 516, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:26,392]\u001b[0m Trial 814 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 514, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:30,996]\u001b[0m Trial 815 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 968, 'n_estimators': 288}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:35,373]\u001b[0m Trial 816 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 976, 'n_estimators': 278}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:39,923]\u001b[0m Trial 817 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 786, 'n_estimators': 297}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:44,796]\u001b[0m Trial 818 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 467, 'n_estimators': 311}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:48,559]\u001b[0m Trial 819 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 678, 'n_estimators': 218}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:52,658]\u001b[0m Trial 820 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 708, 'n_estimators': 220}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:53:56,460]\u001b[0m Trial 821 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 682, 'n_estimators': 233}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:01,122]\u001b[0m Trial 822 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 769, 'n_estimators': 290}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:05,510]\u001b[0m Trial 823 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 778, 'n_estimators': 278}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:10,033]\u001b[0m Trial 824 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 822, 'n_estimators': 285}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:15,649]\u001b[0m Trial 825 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 955, 'n_estimators': 391}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:19,511]\u001b[0m Trial 826 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 342, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:24,103]\u001b[0m Trial 827 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 925, 'n_estimators': 287}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:28,638]\u001b[0m Trial 828 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 910, 'n_estimators': 293}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:32,881]\u001b[0m Trial 829 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 906, 'n_estimators': 276}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:36,924]\u001b[0m Trial 830 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 946, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:40,818]\u001b[0m Trial 831 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 488, 'n_estimators': 226}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:44,965]\u001b[0m Trial 832 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 120, 'n_estimators': 270}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:48,930]\u001b[0m Trial 833 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 979, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:52,657]\u001b[0m Trial 834 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 570, 'n_estimators': 230}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:54:56,999]\u001b[0m Trial 835 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 888, 'n_estimators': 276}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:01,228]\u001b[0m Trial 836 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 911, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:05,569]\u001b[0m Trial 837 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 902, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:10,058]\u001b[0m Trial 838 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 835, 'n_estimators': 283}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:14,241]\u001b[0m Trial 839 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 918, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:18,402]\u001b[0m Trial 840 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 401, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:22,492]\u001b[0m Trial 841 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 335, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:26,719]\u001b[0m Trial 842 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 372, 'n_estimators': 270}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:31,738]\u001b[0m Trial 843 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 959, 'n_estimators': 332}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:35,614]\u001b[0m Trial 844 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 313, 'n_estimators': 240}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:39,353]\u001b[0m Trial 845 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 30, 'n_estimators': 217}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:43,403]\u001b[0m Trial 846 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 656, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:47,967]\u001b[0m Trial 847 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 552, 'n_estimators': 281}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:52,565]\u001b[0m Trial 848 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 551, 'n_estimators': 287}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:55:56,690]\u001b[0m Trial 849 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 239, 'n_estimators': 244}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:01,299]\u001b[0m Trial 850 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 79, 'n_estimators': 298}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:05,836]\u001b[0m Trial 851 finished with value: 0.9028753387357764 and parameters: {'max_depth': 3, 'max_leaf_nodes': 2, 'n_estimators': 307}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:10,317]\u001b[0m Trial 852 finished with value: 0.970096618357488 and parameters: {'max_depth': 8, 'max_leaf_nodes': 894, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:14,307]\u001b[0m Trial 853 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 219, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:20,088]\u001b[0m Trial 854 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 921, 'n_estimators': 399}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:24,159]\u001b[0m Trial 855 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 1000, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:28,163]\u001b[0m Trial 856 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 462, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:32,603]\u001b[0m Trial 857 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 102, 'n_estimators': 292}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:36,600]\u001b[0m Trial 858 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 923, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:41,266]\u001b[0m Trial 859 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 66, 'n_estimators': 291}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:45,211]\u001b[0m Trial 860 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 436, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:49,175]\u001b[0m Trial 861 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 328, 'n_estimators': 254}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:53,046]\u001b[0m Trial 862 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 102, 'n_estimators': 225}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:56:59,300]\u001b[0m Trial 863 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 951, 'n_estimators': 418}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:03,229]\u001b[0m Trial 864 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 483, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:07,793]\u001b[0m Trial 865 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 39, 'n_estimators': 302}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:12,287]\u001b[0m Trial 866 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 121, 'n_estimators': 295}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:16,572]\u001b[0m Trial 867 finished with value: 0.9421930431535361 and parameters: {'max_depth': 2, 'max_leaf_nodes': 146, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:20,222]\u001b[0m Trial 868 finished with value: 0.9560885491320275 and parameters: {'max_depth': 3, 'max_leaf_nodes': 558, 'n_estimators': 229}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:26,683]\u001b[0m Trial 869 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 180, 'n_estimators': 470}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:30,624]\u001b[0m Trial 870 finished with value: 0.9562755524045846 and parameters: {'max_depth': 3, 'max_leaf_nodes': 844, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:34,870]\u001b[0m Trial 871 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 385, 'n_estimators': 270}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:38,960]\u001b[0m Trial 872 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 258, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:42,849]\u001b[0m Trial 873 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 422, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:46,797]\u001b[0m Trial 874 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 304, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:50,958]\u001b[0m Trial 875 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 203, 'n_estimators': 269}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:55,017]\u001b[0m Trial 876 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 320, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:57:58,551]\u001b[0m Trial 877 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 882, 'n_estimators': 207}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:04,510]\u001b[0m Trial 878 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 327, 'n_estimators': 427}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:08,617]\u001b[0m Trial 879 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 473, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:12,511]\u001b[0m Trial 880 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 274, 'n_estimators': 237}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:16,394]\u001b[0m Trial 881 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 299, 'n_estimators': 236}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:20,486]\u001b[0m Trial 882 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 290, 'n_estimators': 242}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:24,464]\u001b[0m Trial 883 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 314, 'n_estimators': 227}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:28,628]\u001b[0m Trial 884 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 759, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:32,493]\u001b[0m Trial 885 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 613, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:36,832]\u001b[0m Trial 886 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 731, 'n_estimators': 276}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:40,791]\u001b[0m Trial 887 finished with value: 0.9718467899429374 and parameters: {'max_depth': 6, 'max_leaf_nodes': 623, 'n_estimators': 223}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:44,898]\u001b[0m Trial 888 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 947, 'n_estimators': 245}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:49,459]\u001b[0m Trial 889 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 971, 'n_estimators': 286}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:54,273]\u001b[0m Trial 890 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 819, 'n_estimators': 286}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:58:58,948]\u001b[0m Trial 891 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 360, 'n_estimators': 304}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:02,904]\u001b[0m Trial 892 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 847, 'n_estimators': 261}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:06,977]\u001b[0m Trial 893 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 374, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:10,841]\u001b[0m Trial 894 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 972, 'n_estimators': 248}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:14,860]\u001b[0m Trial 895 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 859, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:18,867]\u001b[0m Trial 896 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 846, 'n_estimators': 260}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:22,448]\u001b[0m Trial 897 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 22, 'n_estimators': 209}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:26,930]\u001b[0m Trial 898 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 548, 'n_estimators': 282}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:31,218]\u001b[0m Trial 899 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 746, 'n_estimators': 273}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:35,289]\u001b[0m Trial 900 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 70, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:39,035]\u001b[0m Trial 901 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 117, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:43,700]\u001b[0m Trial 902 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 106, 'n_estimators': 310}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:47,874]\u001b[0m Trial 903 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 392, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:51,736]\u001b[0m Trial 904 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 611, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:55,634]\u001b[0m Trial 905 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 615, 'n_estimators': 239}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 07:59:59,541]\u001b[0m Trial 906 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 620, 'n_estimators': 225}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:04,105]\u001b[0m Trial 907 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 579, 'n_estimators': 298}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:08,426]\u001b[0m Trial 908 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 643, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:12,751]\u001b[0m Trial 909 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 606, 'n_estimators': 274}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:17,036]\u001b[0m Trial 910 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 631, 'n_estimators': 274}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:20,948]\u001b[0m Trial 911 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 892, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:24,830]\u001b[0m Trial 912 finished with value: 0.9562755524045846 and parameters: {'max_depth': 3, 'max_leaf_nodes': 658, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:28,781]\u001b[0m Trial 913 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 831, 'n_estimators': 243}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:32,797]\u001b[0m Trial 914 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 983, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:37,077]\u001b[0m Trial 915 finished with value: 0.970096618357488 and parameters: {'max_depth': 7, 'max_leaf_nodes': 931, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:41,172]\u001b[0m Trial 916 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 911, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:45,073]\u001b[0m Trial 917 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 675, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:48,862]\u001b[0m Trial 918 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 680, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:52,772]\u001b[0m Trial 919 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 657, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:00:56,789]\u001b[0m Trial 920 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 872, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:00,840]\u001b[0m Trial 921 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 706, 'n_estimators': 251}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:04,717]\u001b[0m Trial 922 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 889, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:09,139]\u001b[0m Trial 923 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 60, 'n_estimators': 277}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:13,586]\u001b[0m Trial 924 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 54, 'n_estimators': 280}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:18,253]\u001b[0m Trial 925 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 591, 'n_estimators': 290}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:22,649]\u001b[0m Trial 926 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 868, 'n_estimators': 282}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:26,533]\u001b[0m Trial 927 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 541, 'n_estimators': 224}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:31,309]\u001b[0m Trial 928 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 862, 'n_estimators': 314}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:36,725]\u001b[0m Trial 929 finished with value: 0.9739973275773461 and parameters: {'max_depth': 6, 'max_leaf_nodes': 991, 'n_estimators': 367}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:41,174]\u001b[0m Trial 930 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 92, 'n_estimators': 292}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:45,024]\u001b[0m Trial 931 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 43, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:48,885]\u001b[0m Trial 932 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 696, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:52,841]\u001b[0m Trial 933 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 344, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:01:56,605]\u001b[0m Trial 934 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 897, 'n_estimators': 253}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:00,525]\u001b[0m Trial 935 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 891, 'n_estimators': 240}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:04,440]\u001b[0m Trial 936 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 892, 'n_estimators': 241}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:08,542]\u001b[0m Trial 937 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 878, 'n_estimators': 265}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:12,354]\u001b[0m Trial 938 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 361, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:16,560]\u001b[0m Trial 939 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 87, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:20,524]\u001b[0m Trial 940 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 493, 'n_estimators': 247}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:25,808]\u001b[0m Trial 941 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 636, 'n_estimators': 382}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:29,664]\u001b[0m Trial 942 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 956, 'n_estimators': 238}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:33,483]\u001b[0m Trial 943 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 13, 'n_estimators': 224}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:37,911]\u001b[0m Trial 944 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 940, 'n_estimators': 296}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:42,028]\u001b[0m Trial 945 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 260, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:45,786]\u001b[0m Trial 946 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 808, 'n_estimators': 232}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:49,380]\u001b[0m Trial 947 finished with value: 0.9695122331307464 and parameters: {'max_depth': 4, 'max_leaf_nodes': 789, 'n_estimators': 211}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:53,974]\u001b[0m Trial 948 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 747, 'n_estimators': 289}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:02:57,776]\u001b[0m Trial 949 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 864, 'n_estimators': 234}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:01,588]\u001b[0m Trial 950 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 837, 'n_estimators': 235}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:06,094]\u001b[0m Trial 951 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 790, 'n_estimators': 287}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:11,256]\u001b[0m Trial 952 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 795, 'n_estimators': 343}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:15,844]\u001b[0m Trial 953 finished with value: 0.9718467899429374 and parameters: {'max_depth': 5, 'max_leaf_nodes': 597, 'n_estimators': 287}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:20,649]\u001b[0m Trial 954 finished with value: 0.973719806763285 and parameters: {'max_depth': 4, 'max_leaf_nodes': 930, 'n_estimators': 320}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:24,908]\u001b[0m Trial 955 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 563, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:28,531]\u001b[0m Trial 956 finished with value: 0.9123448390003495 and parameters: {'max_depth': 1, 'max_leaf_nodes': 506, 'n_estimators': 256}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:32,603]\u001b[0m Trial 957 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 694, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:36,824]\u001b[0m Trial 958 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 309, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:40,788]\u001b[0m Trial 959 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 366, 'n_estimators': 214}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:44,772]\u001b[0m Trial 960 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 512, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:48,836]\u001b[0m Trial 961 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 239, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:52,918]\u001b[0m Trial 962 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 707, 'n_estimators': 264}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:03:57,447]\u001b[0m Trial 963 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 500, 'n_estimators': 285}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:01,167]\u001b[0m Trial 964 finished with value: 0.9738600592177029 and parameters: {'max_depth': 4, 'max_leaf_nodes': 347, 'n_estimators': 228}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:05,108]\u001b[0m Trial 965 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 507, 'n_estimators': 249}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:09,185]\u001b[0m Trial 966 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 223, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:13,179]\u001b[0m Trial 967 finished with value: 0.9445900468988544 and parameters: {'max_depth': 2, 'max_leaf_nodes': 295, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:17,109]\u001b[0m Trial 968 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 408, 'n_estimators': 254}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:21,381]\u001b[0m Trial 969 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 513, 'n_estimators': 271}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:25,635]\u001b[0m Trial 970 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 527, 'n_estimators': 272}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:29,611]\u001b[0m Trial 971 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 492, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:33,675]\u001b[0m Trial 972 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 482, 'n_estimators': 263}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:37,769]\u001b[0m Trial 973 finished with value: 0.9603832158617289 and parameters: {'max_depth': 3, 'max_leaf_nodes': 471, 'n_estimators': 266}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:41,608]\u001b[0m Trial 974 finished with value: 0.9738600592177029 and parameters: {'max_depth': 5, 'max_leaf_nodes': 624, 'n_estimators': 218}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:45,644]\u001b[0m Trial 975 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 387, 'n_estimators': 262}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:49,803]\u001b[0m Trial 976 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 531, 'n_estimators': 267}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:54,272]\u001b[0m Trial 977 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 271, 'n_estimators': 280}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:04:58,187]\u001b[0m Trial 978 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 492, 'n_estimators': 255}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:02,155]\u001b[0m Trial 979 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 513, 'n_estimators': 259}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:06,587]\u001b[0m Trial 980 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 799, 'n_estimators': 279}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:10,865]\u001b[0m Trial 981 finished with value: 0.9582858375845753 and parameters: {'max_depth': 3, 'max_leaf_nodes': 404, 'n_estimators': 275}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:14,810]\u001b[0m Trial 982 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 527, 'n_estimators': 258}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:18,513]\u001b[0m Trial 983 finished with value: 0.9717095215832945 and parameters: {'max_depth': 4, 'max_leaf_nodes': 265, 'n_estimators': 217}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:23,606]\u001b[0m Trial 984 finished with value: 0.970096618357488 and parameters: {'max_depth': 9, 'max_leaf_nodes': 861, 'n_estimators': 327}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:26,048]\u001b[0m Trial 985 finished with value: 0.967498963855981 and parameters: {'max_depth': 5, 'max_leaf_nodes': 207, 'n_estimators': 128}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:30,201]\u001b[0m Trial 986 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 542, 'n_estimators': 268}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:34,111]\u001b[0m Trial 987 finished with value: 0.9716627707651551 and parameters: {'max_depth': 4, 'max_leaf_nodes': 229, 'n_estimators': 224}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:38,409]\u001b[0m Trial 988 finished with value: 0.9739973275773461 and parameters: {'max_depth': 5, 'max_leaf_nodes': 376, 'n_estimators': 270}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:42,526]\u001b[0m Trial 989 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 769, 'n_estimators': 246}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:46,824]\u001b[0m Trial 990 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 750, 'n_estimators': 274}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:50,663]\u001b[0m Trial 991 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 173, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:54,300]\u001b[0m Trial 992 finished with value: 0.9583729306817382 and parameters: {'max_depth': 3, 'max_leaf_nodes': 844, 'n_estimators': 252}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:05:58,087]\u001b[0m Trial 993 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 43, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:06:01,850]\u001b[0m Trial 994 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 266, 'n_estimators': 231}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:06:05,861]\u001b[0m Trial 995 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 284, 'n_estimators': 242}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:06:09,591]\u001b[0m Trial 996 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 276, 'n_estimators': 229}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:06:13,468]\u001b[0m Trial 997 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 242, 'n_estimators': 236}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:06:17,563]\u001b[0m Trial 998 finished with value: 0.9760105968521117 and parameters: {'max_depth': 4, 'max_leaf_nodes': 247, 'n_estimators': 243}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 08:06:22,810]\u001b[0m Trial 999 finished with value: 0.9760105968521117 and parameters: {'max_depth': 5, 'max_leaf_nodes': 255, 'n_estimators': 354}. Best is trial 7 with value: 0.9760105968521117.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_RF = RandomForestClassifier(n_estimators=263, max_depth=4, max_leaf_nodes=219, random_state=25)\n",
        "optimized_RF.fit(X_over, y_over)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oEhaQjYRZ-R",
        "outputId": "391a01d9-8052-4915-9b1e-7343cff3a5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(max_depth=4, max_leaf_nodes=219, n_estimators=263,\n",
              "                       random_state=25)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds = optimized_RF.predict(X_over)\n",
        "f1 = f1_score(train_preds, y_over)\n",
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj7SKZ6xRgga",
        "outputId": "219fdad8-f02d-41d7-8863-865ff319503c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9848812095032397"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('test_bedCount범주_standard.csv')"
      ],
      "metadata": {
        "id": "jHAYglb6RzAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test.drop(['OC'],axis=1)\n",
        "test = test[['openYear', 'salescost1', 'interest1', 'ctax1', 'profit1', 'quickAsset1', 'inventoryAsset1', 'OnonCAsset1', 'liquidLiabilities1', 'NCLiabilities1', 'netAsset1', 'surplus1', 'inventoryAsset2', 'nonCAsset2', 'tanAsset2', 'OnonCAsset2', 'surplus2', 'employee1', '평균_당좌비율', '당좌비율_증감', '평균_총자본회전율', '총자본회전율_증감', 'sido', 'ownerChange_same']]"
      ],
      "metadata": {
        "id": "Ajcs1cXaTo81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_preds = optimized_RF.predict(test)"
      ],
      "metadata": {
        "id": "c5pzCQ8aRphd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_RF.predict_proba(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuT4uVPDWilb",
        "outputId": "865ca30f-172e-4dff-e54e-cf77f2e2a8f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02000943, 0.97999057],\n",
              "       [0.08352129, 0.91647871],\n",
              "       [0.11614146, 0.88385854],\n",
              "       [0.14737963, 0.85262037],\n",
              "       [0.05890695, 0.94109305],\n",
              "       [0.07662406, 0.92337594],\n",
              "       [0.08791205, 0.91208795],\n",
              "       [0.0695891 , 0.9304109 ],\n",
              "       [0.26276312, 0.73723688],\n",
              "       [0.01555606, 0.98444394],\n",
              "       [0.01837015, 0.98162985],\n",
              "       [0.14601489, 0.85398511],\n",
              "       [0.11216044, 0.88783956],\n",
              "       [0.17204924, 0.82795076],\n",
              "       [0.04193039, 0.95806961],\n",
              "       [0.09293768, 0.90706232],\n",
              "       [0.10100233, 0.89899767],\n",
              "       [0.16020651, 0.83979349],\n",
              "       [0.09846561, 0.90153439],\n",
              "       [0.02254797, 0.97745203],\n",
              "       [0.28143422, 0.71856578],\n",
              "       [0.13207445, 0.86792555],\n",
              "       [0.09127484, 0.90872516],\n",
              "       [0.21187454, 0.78812546],\n",
              "       [0.06689009, 0.93310991],\n",
              "       [0.42770315, 0.57229685],\n",
              "       [0.02658629, 0.97341371],\n",
              "       [0.22789999, 0.77210001],\n",
              "       [0.42983487, 0.57016513],\n",
              "       [0.05128678, 0.94871322],\n",
              "       [0.41869299, 0.58130701],\n",
              "       [0.28261553, 0.71738447],\n",
              "       [0.0433852 , 0.9566148 ],\n",
              "       [0.1293035 , 0.8706965 ],\n",
              "       [0.28717913, 0.71282087],\n",
              "       [0.08552637, 0.91447363],\n",
              "       [0.27557695, 0.72442305],\n",
              "       [0.23816238, 0.76183762],\n",
              "       [0.07449045, 0.92550955],\n",
              "       [0.15286493, 0.84713507],\n",
              "       [0.34716673, 0.65283327],\n",
              "       [0.13146867, 0.86853133],\n",
              "       [0.26761275, 0.73238725],\n",
              "       [0.10469362, 0.89530638],\n",
              "       [0.08978698, 0.91021302],\n",
              "       [0.15249798, 0.84750202],\n",
              "       [0.16487697, 0.83512303],\n",
              "       [0.20607822, 0.79392178],\n",
              "       [0.31638529, 0.68361471],\n",
              "       [0.10038357, 0.89961643],\n",
              "       [0.02405623, 0.97594377],\n",
              "       [0.05365243, 0.94634757],\n",
              "       [0.20637176, 0.79362824],\n",
              "       [0.22750389, 0.77249611],\n",
              "       [0.3486932 , 0.6513068 ],\n",
              "       [0.06932871, 0.93067129],\n",
              "       [0.09520423, 0.90479577],\n",
              "       [0.20255925, 0.79744075],\n",
              "       [0.24564099, 0.75435901],\n",
              "       [0.37026297, 0.62973703],\n",
              "       [0.31191812, 0.68808188],\n",
              "       [0.19233366, 0.80766634],\n",
              "       [0.50699611, 0.49300389],\n",
              "       [0.37977151, 0.62022849],\n",
              "       [0.09954131, 0.90045869],\n",
              "       [0.03197295, 0.96802705],\n",
              "       [0.06182971, 0.93817029],\n",
              "       [0.19391181, 0.80608819],\n",
              "       [0.14457676, 0.85542324],\n",
              "       [0.39720512, 0.60279488],\n",
              "       [0.02213465, 0.97786535],\n",
              "       [0.13324934, 0.86675066],\n",
              "       [0.13324934, 0.86675066],\n",
              "       [0.07871133, 0.92128867],\n",
              "       [0.04439556, 0.95560444],\n",
              "       [0.03793333, 0.96206667],\n",
              "       [0.05097027, 0.94902973],\n",
              "       [0.03557   , 0.96443   ],\n",
              "       [0.26794423, 0.73205577],\n",
              "       [0.17170922, 0.82829078],\n",
              "       [0.06109473, 0.93890527],\n",
              "       [0.03901694, 0.96098306],\n",
              "       [0.097343  , 0.902657  ],\n",
              "       [0.37172321, 0.62827679],\n",
              "       [0.30138145, 0.69861855],\n",
              "       [0.137178  , 0.862822  ],\n",
              "       [0.0609829 , 0.9390171 ],\n",
              "       [0.02918899, 0.97081101],\n",
              "       [0.13944205, 0.86055795],\n",
              "       [0.73034401, 0.26965599],\n",
              "       [0.03935747, 0.96064253],\n",
              "       [0.35257512, 0.64742488],\n",
              "       [0.04711804, 0.95288196],\n",
              "       [0.02687728, 0.97312272],\n",
              "       [0.27140611, 0.72859389],\n",
              "       [0.01935498, 0.98064502],\n",
              "       [0.14896295, 0.85103705],\n",
              "       [0.06104028, 0.93895972],\n",
              "       [0.35462568, 0.64537432],\n",
              "       [0.20749788, 0.79250212],\n",
              "       [0.18012315, 0.81987685],\n",
              "       [0.35770683, 0.64229317],\n",
              "       [0.1591939 , 0.8408061 ],\n",
              "       [0.11165567, 0.88834433],\n",
              "       [0.13280791, 0.86719209],\n",
              "       [0.38842807, 0.61157193],\n",
              "       [0.11862524, 0.88137476],\n",
              "       [0.05472907, 0.94527093],\n",
              "       [0.14658864, 0.85341136],\n",
              "       [0.07231492, 0.92768508],\n",
              "       [0.21135716, 0.78864284],\n",
              "       [0.12559611, 0.87440389],\n",
              "       [0.13700169, 0.86299831],\n",
              "       [0.09075054, 0.90924946],\n",
              "       [0.24638455, 0.75361545],\n",
              "       [0.32911939, 0.67088061],\n",
              "       [0.18701299, 0.81298701],\n",
              "       [0.12334331, 0.87665669],\n",
              "       [0.3053226 , 0.6946774 ],\n",
              "       [0.2111799 , 0.7888201 ],\n",
              "       [0.12467629, 0.87532371],\n",
              "       [0.30853324, 0.69146676],\n",
              "       [0.70004875, 0.29995125],\n",
              "       [0.10674226, 0.89325774],\n",
              "       [0.26274611, 0.73725389],\n",
              "       [0.17153763, 0.82846237],\n",
              "       [0.28695442, 0.71304558]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZahwMiOYRwnb",
        "outputId": "88134900-dd0e-44b9-9396-4825ca06f8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv('submission_sample.csv')\n",
        "submission['OC'] = submission_preds"
      ],
      "metadata": {
        "id": "Vm35NOm9T9I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission[submission['OC']==0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "kdRCAoQfUXZS",
        "outputId": "42f4804a-b1df-4e30-8b4a-661934c2b2d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-77ef0477-2bdb-4c22-8d1f-af5e19237dc3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inst_id</th>\n",
              "      <th>OC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>198</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>294</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>424</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-77ef0477-2bdb-4c22-8d1f-af5e19237dc3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-77ef0477-2bdb-4c22-8d1f-af5e19237dc3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-77ef0477-2bdb-4c22-8d1f-af5e19237dc3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     inst_id  OC\n",
              "62       198   0\n",
              "89       294   0\n",
              "122      424   0"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "submission.to_csv('submission_final1.csv')"
      ],
      "metadata": {
        "id": "zXqzYgVKUYlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "zEtaQtazdd-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "def KNN_objective(trial):\n",
        "    ## Setting parameter \n",
        "    n_neighbors = trial.suggest_int('n_neighbors', 1, 31)\n",
        "    algorithm = trial.suggest_categorical('algorithm', ['ball_tree', 'kd_tree'])\n",
        "    p = trial.suggest_categorical('p', [1, 2])\n",
        "    \n",
        "    ## initialize the model\n",
        "    model = KNeighborsClassifier(weights = 'distance', \n",
        "                             n_neighbors = n_neighbors, \n",
        "                             algorithm = algorithm, \n",
        "                             p = p)\n",
        "    \n",
        "    model.fit(X_over, y_over)    \n",
        "    score = cross_val_score(model, X_over, y_over, cv=5, scoring=\"f1\")\n",
        "    f1_mean = score.mean()\n",
        "\n",
        "    return f1_mean"
      ],
      "metadata": {
        "id": "_uP91S6uVbLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KNN_study = optuna.create_study(direction='maximize')\n",
        "KNN_study.optimize(KNN_objective, n_trials=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vC92_gmadT9",
        "outputId": "00425a8d-3d20-43ba-bc41-fce2b4e3419f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-01 12:18:08,857]\u001b[0m A new study created in memory with name: no-name-32bc8736-518e-4bd6-a0ce-285ebbae8870\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:08,965]\u001b[0m Trial 0 finished with value: 0.901529297069225 and parameters: {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 0 with value: 0.901529297069225.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:09,070]\u001b[0m Trial 1 finished with value: 0.8584495279593319 and parameters: {'n_neighbors': 17, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 0 with value: 0.901529297069225.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:09,182]\u001b[0m Trial 2 finished with value: 0.8557599980392568 and parameters: {'n_neighbors': 28, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 0 with value: 0.901529297069225.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:09,294]\u001b[0m Trial 3 finished with value: 0.8420534688121863 and parameters: {'n_neighbors': 30, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 0 with value: 0.901529297069225.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:09,406]\u001b[0m Trial 4 finished with value: 0.8827734824917839 and parameters: {'n_neighbors': 15, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 0 with value: 0.901529297069225.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:09,517]\u001b[0m Trial 5 finished with value: 0.8363217614951133 and parameters: {'n_neighbors': 29, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 0 with value: 0.901529297069225.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:09,620]\u001b[0m Trial 6 finished with value: 0.8667530547018496 and parameters: {'n_neighbors': 13, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 0 with value: 0.901529297069225.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:09,737]\u001b[0m Trial 7 finished with value: 0.8693248741272942 and parameters: {'n_neighbors': 14, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 0 with value: 0.901529297069225.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:09,848]\u001b[0m Trial 8 finished with value: 0.8306582583406705 and parameters: {'n_neighbors': 27, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 0 with value: 0.901529297069225.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:09,958]\u001b[0m Trial 9 finished with value: 0.8751762687867346 and parameters: {'n_neighbors': 11, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 0 with value: 0.901529297069225.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:10,081]\u001b[0m Trial 10 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:10,193]\u001b[0m Trial 11 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:10,304]\u001b[0m Trial 12 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:10,416]\u001b[0m Trial 13 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:10,528]\u001b[0m Trial 14 finished with value: 0.9012222679066951 and parameters: {'n_neighbors': 7, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:10,661]\u001b[0m Trial 15 finished with value: 0.8807008059598174 and parameters: {'n_neighbors': 21, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:10,775]\u001b[0m Trial 16 finished with value: 0.9012222679066951 and parameters: {'n_neighbors': 7, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:10,884]\u001b[0m Trial 17 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:10,991]\u001b[0m Trial 18 finished with value: 0.8935046981201596 and parameters: {'n_neighbors': 9, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:11,104]\u001b[0m Trial 19 finished with value: 0.8830230539385229 and parameters: {'n_neighbors': 19, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:11,223]\u001b[0m Trial 20 finished with value: 0.8698467152785845 and parameters: {'n_neighbors': 23, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:11,329]\u001b[0m Trial 21 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:11,437]\u001b[0m Trial 22 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:11,542]\u001b[0m Trial 23 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:11,652]\u001b[0m Trial 24 finished with value: 0.9012222679066951 and parameters: {'n_neighbors': 7, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:11,767]\u001b[0m Trial 25 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:11,873]\u001b[0m Trial 26 finished with value: 0.9009896232655905 and parameters: {'n_neighbors': 10, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:11,972]\u001b[0m Trial 27 finished with value: 0.9069791888996533 and parameters: {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:12,078]\u001b[0m Trial 28 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:12,179]\u001b[0m Trial 29 finished with value: 0.9012222679066951 and parameters: {'n_neighbors': 7, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:12,290]\u001b[0m Trial 30 finished with value: 0.9069791888996533 and parameters: {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:12,399]\u001b[0m Trial 31 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:12,511]\u001b[0m Trial 32 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:12,615]\u001b[0m Trial 33 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:12,722]\u001b[0m Trial 34 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:12,857]\u001b[0m Trial 35 finished with value: 0.9069791888996533 and parameters: {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:12,970]\u001b[0m Trial 36 finished with value: 0.8935046981201596 and parameters: {'n_neighbors': 9, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:13,083]\u001b[0m Trial 37 finished with value: 0.9192087621565479 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:13,184]\u001b[0m Trial 38 finished with value: 0.9121271702896816 and parameters: {'n_neighbors': 6, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:13,292]\u001b[0m Trial 39 finished with value: 0.8584495279593319 and parameters: {'n_neighbors': 16, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:13,412]\u001b[0m Trial 40 finished with value: 0.8908653045864753 and parameters: {'n_neighbors': 11, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:13,517]\u001b[0m Trial 41 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:13,638]\u001b[0m Trial 42 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:13,745]\u001b[0m Trial 43 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:13,874]\u001b[0m Trial 44 finished with value: 0.9192087621565479 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:13,987]\u001b[0m Trial 45 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:14,093]\u001b[0m Trial 46 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:14,203]\u001b[0m Trial 47 finished with value: 0.9121271702896816 and parameters: {'n_neighbors': 6, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:14,322]\u001b[0m Trial 48 finished with value: 0.8306582583406705 and parameters: {'n_neighbors': 27, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:14,430]\u001b[0m Trial 49 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:14,541]\u001b[0m Trial 50 finished with value: 0.8882322513651868 and parameters: {'n_neighbors': 12, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:14,648]\u001b[0m Trial 51 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:14,757]\u001b[0m Trial 52 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:14,871]\u001b[0m Trial 53 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:14,975]\u001b[0m Trial 54 finished with value: 0.9069791888996533 and parameters: {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:15,082]\u001b[0m Trial 55 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:15,188]\u001b[0m Trial 56 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:15,288]\u001b[0m Trial 57 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:15,394]\u001b[0m Trial 58 finished with value: 0.9458287415314717 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:15,501]\u001b[0m Trial 59 finished with value: 0.9085909640133061 and parameters: {'n_neighbors': 8, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:15,620]\u001b[0m Trial 60 finished with value: 0.8444817877524352 and parameters: {'n_neighbors': 31, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:15,742]\u001b[0m Trial 61 finished with value: 0.8698467152785845 and parameters: {'n_neighbors': 23, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:15,861]\u001b[0m Trial 62 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:15,967]\u001b[0m Trial 63 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:16,072]\u001b[0m Trial 64 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:16,177]\u001b[0m Trial 65 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:16,288]\u001b[0m Trial 66 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:16,408]\u001b[0m Trial 67 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:16,519]\u001b[0m Trial 68 finished with value: 0.9121271702896816 and parameters: {'n_neighbors': 6, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:16,639]\u001b[0m Trial 69 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:16,753]\u001b[0m Trial 70 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:16,877]\u001b[0m Trial 71 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:16,992]\u001b[0m Trial 72 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:17,106]\u001b[0m Trial 73 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:17,219]\u001b[0m Trial 74 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:17,332]\u001b[0m Trial 75 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:17,449]\u001b[0m Trial 76 finished with value: 0.9069791888996533 and parameters: {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:17,563]\u001b[0m Trial 77 finished with value: 0.8751762687867348 and parameters: {'n_neighbors': 18, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:17,667]\u001b[0m Trial 78 finished with value: 0.9192087621565479 and parameters: {'n_neighbors': 4, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:17,773]\u001b[0m Trial 79 finished with value: 0.9121271702896816 and parameters: {'n_neighbors': 6, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:17,885]\u001b[0m Trial 80 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:18,001]\u001b[0m Trial 81 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:18,109]\u001b[0m Trial 82 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:18,217]\u001b[0m Trial 83 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:18,322]\u001b[0m Trial 84 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:18,430]\u001b[0m Trial 85 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:18,536]\u001b[0m Trial 86 finished with value: 0.9069791888996533 and parameters: {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:18,642]\u001b[0m Trial 87 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:18,754]\u001b[0m Trial 88 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:18,860]\u001b[0m Trial 89 finished with value: 0.9458287415314717 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:18,976]\u001b[0m Trial 90 finished with value: 0.8827734824917839 and parameters: {'n_neighbors': 15, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:19,088]\u001b[0m Trial 91 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:19,204]\u001b[0m Trial 92 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:19,309]\u001b[0m Trial 93 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:19,417]\u001b[0m Trial 94 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:19,522]\u001b[0m Trial 95 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:19,636]\u001b[0m Trial 96 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:19,755]\u001b[0m Trial 97 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:19,866]\u001b[0m Trial 98 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:19,986]\u001b[0m Trial 99 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:20,111]\u001b[0m Trial 100 finished with value: 0.8807008059598174 and parameters: {'n_neighbors': 21, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:20,221]\u001b[0m Trial 101 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:20,335]\u001b[0m Trial 102 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:20,444]\u001b[0m Trial 103 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:20,565]\u001b[0m Trial 104 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:20,680]\u001b[0m Trial 105 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:20,790]\u001b[0m Trial 106 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:20,900]\u001b[0m Trial 107 finished with value: 0.9458287415314717 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:21,027]\u001b[0m Trial 108 finished with value: 0.9069791888996533 and parameters: {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:21,125]\u001b[0m Trial 109 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:21,229]\u001b[0m Trial 110 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:21,327]\u001b[0m Trial 111 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:21,428]\u001b[0m Trial 112 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:21,533]\u001b[0m Trial 113 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:21,634]\u001b[0m Trial 114 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:21,737]\u001b[0m Trial 115 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:21,843]\u001b[0m Trial 116 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:21,947]\u001b[0m Trial 117 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:22,069]\u001b[0m Trial 118 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:22,178]\u001b[0m Trial 119 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:22,290]\u001b[0m Trial 120 finished with value: 0.8614235011936995 and parameters: {'n_neighbors': 27, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:22,393]\u001b[0m Trial 121 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:22,502]\u001b[0m Trial 122 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:22,611]\u001b[0m Trial 123 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:22,721]\u001b[0m Trial 124 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:22,822]\u001b[0m Trial 125 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:22,932]\u001b[0m Trial 126 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:23,044]\u001b[0m Trial 127 finished with value: 0.8827052783291531 and parameters: {'n_neighbors': 13, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:23,170]\u001b[0m Trial 128 finished with value: 0.9192087621565479 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:23,284]\u001b[0m Trial 129 finished with value: 0.9069791888996533 and parameters: {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:23,398]\u001b[0m Trial 130 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:23,566]\u001b[0m Trial 131 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:23,689]\u001b[0m Trial 132 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:23,798]\u001b[0m Trial 133 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:23,907]\u001b[0m Trial 134 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:24,019]\u001b[0m Trial 135 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:24,140]\u001b[0m Trial 136 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:24,254]\u001b[0m Trial 137 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:24,364]\u001b[0m Trial 138 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:24,470]\u001b[0m Trial 139 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:24,577]\u001b[0m Trial 140 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:24,677]\u001b[0m Trial 141 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:24,785]\u001b[0m Trial 142 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:24,890]\u001b[0m Trial 143 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:24,969]\u001b[0m Trial 144 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,039]\u001b[0m Trial 145 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,103]\u001b[0m Trial 146 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,173]\u001b[0m Trial 147 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,235]\u001b[0m Trial 148 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,298]\u001b[0m Trial 149 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,363]\u001b[0m Trial 150 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,426]\u001b[0m Trial 151 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,490]\u001b[0m Trial 152 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,558]\u001b[0m Trial 153 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,622]\u001b[0m Trial 154 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,690]\u001b[0m Trial 155 finished with value: 0.9192087621565479 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,753]\u001b[0m Trial 156 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,814]\u001b[0m Trial 157 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,874]\u001b[0m Trial 158 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,934]\u001b[0m Trial 159 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:25,998]\u001b[0m Trial 160 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,064]\u001b[0m Trial 161 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,124]\u001b[0m Trial 162 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,193]\u001b[0m Trial 163 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,254]\u001b[0m Trial 164 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,318]\u001b[0m Trial 165 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,380]\u001b[0m Trial 166 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,448]\u001b[0m Trial 167 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,511]\u001b[0m Trial 168 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,571]\u001b[0m Trial 169 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,639]\u001b[0m Trial 170 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,705]\u001b[0m Trial 171 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,771]\u001b[0m Trial 172 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,836]\u001b[0m Trial 173 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,904]\u001b[0m Trial 174 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:26,966]\u001b[0m Trial 175 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,033]\u001b[0m Trial 176 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,099]\u001b[0m Trial 177 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,166]\u001b[0m Trial 178 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,234]\u001b[0m Trial 179 finished with value: 0.8698467152785845 and parameters: {'n_neighbors': 25, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,301]\u001b[0m Trial 180 finished with value: 0.9192087621565479 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,365]\u001b[0m Trial 181 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,428]\u001b[0m Trial 182 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,490]\u001b[0m Trial 183 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,554]\u001b[0m Trial 184 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,618]\u001b[0m Trial 185 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,682]\u001b[0m Trial 186 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,742]\u001b[0m Trial 187 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,810]\u001b[0m Trial 188 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,873]\u001b[0m Trial 189 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:27,935]\u001b[0m Trial 190 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,000]\u001b[0m Trial 191 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,072]\u001b[0m Trial 192 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,137]\u001b[0m Trial 193 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,211]\u001b[0m Trial 194 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,277]\u001b[0m Trial 195 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,350]\u001b[0m Trial 196 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,418]\u001b[0m Trial 197 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,482]\u001b[0m Trial 198 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,545]\u001b[0m Trial 199 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,607]\u001b[0m Trial 200 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,671]\u001b[0m Trial 201 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,742]\u001b[0m Trial 202 finished with value: 0.8499548417457812 and parameters: {'n_neighbors': 29, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,803]\u001b[0m Trial 203 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,868]\u001b[0m Trial 204 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,933]\u001b[0m Trial 205 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:28,998]\u001b[0m Trial 206 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,062]\u001b[0m Trial 207 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,127]\u001b[0m Trial 208 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,191]\u001b[0m Trial 209 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,271]\u001b[0m Trial 210 finished with value: 0.8753712524516672 and parameters: {'n_neighbors': 20, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,337]\u001b[0m Trial 211 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,401]\u001b[0m Trial 212 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,462]\u001b[0m Trial 213 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,524]\u001b[0m Trial 214 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,588]\u001b[0m Trial 215 finished with value: 0.8803854662133987 and parameters: {'n_neighbors': 10, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,657]\u001b[0m Trial 216 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,728]\u001b[0m Trial 217 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,794]\u001b[0m Trial 218 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,857]\u001b[0m Trial 219 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,918]\u001b[0m Trial 220 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:29,983]\u001b[0m Trial 221 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,047]\u001b[0m Trial 222 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,108]\u001b[0m Trial 223 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,172]\u001b[0m Trial 224 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,250]\u001b[0m Trial 225 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,316]\u001b[0m Trial 226 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,381]\u001b[0m Trial 227 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,440]\u001b[0m Trial 228 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,501]\u001b[0m Trial 229 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,566]\u001b[0m Trial 230 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,631]\u001b[0m Trial 231 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,694]\u001b[0m Trial 232 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,756]\u001b[0m Trial 233 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,817]\u001b[0m Trial 234 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,883]\u001b[0m Trial 235 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:30,947]\u001b[0m Trial 236 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,015]\u001b[0m Trial 237 finished with value: 0.877688965926537 and parameters: {'n_neighbors': 16, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,079]\u001b[0m Trial 238 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,142]\u001b[0m Trial 239 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,209]\u001b[0m Trial 240 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,282]\u001b[0m Trial 241 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,346]\u001b[0m Trial 242 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,406]\u001b[0m Trial 243 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,468]\u001b[0m Trial 244 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,532]\u001b[0m Trial 245 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,598]\u001b[0m Trial 246 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,659]\u001b[0m Trial 247 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,721]\u001b[0m Trial 248 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,793]\u001b[0m Trial 249 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,859]\u001b[0m Trial 250 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,919]\u001b[0m Trial 251 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:31,979]\u001b[0m Trial 252 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,045]\u001b[0m Trial 253 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,108]\u001b[0m Trial 254 finished with value: 0.9458287415314717 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,174]\u001b[0m Trial 255 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,247]\u001b[0m Trial 256 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,321]\u001b[0m Trial 257 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,387]\u001b[0m Trial 258 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,451]\u001b[0m Trial 259 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,517]\u001b[0m Trial 260 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,577]\u001b[0m Trial 261 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,638]\u001b[0m Trial 262 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,716]\u001b[0m Trial 263 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,778]\u001b[0m Trial 264 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,843]\u001b[0m Trial 265 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,907]\u001b[0m Trial 266 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:32,973]\u001b[0m Trial 267 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,041]\u001b[0m Trial 268 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,105]\u001b[0m Trial 269 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,172]\u001b[0m Trial 270 finished with value: 0.8584495279593319 and parameters: {'n_neighbors': 17, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,237]\u001b[0m Trial 271 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,315]\u001b[0m Trial 272 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,382]\u001b[0m Trial 273 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,447]\u001b[0m Trial 274 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,515]\u001b[0m Trial 275 finished with value: 0.8854699827786454 and parameters: {'n_neighbors': 14, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,582]\u001b[0m Trial 276 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,652]\u001b[0m Trial 277 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,717]\u001b[0m Trial 278 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,794]\u001b[0m Trial 279 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,862]\u001b[0m Trial 280 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:33,946]\u001b[0m Trial 281 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,012]\u001b[0m Trial 282 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,076]\u001b[0m Trial 283 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,143]\u001b[0m Trial 284 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,209]\u001b[0m Trial 285 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,280]\u001b[0m Trial 286 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,354]\u001b[0m Trial 287 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,420]\u001b[0m Trial 288 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,484]\u001b[0m Trial 289 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,550]\u001b[0m Trial 290 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,612]\u001b[0m Trial 291 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,676]\u001b[0m Trial 292 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,742]\u001b[0m Trial 293 finished with value: 0.9458287415314717 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,819]\u001b[0m Trial 294 finished with value: 0.8698467152785845 and parameters: {'n_neighbors': 23, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,888]\u001b[0m Trial 295 finished with value: 0.9012222679066951 and parameters: {'n_neighbors': 7, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:34,954]\u001b[0m Trial 296 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,016]\u001b[0m Trial 297 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,088]\u001b[0m Trial 298 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,155]\u001b[0m Trial 299 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,220]\u001b[0m Trial 300 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,288]\u001b[0m Trial 301 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,365]\u001b[0m Trial 302 finished with value: 0.8908653045864753 and parameters: {'n_neighbors': 11, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,428]\u001b[0m Trial 303 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,491]\u001b[0m Trial 304 finished with value: 0.8935046981201596 and parameters: {'n_neighbors': 9, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,555]\u001b[0m Trial 305 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,621]\u001b[0m Trial 306 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,695]\u001b[0m Trial 307 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,761]\u001b[0m Trial 308 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,829]\u001b[0m Trial 309 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,889]\u001b[0m Trial 310 finished with value: 0.9458287415314717 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:35,955]\u001b[0m Trial 311 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,021]\u001b[0m Trial 312 finished with value: 0.9085909640133061 and parameters: {'n_neighbors': 8, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,087]\u001b[0m Trial 313 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,152]\u001b[0m Trial 314 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,221]\u001b[0m Trial 315 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,285]\u001b[0m Trial 316 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,356]\u001b[0m Trial 317 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,434]\u001b[0m Trial 318 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,500]\u001b[0m Trial 319 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,569]\u001b[0m Trial 320 finished with value: 0.8698467152785845 and parameters: {'n_neighbors': 25, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,640]\u001b[0m Trial 321 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,704]\u001b[0m Trial 322 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,769]\u001b[0m Trial 323 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,838]\u001b[0m Trial 324 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,905]\u001b[0m Trial 325 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:36,975]\u001b[0m Trial 326 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,046]\u001b[0m Trial 327 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,110]\u001b[0m Trial 328 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,177]\u001b[0m Trial 329 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,244]\u001b[0m Trial 330 finished with value: 0.8882322513651868 and parameters: {'n_neighbors': 12, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,310]\u001b[0m Trial 331 finished with value: 0.9458287415314717 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,379]\u001b[0m Trial 332 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,452]\u001b[0m Trial 333 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,521]\u001b[0m Trial 334 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,585]\u001b[0m Trial 335 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,647]\u001b[0m Trial 336 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,708]\u001b[0m Trial 337 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,772]\u001b[0m Trial 338 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,842]\u001b[0m Trial 339 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,907]\u001b[0m Trial 340 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:37,977]\u001b[0m Trial 341 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,047]\u001b[0m Trial 342 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,120]\u001b[0m Trial 343 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,192]\u001b[0m Trial 344 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,259]\u001b[0m Trial 345 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,323]\u001b[0m Trial 346 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,395]\u001b[0m Trial 347 finished with value: 0.8444817877524352 and parameters: {'n_neighbors': 31, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,476]\u001b[0m Trial 348 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,542]\u001b[0m Trial 349 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,607]\u001b[0m Trial 350 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,670]\u001b[0m Trial 351 finished with value: 0.9192087621565479 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,743]\u001b[0m Trial 352 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,811]\u001b[0m Trial 353 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,879]\u001b[0m Trial 354 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:38,946]\u001b[0m Trial 355 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,015]\u001b[0m Trial 356 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,085]\u001b[0m Trial 357 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,150]\u001b[0m Trial 358 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,220]\u001b[0m Trial 359 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,287]\u001b[0m Trial 360 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,356]\u001b[0m Trial 361 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,424]\u001b[0m Trial 362 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,504]\u001b[0m Trial 363 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,575]\u001b[0m Trial 364 finished with value: 0.8830230539385229 and parameters: {'n_neighbors': 19, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,641]\u001b[0m Trial 365 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,712]\u001b[0m Trial 366 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,781]\u001b[0m Trial 367 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,855]\u001b[0m Trial 368 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,923]\u001b[0m Trial 369 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:39,988]\u001b[0m Trial 370 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,054]\u001b[0m Trial 371 finished with value: 0.9458287415314717 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,124]\u001b[0m Trial 372 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,193]\u001b[0m Trial 373 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,259]\u001b[0m Trial 374 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,332]\u001b[0m Trial 375 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,407]\u001b[0m Trial 376 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,486]\u001b[0m Trial 377 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,559]\u001b[0m Trial 378 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,626]\u001b[0m Trial 379 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,692]\u001b[0m Trial 380 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,761]\u001b[0m Trial 381 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,833]\u001b[0m Trial 382 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,906]\u001b[0m Trial 383 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:40,976]\u001b[0m Trial 384 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,051]\u001b[0m Trial 385 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,123]\u001b[0m Trial 386 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,192]\u001b[0m Trial 387 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,257]\u001b[0m Trial 388 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,321]\u001b[0m Trial 389 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,391]\u001b[0m Trial 390 finished with value: 0.9458287415314717 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,458]\u001b[0m Trial 391 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,538]\u001b[0m Trial 392 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,604]\u001b[0m Trial 393 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,671]\u001b[0m Trial 394 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,740]\u001b[0m Trial 395 finished with value: 0.8807008059598174 and parameters: {'n_neighbors': 21, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,811]\u001b[0m Trial 396 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,883]\u001b[0m Trial 397 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:41,955]\u001b[0m Trial 398 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,032]\u001b[0m Trial 399 finished with value: 0.9069791888996533 and parameters: {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,100]\u001b[0m Trial 400 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,165]\u001b[0m Trial 401 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,231]\u001b[0m Trial 402 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,296]\u001b[0m Trial 403 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,364]\u001b[0m Trial 404 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,433]\u001b[0m Trial 405 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,502]\u001b[0m Trial 406 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,580]\u001b[0m Trial 407 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,650]\u001b[0m Trial 408 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,726]\u001b[0m Trial 409 finished with value: 0.9458287415314717 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,795]\u001b[0m Trial 410 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,862]\u001b[0m Trial 411 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,930]\u001b[0m Trial 412 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:42,999]\u001b[0m Trial 413 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,066]\u001b[0m Trial 414 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,134]\u001b[0m Trial 415 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,204]\u001b[0m Trial 416 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,275]\u001b[0m Trial 417 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,342]\u001b[0m Trial 418 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,414]\u001b[0m Trial 419 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,484]\u001b[0m Trial 420 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,559]\u001b[0m Trial 421 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,630]\u001b[0m Trial 422 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,700]\u001b[0m Trial 423 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,771]\u001b[0m Trial 424 finished with value: 0.8854699827786454 and parameters: {'n_neighbors': 14, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,845]\u001b[0m Trial 425 finished with value: 0.8698467152785845 and parameters: {'n_neighbors': 25, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,920]\u001b[0m Trial 426 finished with value: 0.8774303126028096 and parameters: {'n_neighbors': 17, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:43,993]\u001b[0m Trial 427 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,061]\u001b[0m Trial 428 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,126]\u001b[0m Trial 429 finished with value: 0.9192087621565479 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,194]\u001b[0m Trial 430 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,261]\u001b[0m Trial 431 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,343]\u001b[0m Trial 432 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,416]\u001b[0m Trial 433 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,489]\u001b[0m Trial 434 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,563]\u001b[0m Trial 435 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,633]\u001b[0m Trial 436 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,700]\u001b[0m Trial 437 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,771]\u001b[0m Trial 438 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,841]\u001b[0m Trial 439 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,910]\u001b[0m Trial 440 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:44,983]\u001b[0m Trial 441 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,059]\u001b[0m Trial 442 finished with value: 0.8474564712967391 and parameters: {'n_neighbors': 30, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,128]\u001b[0m Trial 443 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,196]\u001b[0m Trial 444 finished with value: 0.8827734824917839 and parameters: {'n_neighbors': 15, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,265]\u001b[0m Trial 445 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,335]\u001b[0m Trial 446 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,407]\u001b[0m Trial 447 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,476]\u001b[0m Trial 448 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,546]\u001b[0m Trial 449 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,631]\u001b[0m Trial 450 finished with value: 0.8359673043006376 and parameters: {'n_neighbors': 24, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,706]\u001b[0m Trial 451 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,777]\u001b[0m Trial 452 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,847]\u001b[0m Trial 453 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,918]\u001b[0m Trial 454 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:45,986]\u001b[0m Trial 455 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,060]\u001b[0m Trial 456 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,141]\u001b[0m Trial 457 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,212]\u001b[0m Trial 458 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,283]\u001b[0m Trial 459 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,351]\u001b[0m Trial 460 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,424]\u001b[0m Trial 461 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,494]\u001b[0m Trial 462 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,565]\u001b[0m Trial 463 finished with value: 0.9069791888996533 and parameters: {'n_neighbors': 5, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,647]\u001b[0m Trial 464 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,717]\u001b[0m Trial 465 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,785]\u001b[0m Trial 466 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,859]\u001b[0m Trial 467 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:46,933]\u001b[0m Trial 468 finished with value: 0.9192087621565479 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,005]\u001b[0m Trial 469 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,078]\u001b[0m Trial 470 finished with value: 0.9009896232655905 and parameters: {'n_neighbors': 10, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,146]\u001b[0m Trial 471 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,214]\u001b[0m Trial 472 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,288]\u001b[0m Trial 473 finished with value: 0.8614235011936995 and parameters: {'n_neighbors': 27, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,360]\u001b[0m Trial 474 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,432]\u001b[0m Trial 475 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,500]\u001b[0m Trial 476 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,577]\u001b[0m Trial 477 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,660]\u001b[0m Trial 478 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,732]\u001b[0m Trial 479 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,807]\u001b[0m Trial 480 finished with value: 0.8827052783291531 and parameters: {'n_neighbors': 13, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,885]\u001b[0m Trial 481 finished with value: 0.9290931914527419 and parameters: {'n_neighbors': 4, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:47,955]\u001b[0m Trial 482 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,035]\u001b[0m Trial 483 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,107]\u001b[0m Trial 484 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,178]\u001b[0m Trial 485 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,249]\u001b[0m Trial 486 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,319]\u001b[0m Trial 487 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,389]\u001b[0m Trial 488 finished with value: 0.9458287415314717 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 2}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,457]\u001b[0m Trial 489 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,528]\u001b[0m Trial 490 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,597]\u001b[0m Trial 491 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,675]\u001b[0m Trial 492 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 1, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,753]\u001b[0m Trial 493 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,836]\u001b[0m Trial 494 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:48,943]\u001b[0m Trial 495 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:49,029]\u001b[0m Trial 496 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:49,098]\u001b[0m Trial 497 finished with value: 0.9266376010661173 and parameters: {'n_neighbors': 3, 'algorithm': 'kd_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:49,166]\u001b[0m Trial 498 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:18:49,234]\u001b[0m Trial 499 finished with value: 0.9480521751621496 and parameters: {'n_neighbors': 2, 'algorithm': 'ball_tree', 'p': 1}. Best is trial 10 with value: 0.9480521751621496.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_KNN = KNeighborsClassifier(n_neighbors= 2, algorithm= 'kd_tree', p=1)\n",
        "optimized_KNN.fit(X_over, y_over)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGfJv66falet",
        "outputId": "470c4e8d-d403-4922-8e81-01ccb4295f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='kd_tree', n_neighbors=2, p=1)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds = optimized_KNN.predict(X_over)\n",
        "f1 = f1_score(train_preds, y_over)\n",
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INs5WZwvbLyh",
        "outputId": "a7c8bee5-38a9-4289-a686-7e89dbf79278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9578713968957872"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_preds = optimized_KNN.predict(test)\n",
        "submission_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82-JyoiAbCX6",
        "outputId": "234e5e02-c3ee-4a9e-d212-6cdc01897ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
              "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "       0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_KNN.predict_proba(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN9Tc5vJcFqa",
        "outputId": "af3b6580-4587-4920-ddd3-0d81be3cf75f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0.5, 0.5],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0.5, 0.5],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0.5, 0.5],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0.5, 0.5],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0.5, 0.5],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [0. , 1. ],\n",
              "       [0.5, 0.5],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ],\n",
              "       [0. , 1. ],\n",
              "       [1. , 0. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission['OC'] = submission_preds\n",
        "submission[submission['OC']==0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pdCdKlLybCaC",
        "outputId": "b9ab1fc8-0ab5-4bad-caaa-23a89cbba2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8d3011e2-7c5e-495c-9199-ec7dbdb7e3fa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inst_id</th>\n",
              "      <th>OC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>123</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>185</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>186</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>188</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>198</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>210</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>212</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>220</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>258</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>260</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>301</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>341</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>368</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>374</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>389</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>395</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>396</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>403</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>413</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>424</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>429</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>431</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d3011e2-7c5e-495c-9199-ec7dbdb7e3fa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d3011e2-7c5e-495c-9199-ec7dbdb7e3fa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d3011e2-7c5e-495c-9199-ec7dbdb7e3fa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     inst_id  OC\n",
              "2          6   0\n",
              "3          8   0\n",
              "8         21   0\n",
              "13        30   0\n",
              "20        48   0\n",
              "21        54   0\n",
              "40       123   0\n",
              "48       151   0\n",
              "52       165   0\n",
              "58       185   0\n",
              "59       186   0\n",
              "60       188   0\n",
              "62       198   0\n",
              "67       210   0\n",
              "68       212   0\n",
              "71       220   0\n",
              "72       221   0\n",
              "84       258   0\n",
              "85       260   0\n",
              "91       301   0\n",
              "99       341   0\n",
              "102      368   0\n",
              "104      374   0\n",
              "110      389   0\n",
              "112      395   0\n",
              "113      396   0\n",
              "117      403   0\n",
              "120      413   0\n",
              "122      424   0\n",
              "124      429   0\n",
              "126      431   0"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def XGB_objective(trial):\n",
        "    ### Setting hyperparameter range ###\n",
        "    eta = trial.suggest_loguniform('eta', 0.001, 1)\n",
        "    max_depth = trial.suggest_int('max_depth', 1, 50)\n",
        "    subsample = trial.suggest_loguniform('subsample', 0.4, 1.0)\n",
        "    colsample_bytree = trial.suggest_loguniform('colsample_bytree', 0.01, 1.0)\n",
        "    colsample_bylevel = trial.suggest_loguniform('colsample_bylevel', 0.01, 1.0)\n",
        "    colsample_bynode = trial.suggest_loguniform('colsample_bynode', 0.01, 1.0)\n",
        "\n",
        "    # Initializing the model\n",
        "    model = xgb.XGBClassifier(objective = 'binary:logistic', \n",
        "                             eta = eta, \n",
        "                             max_depth = max_depth, \n",
        "                             subsample = subsample, \n",
        "                             colsample_bytree = colsample_bytree, \n",
        "                             colsample_bynode = colsample_bynode, \n",
        "                             colsample_bylevel = colsample_bylevel)\n",
        "    \n",
        "    model.fit(X_over, y_over)    \n",
        "    score = cross_val_score(model, X_over, y_over, cv=5, scoring=\"f1\")\n",
        "    f1_mean = score.mean()\n",
        "\n",
        "    return f1_mean"
      ],
      "metadata": {
        "id": "_zDoftlzbCcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_study = optuna.create_study(direction='maximize')\n",
        "XGB_study.optimize(XGB_objective, n_trials=500)"
      ],
      "metadata": {
        "id": "R4oCXPd4cxsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_XGB = xgb.XGBClassifier(eta = 0.011458247652151282, max_depth = 20, subsample = 0.7466211067904115, colsample_bytree = 0.3588367800362946, colsample_bylevel = 0.03680792380971084, colsample_bynode = 0.4037780233869643 )\n",
        "optimized_XGB.fit(X_over, y_over)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prStyCgYdwU8",
        "outputId": "90da7e44-80f4-404c-af9d-e7bb71d2830d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(colsample_bylevel=0.03680792380971084,\n",
              "              colsample_bynode=0.4037780233869643,\n",
              "              colsample_bytree=0.3588367800362946, eta=0.011458247652151282,\n",
              "              max_depth=20, subsample=0.7466211067904115)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds = optimized_XGB.predict(X_over)\n",
        "f1 = f1_score(train_preds, y_over)\n",
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmk-lV21eG6D",
        "outputId": "e72bcfe1-0ef3-4ff9-f154-fb93ec378bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9935760171306209"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_preds = optimized_XGB.predict(test)\n",
        "submission_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FesUiM1PeKMa",
        "outputId": "110edffe-c3ad-4902-dd36-8c8075de66bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_XGB.predict_proba(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcIewcnMePnc",
        "outputId": "1bb2c285-eb1f-4be3-dfea-e8585137fac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00250757, 0.99749243],\n",
              "       [0.01473689, 0.9852631 ],\n",
              "       [0.02220303, 0.977797  ],\n",
              "       [0.01473606, 0.98526394],\n",
              "       [0.00338441, 0.9966156 ],\n",
              "       [0.0071665 , 0.9928335 ],\n",
              "       [0.00931948, 0.9906805 ],\n",
              "       [0.00607204, 0.99392796],\n",
              "       [0.06590629, 0.9340937 ],\n",
              "       [0.00289422, 0.9971058 ],\n",
              "       [0.00254112, 0.9974589 ],\n",
              "       [0.10401744, 0.89598256],\n",
              "       [0.00901932, 0.9909807 ],\n",
              "       [0.01861709, 0.9813829 ],\n",
              "       [0.00546271, 0.9945373 ],\n",
              "       [0.00600266, 0.99399734],\n",
              "       [0.01290673, 0.98709327],\n",
              "       [0.04626757, 0.95373243],\n",
              "       [0.01306313, 0.98693687],\n",
              "       [0.00321013, 0.9967899 ],\n",
              "       [0.05788553, 0.9421145 ],\n",
              "       [0.06225228, 0.9377477 ],\n",
              "       [0.00970721, 0.9902928 ],\n",
              "       [0.0300957 , 0.9699043 ],\n",
              "       [0.00829142, 0.9917086 ],\n",
              "       [0.26414907, 0.73585093],\n",
              "       [0.0030005 , 0.9969995 ],\n",
              "       [0.04365063, 0.9563494 ],\n",
              "       [0.11046433, 0.88953567],\n",
              "       [0.00265688, 0.9973431 ],\n",
              "       [0.16501653, 0.83498347],\n",
              "       [0.05516416, 0.94483584],\n",
              "       [0.00601149, 0.9939885 ],\n",
              "       [0.02898461, 0.9710154 ],\n",
              "       [0.05933392, 0.9406661 ],\n",
              "       [0.01511192, 0.9848881 ],\n",
              "       [0.05515033, 0.94484967],\n",
              "       [0.02760571, 0.9723943 ],\n",
              "       [0.01271737, 0.98728263],\n",
              "       [0.01795477, 0.98204523],\n",
              "       [0.07322454, 0.92677546],\n",
              "       [0.01581091, 0.9841891 ],\n",
              "       [0.09013045, 0.90986955],\n",
              "       [0.01402235, 0.98597765],\n",
              "       [0.00442696, 0.99557304],\n",
              "       [0.01427644, 0.98572356],\n",
              "       [0.02724433, 0.9727557 ],\n",
              "       [0.11162931, 0.8883707 ],\n",
              "       [0.07512206, 0.92487794],\n",
              "       [0.02178586, 0.97821414],\n",
              "       [0.00235122, 0.9976488 ],\n",
              "       [0.00931841, 0.9906816 ],\n",
              "       [0.06147552, 0.9385245 ],\n",
              "       [0.06019121, 0.9398088 ],\n",
              "       [0.22518641, 0.7748136 ],\n",
              "       [0.01470101, 0.985299  ],\n",
              "       [0.00879437, 0.99120563],\n",
              "       [0.15560097, 0.84439903],\n",
              "       [0.07234609, 0.9276539 ],\n",
              "       [0.28105462, 0.7189454 ],\n",
              "       [0.20958602, 0.790414  ],\n",
              "       [0.03518754, 0.96481246],\n",
              "       [0.6626122 , 0.3373878 ],\n",
              "       [0.27208138, 0.7279186 ],\n",
              "       [0.00973642, 0.9902636 ],\n",
              "       [0.00326127, 0.99673873],\n",
              "       [0.01247364, 0.98752636],\n",
              "       [0.1492346 , 0.8507654 ],\n",
              "       [0.02124417, 0.97875583],\n",
              "       [0.23903298, 0.760967  ],\n",
              "       [0.00270504, 0.99729496],\n",
              "       [0.02635205, 0.97364795],\n",
              "       [0.02635205, 0.97364795],\n",
              "       [0.00503153, 0.9949685 ],\n",
              "       [0.01443624, 0.98556376],\n",
              "       [0.00410545, 0.99589455],\n",
              "       [0.00997996, 0.99002004],\n",
              "       [0.0053044 , 0.9946956 ],\n",
              "       [0.09637648, 0.9036235 ],\n",
              "       [0.02899379, 0.9710062 ],\n",
              "       [0.00363064, 0.99636936],\n",
              "       [0.00412178, 0.9958782 ],\n",
              "       [0.00834733, 0.99165267],\n",
              "       [0.07013428, 0.9298657 ],\n",
              "       [0.04373008, 0.9562699 ],\n",
              "       [0.01790023, 0.9820998 ],\n",
              "       [0.00724477, 0.99275523],\n",
              "       [0.00590134, 0.99409866],\n",
              "       [0.02806813, 0.9719319 ],\n",
              "       [0.48795056, 0.51204944],\n",
              "       [0.0038352 , 0.9961648 ],\n",
              "       [0.07487619, 0.9251238 ],\n",
              "       [0.00907433, 0.99092567],\n",
              "       [0.00798941, 0.9920106 ],\n",
              "       [0.09219867, 0.90780133],\n",
              "       [0.00478822, 0.9952118 ],\n",
              "       [0.03192252, 0.9680775 ],\n",
              "       [0.0140087 , 0.9859913 ],\n",
              "       [0.05265206, 0.94734794],\n",
              "       [0.17611712, 0.8238829 ],\n",
              "       [0.0571804 , 0.9428196 ],\n",
              "       [0.09279305, 0.90720695],\n",
              "       [0.01603806, 0.98396194],\n",
              "       [0.03892207, 0.9610779 ],\n",
              "       [0.0159179 , 0.9840821 ],\n",
              "       [0.04060298, 0.959397  ],\n",
              "       [0.01654428, 0.9834557 ],\n",
              "       [0.00678718, 0.9932128 ],\n",
              "       [0.01115543, 0.9888446 ],\n",
              "       [0.02154589, 0.9784541 ],\n",
              "       [0.02252436, 0.97747564],\n",
              "       [0.01278448, 0.9872155 ],\n",
              "       [0.02066988, 0.9793301 ],\n",
              "       [0.01293296, 0.98706704],\n",
              "       [0.10954279, 0.8904572 ],\n",
              "       [0.0566209 , 0.9433791 ],\n",
              "       [0.04437011, 0.9556299 ],\n",
              "       [0.01180005, 0.98819995],\n",
              "       [0.06421614, 0.93578386],\n",
              "       [0.03882462, 0.9611754 ],\n",
              "       [0.02016377, 0.9798362 ],\n",
              "       [0.08892125, 0.91107875],\n",
              "       [0.862092  , 0.137908  ],\n",
              "       [0.08008766, 0.91991234],\n",
              "       [0.04635006, 0.95364994],\n",
              "       [0.02604276, 0.97395724],\n",
              "       [0.03400308, 0.9659969 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.model_selection\n",
        "import sklearn.svm"
      ],
      "metadata": {
        "id": "tl-NeRb6do8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.model_selection\n",
        "import sklearn.svm\n",
        "\n",
        "def SVC_objective(trial):\n",
        "\n",
        "    svc_c = trial.suggest_float(\"C\", 1e-10, 1e10, log=True)\n",
        "    classifier_obj = sklearn.svm.SVC(C=svc_c, gamma=\"auto\")\n",
        "\n",
        "    score = sklearn.model_selection.cross_val_score(classifier_obj, X_over, y_over, cv=5, scoring=\"f1\")\n",
        "    f1_mean = score.mean()\n",
        "\n",
        "    return f1_mean"
      ],
      "metadata": {
        "id": "LJrTXh76jBrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVC_study = optuna.create_study(direction='maximize')\n",
        "SVC_study.optimize(SVC_objective, n_trials=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB0mtENpjBto",
        "outputId": "143576cc-65db-4acc-cdb7-089c0daf2643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-01 12:58:48,592]\u001b[0m A new study created in memory with name: no-name-06a3acf9-a7b6-46af-aac0-59672271247e\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:48,818]\u001b[0m Trial 0 finished with value: 0.7050641313156418 and parameters: {'C': 3.206339649827112e-06}. Best is trial 0 with value: 0.7050641313156418.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:48,985]\u001b[0m Trial 1 finished with value: 0.8140855305011663 and parameters: {'C': 0.09599755999071351}. Best is trial 1 with value: 0.8140855305011663.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:49,109]\u001b[0m Trial 2 finished with value: 0.9445567429675069 and parameters: {'C': 28433.609475921236}. Best is trial 2 with value: 0.9445567429675069.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:49,276]\u001b[0m Trial 3 finished with value: 0.7050641313156418 and parameters: {'C': 5.082570630903361e-05}. Best is trial 2 with value: 0.9445567429675069.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:49,449]\u001b[0m Trial 4 finished with value: 0.7050641313156418 and parameters: {'C': 1.4476980222979825e-07}. Best is trial 2 with value: 0.9445567429675069.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:49,560]\u001b[0m Trial 5 finished with value: 0.9001456238250126 and parameters: {'C': 1.3255955906707255}. Best is trial 2 with value: 0.9445567429675069.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:49,740]\u001b[0m Trial 6 finished with value: 0.7050641313156418 and parameters: {'C': 0.00041732402792790186}. Best is trial 2 with value: 0.9445567429675069.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:49,840]\u001b[0m Trial 7 finished with value: 0.9445567429675069 and parameters: {'C': 3715392466.206382}. Best is trial 2 with value: 0.9445567429675069.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:49,947]\u001b[0m Trial 8 finished with value: 0.9483045044842798 and parameters: {'C': 22.988590475662146}. Best is trial 8 with value: 0.9483045044842798.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:50,061]\u001b[0m Trial 9 finished with value: 0.9445567429675069 and parameters: {'C': 85038.52896023665}. Best is trial 8 with value: 0.9483045044842798.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:50,266]\u001b[0m Trial 10 finished with value: 0.7050641313156418 and parameters: {'C': 1.2086157974023268e-10}. Best is trial 8 with value: 0.9483045044842798.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:50,369]\u001b[0m Trial 11 finished with value: 0.9445567429675069 and parameters: {'C': 6174.782932951147}. Best is trial 8 with value: 0.9483045044842798.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:50,474]\u001b[0m Trial 12 finished with value: 0.9445567429675069 and parameters: {'C': 4043.47058621957}. Best is trial 8 with value: 0.9483045044842798.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:50,623]\u001b[0m Trial 13 finished with value: 0.9445567429675069 and parameters: {'C': 30431567.52005701}. Best is trial 8 with value: 0.9483045044842798.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:50,751]\u001b[0m Trial 14 finished with value: 0.9001456238250126 and parameters: {'C': 1.1456684465497098}. Best is trial 8 with value: 0.9483045044842798.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:50,883]\u001b[0m Trial 15 finished with value: 0.9597300983208029 and parameters: {'C': 232.19398366913614}. Best is trial 15 with value: 0.9597300983208029.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:50,995]\u001b[0m Trial 16 finished with value: 0.9458489140976549 and parameters: {'C': 14.405138888220355}. Best is trial 15 with value: 0.9597300983208029.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:51,120]\u001b[0m Trial 17 finished with value: 0.9645860972876118 and parameters: {'C': 342.33981304893285}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:51,229]\u001b[0m Trial 18 finished with value: 0.9445567429675069 and parameters: {'C': 6339723.708029215}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:51,415]\u001b[0m Trial 19 finished with value: 0.7050641313156418 and parameters: {'C': 0.0044194787317231084}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:51,548]\u001b[0m Trial 20 finished with value: 0.9645860972876118 and parameters: {'C': 330.24893021550366}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:51,665]\u001b[0m Trial 21 finished with value: 0.9645860972876118 and parameters: {'C': 243.36945203829487}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:51,777]\u001b[0m Trial 22 finished with value: 0.9445567429675069 and parameters: {'C': 4066031.348671871}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:51,904]\u001b[0m Trial 23 finished with value: 0.9645860972876118 and parameters: {'C': 280.1490768450836}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:52,106]\u001b[0m Trial 24 finished with value: 0.7050641313156418 and parameters: {'C': 0.019424431384235042}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:52,231]\u001b[0m Trial 25 finished with value: 0.9445567429675069 and parameters: {'C': 308127.559392299}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:52,352]\u001b[0m Trial 26 finished with value: 0.9645860972876118 and parameters: {'C': 325.35338829851634}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:52,464]\u001b[0m Trial 27 finished with value: 0.9445567429675069 and parameters: {'C': 91758596.7354939}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:52,577]\u001b[0m Trial 28 finished with value: 0.9458489140976549 and parameters: {'C': 14.880909863610784}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:52,687]\u001b[0m Trial 29 finished with value: 0.9445567429675069 and parameters: {'C': 460261.8800081979}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:52,827]\u001b[0m Trial 30 finished with value: 0.9530170998149604 and parameters: {'C': 2093.9609553374953}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:52,940]\u001b[0m Trial 31 finished with value: 0.9577417922579213 and parameters: {'C': 113.17013807010461}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:53,133]\u001b[0m Trial 32 finished with value: 0.8141501774549509 and parameters: {'C': 0.11228710948317999}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:53,257]\u001b[0m Trial 33 finished with value: 0.9019638056431946 and parameters: {'C': 1.6559319629835028}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:53,378]\u001b[0m Trial 34 finished with value: 0.9530170998149604 and parameters: {'C': 1587.5321548066859}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:53,498]\u001b[0m Trial 35 finished with value: 0.9445567429675069 and parameters: {'C': 22663.20692200035}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:53,689]\u001b[0m Trial 36 finished with value: 0.7691579353344716 and parameters: {'C': 0.06669100210910428}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:53,807]\u001b[0m Trial 37 finished with value: 0.9503967093413545 and parameters: {'C': 61.86678891089767}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:53,990]\u001b[0m Trial 38 finished with value: 0.7050641313156418 and parameters: {'C': 0.000159491647172756}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:54,113]\u001b[0m Trial 39 finished with value: 0.940091091703995 and parameters: {'C': 3.4113848279572787}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:54,302]\u001b[0m Trial 40 finished with value: 0.7050641313156418 and parameters: {'C': 1.0600310955063985e-05}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:54,417]\u001b[0m Trial 41 finished with value: 0.9645860972876118 and parameters: {'C': 345.0959018396676}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:54,539]\u001b[0m Trial 42 finished with value: 0.9576366418809334 and parameters: {'C': 655.9019529032163}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:54,651]\u001b[0m Trial 43 finished with value: 0.9445567429675069 and parameters: {'C': 58057.379399783415}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:54,766]\u001b[0m Trial 44 finished with value: 0.9445567429675069 and parameters: {'C': 12256.690124966584}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:54,923]\u001b[0m Trial 45 finished with value: 0.9012525321585809 and parameters: {'C': 0.2628043952772923}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:55,038]\u001b[0m Trial 46 finished with value: 0.9435534318021727 and parameters: {'C': 10.255793994419772}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:55,165]\u001b[0m Trial 47 finished with value: 0.9445567429675069 and parameters: {'C': 283576.4212516448}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:55,364]\u001b[0m Trial 48 finished with value: 0.7050641313156418 and parameters: {'C': 0.0018118988219307738}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:55,559]\u001b[0m Trial 49 finished with value: 0.7050641313156418 and parameters: {'C': 6.606463389258342e-09}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:55,679]\u001b[0m Trial 50 finished with value: 0.9554996523753321 and parameters: {'C': 69.96327264035827}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:55,814]\u001b[0m Trial 51 finished with value: 0.9622906149921295 and parameters: {'C': 447.5188603340043}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:55,928]\u001b[0m Trial 52 finished with value: 0.9445567429675069 and parameters: {'C': 5256.80730805978}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:56,056]\u001b[0m Trial 53 finished with value: 0.9621856887074278 and parameters: {'C': 180.2622541660061}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:56,191]\u001b[0m Trial 54 finished with value: 0.9622906149921295 and parameters: {'C': 483.27671729656987}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:56,311]\u001b[0m Trial 55 finished with value: 0.943911643911644 and parameters: {'C': 4.985209856519429}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:56,461]\u001b[0m Trial 56 finished with value: 0.9001456238250126 and parameters: {'C': 0.5000624139670664}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:56,576]\u001b[0m Trial 57 finished with value: 0.9484094307689814 and parameters: {'C': 35.23527420200389}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:56,701]\u001b[0m Trial 58 finished with value: 0.9445567429675069 and parameters: {'C': 66918.39348879603}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:56,816]\u001b[0m Trial 59 finished with value: 0.9445567429675069 and parameters: {'C': 3332074.867422657}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:56,932]\u001b[0m Trial 60 finished with value: 0.9530170998149604 and parameters: {'C': 1957.5986021981792}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:57,055]\u001b[0m Trial 61 finished with value: 0.9622906149921295 and parameters: {'C': 491.4224888011334}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:57,171]\u001b[0m Trial 62 finished with value: 0.9445567429675069 and parameters: {'C': 10184.215192820922}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:57,290]\u001b[0m Trial 63 finished with value: 0.9621856887074278 and parameters: {'C': 219.97337102584098}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:57,412]\u001b[0m Trial 64 finished with value: 0.9526549403095668 and parameters: {'C': 1050.1456089388366}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:57,519]\u001b[0m Trial 65 finished with value: 0.9458489140976549 and parameters: {'C': 19.38043972171273}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:57,644]\u001b[0m Trial 66 finished with value: 0.9019638056431946 and parameters: {'C': 1.6919841159312672}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:57,762]\u001b[0m Trial 67 finished with value: 0.9530992437951481 and parameters: {'C': 76.7228446623235}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:57,876]\u001b[0m Trial 68 finished with value: 0.9445567429675069 and parameters: {'C': 508993.8488119524}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:57,992]\u001b[0m Trial 69 finished with value: 0.9445567429675069 and parameters: {'C': 20677.51946847119}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:58,108]\u001b[0m Trial 70 finished with value: 0.9445567429675069 and parameters: {'C': 4111.226091652675}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:58,226]\u001b[0m Trial 71 finished with value: 0.9530170998149604 and parameters: {'C': 1333.2862527255477}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:58,356]\u001b[0m Trial 72 finished with value: 0.9645860972876118 and parameters: {'C': 278.15295386511957}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:58,468]\u001b[0m Trial 73 finished with value: 0.9458489140976549 and parameters: {'C': 14.31121841538201}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:58,592]\u001b[0m Trial 74 finished with value: 0.9621856887074278 and parameters: {'C': 161.79208743687926}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:58,710]\u001b[0m Trial 75 finished with value: 0.9442014989841077 and parameters: {'C': 4.395884451383432}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:58,828]\u001b[0m Trial 76 finished with value: 0.9445567429675069 and parameters: {'C': 135645.15885760446}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:58,940]\u001b[0m Trial 77 finished with value: 0.9445567429675069 and parameters: {'C': 3855361957.516409}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:59,071]\u001b[0m Trial 78 finished with value: 0.9622906149921295 and parameters: {'C': 494.43061922597735}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:59,274]\u001b[0m Trial 79 finished with value: 0.7306977681063731 and parameters: {'C': 0.04380568049363495}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:59,413]\u001b[0m Trial 80 finished with value: 0.9528522997279794 and parameters: {'C': 54.89812325421839}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:59,543]\u001b[0m Trial 81 finished with value: 0.9623841166284081 and parameters: {'C': 575.1294914527183}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:59,665]\u001b[0m Trial 82 finished with value: 0.9445567429675069 and parameters: {'C': 4690.945181464034}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:59,787]\u001b[0m Trial 83 finished with value: 0.9645860972876118 and parameters: {'C': 307.74292842713663}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:58:59,915]\u001b[0m Trial 84 finished with value: 0.9445567429675069 and parameters: {'C': 29088.436507413673}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:00,023]\u001b[0m Trial 85 finished with value: 0.9506515706515707 and parameters: {'C': 31.96113232295855}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:00,168]\u001b[0m Trial 86 finished with value: 0.8979234016027904 and parameters: {'C': 0.45744861519914415}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:00,279]\u001b[0m Trial 87 finished with value: 0.9434449075828386 and parameters: {'C': 7.727303774517979}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:00,420]\u001b[0m Trial 88 finished with value: 0.9530170998149604 and parameters: {'C': 2116.294422190924}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:00,536]\u001b[0m Trial 89 finished with value: 0.95989232989233 and parameters: {'C': 121.76383268094754}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:00,661]\u001b[0m Trial 90 finished with value: 0.9001456238250126 and parameters: {'C': 1.5093920561229397}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:00,784]\u001b[0m Trial 91 finished with value: 0.9645860972876118 and parameters: {'C': 327.20095985698356}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:00,910]\u001b[0m Trial 92 finished with value: 0.9621856887074278 and parameters: {'C': 199.82073226629103}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:01,035]\u001b[0m Trial 93 finished with value: 0.9445567429675069 and parameters: {'C': 8531.11734908177}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:01,157]\u001b[0m Trial 94 finished with value: 0.9553277455405116 and parameters: {'C': 995.7083941040581}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:01,275]\u001b[0m Trial 95 finished with value: 0.9645860972876118 and parameters: {'C': 344.67919363054347}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:01,404]\u001b[0m Trial 96 finished with value: 0.952947052947053 and parameters: {'C': 29.01272837884351}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:01,519]\u001b[0m Trial 97 finished with value: 0.9445567429675069 and parameters: {'C': 46084.5183032231}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:01,644]\u001b[0m Trial 98 finished with value: 0.9645860972876118 and parameters: {'C': 271.8988792869194}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:01,759]\u001b[0m Trial 99 finished with value: 0.9465670281474974 and parameters: {'C': 3276.1887177248022}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:01,877]\u001b[0m Trial 100 finished with value: 0.9445567429675069 and parameters: {'C': 1110014.4090304058}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:01,997]\u001b[0m Trial 101 finished with value: 0.9645860972876118 and parameters: {'C': 237.58805536268758}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:02,113]\u001b[0m Trial 102 finished with value: 0.9435534318021727 and parameters: {'C': 10.034551732282145}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:02,227]\u001b[0m Trial 103 finished with value: 0.9528522997279794 and parameters: {'C': 51.58114331035276}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:02,340]\u001b[0m Trial 104 finished with value: 0.9445567429675069 and parameters: {'C': 11798.461971142595}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:02,469]\u001b[0m Trial 105 finished with value: 0.9645860972876118 and parameters: {'C': 254.6142389573357}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:02,585]\u001b[0m Trial 106 finished with value: 0.9530992437951481 and parameters: {'C': 77.54383081993444}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:02,712]\u001b[0m Trial 107 finished with value: 0.9530170998149604 and parameters: {'C': 1279.3049404644646}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:02,835]\u001b[0m Trial 108 finished with value: 0.9645860972876118 and parameters: {'C': 241.73585965117985}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:02,941]\u001b[0m Trial 109 finished with value: 0.9445567429675069 and parameters: {'C': 916965433.3038788}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:03,131]\u001b[0m Trial 110 finished with value: 0.7050641313156418 and parameters: {'C': 2.845108612358469e-07}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:03,254]\u001b[0m Trial 111 finished with value: 0.9645860972876118 and parameters: {'C': 273.3360352035704}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:03,372]\u001b[0m Trial 112 finished with value: 0.9530170998149604 and parameters: {'C': 2167.1717481529727}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:03,508]\u001b[0m Trial 113 finished with value: 0.9531772079061029 and parameters: {'C': 788.3610510577053}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:03,630]\u001b[0m Trial 114 finished with value: 0.940091091703995 and parameters: {'C': 3.384172528898495}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:03,741]\u001b[0m Trial 115 finished with value: 0.9506515706515707 and parameters: {'C': 24.560648935779625}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:03,859]\u001b[0m Trial 116 finished with value: 0.95989232989233 and parameters: {'C': 120.26018789840732}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:03,967]\u001b[0m Trial 117 finished with value: 0.9445567429675069 and parameters: {'C': 7791.125784104306}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:04,086]\u001b[0m Trial 118 finished with value: 0.9531525862080411 and parameters: {'C': 67.57423562694562}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:04,198]\u001b[0m Trial 119 finished with value: 0.9458489140976549 and parameters: {'C': 17.405435501681236}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:04,313]\u001b[0m Trial 120 finished with value: 0.9445567429675069 and parameters: {'C': 139114.42812144908}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:04,436]\u001b[0m Trial 121 finished with value: 0.9645860972876118 and parameters: {'C': 293.65795440098066}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:04,555]\u001b[0m Trial 122 finished with value: 0.9645860972876118 and parameters: {'C': 379.7225245436946}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:04,679]\u001b[0m Trial 123 finished with value: 0.9551676374493692 and parameters: {'C': 1175.9302005110537}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:04,806]\u001b[0m Trial 124 finished with value: 0.9621856887074278 and parameters: {'C': 151.5165793276985}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:04,922]\u001b[0m Trial 125 finished with value: 0.9445567429675069 and parameters: {'C': 3674.413962858846}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:05,036]\u001b[0m Trial 126 finished with value: 0.9445567429675069 and parameters: {'C': 19228.501835839033}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:05,142]\u001b[0m Trial 127 finished with value: 0.9434449075828386 and parameters: {'C': 6.095100610848788}. Best is trial 17 with value: 0.9645860972876118.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:05,266]\u001b[0m Trial 128 finished with value: 0.9646376811594204 and parameters: {'C': 514.4366005927387}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:05,382]\u001b[0m Trial 129 finished with value: 0.9531772079061029 and parameters: {'C': 816.9536763467157}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:05,499]\u001b[0m Trial 130 finished with value: 0.9506550112754315 and parameters: {'C': 42.29400547504223}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:05,612]\u001b[0m Trial 131 finished with value: 0.9645860972876118 and parameters: {'C': 297.7613529453552}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:05,737]\u001b[0m Trial 132 finished with value: 0.9646376811594204 and parameters: {'C': 495.23404905213687}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:05,849]\u001b[0m Trial 133 finished with value: 0.9622906149921295 and parameters: {'C': 392.02770619812605}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:05,979]\u001b[0m Trial 134 finished with value: 0.9488578182363241 and parameters: {'C': 3183.4108764772513}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:06,090]\u001b[0m Trial 135 finished with value: 0.9528522997279794 and parameters: {'C': 62.63044669820011}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:06,205]\u001b[0m Trial 136 finished with value: 0.9621856887074278 and parameters: {'C': 137.1337867350047}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:06,331]\u001b[0m Trial 137 finished with value: 0.9530170998149604 and parameters: {'C': 1685.6145131576122}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:06,449]\u001b[0m Trial 138 finished with value: 0.9576366418809334 and parameters: {'C': 729.6022782749452}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:06,549]\u001b[0m Trial 139 finished with value: 0.9458489140976549 and parameters: {'C': 18.815878320906826}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:06,619]\u001b[0m Trial 140 finished with value: 0.9554463099624391 and parameters: {'C': 105.13032907974886}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:06,687]\u001b[0m Trial 141 finished with value: 0.9645860972876118 and parameters: {'C': 288.08791368661576}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:06,757]\u001b[0m Trial 142 finished with value: 0.9645860972876118 and parameters: {'C': 304.24005560971153}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:06,825]\u001b[0m Trial 143 finished with value: 0.9645860972876118 and parameters: {'C': 237.9521280345799}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:06,932]\u001b[0m Trial 144 finished with value: 0.7050641313156418 and parameters: {'C': 1.8236747970126453e-10}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:06,996]\u001b[0m Trial 145 finished with value: 0.9445567429675069 and parameters: {'C': 5266.336196618337}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,067]\u001b[0m Trial 146 finished with value: 0.9526549403095668 and parameters: {'C': 1126.0103175941208}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,131]\u001b[0m Trial 147 finished with value: 0.9484094307689814 and parameters: {'C': 40.112027643995056}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,204]\u001b[0m Trial 148 finished with value: 0.9530170998149604 and parameters: {'C': 2197.1395833132865}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,272]\u001b[0m Trial 149 finished with value: 0.9576366418809334 and parameters: {'C': 689.3377440899519}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,347]\u001b[0m Trial 150 finished with value: 0.9554463099624391 and parameters: {'C': 106.89426169035646}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,417]\u001b[0m Trial 151 finished with value: 0.9576366418809334 and parameters: {'C': 679.0757331858524}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,486]\u001b[0m Trial 152 finished with value: 0.9622906149921295 and parameters: {'C': 391.851751302443}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,561]\u001b[0m Trial 153 finished with value: 0.9445567429675069 and parameters: {'C': 7893.049034327108}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,622]\u001b[0m Trial 154 finished with value: 0.9435534318021727 and parameters: {'C': 11.759165560224632}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,687]\u001b[0m Trial 155 finished with value: 0.9530170998149604 and parameters: {'C': 2431.3418388124555}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,757]\u001b[0m Trial 156 finished with value: 0.9530992437951481 and parameters: {'C': 84.28750580978802}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,825]\u001b[0m Trial 157 finished with value: 0.9460090221887976 and parameters: {'C': 37.584932586705676}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,896]\u001b[0m Trial 158 finished with value: 0.9597300983208029 and parameters: {'C': 230.74383755235203}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:07,966]\u001b[0m Trial 159 finished with value: 0.9621856887074278 and parameters: {'C': 189.1833883692428}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,037]\u001b[0m Trial 160 finished with value: 0.9361311265566584 and parameters: {'C': 3.0613052306095674}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,103]\u001b[0m Trial 161 finished with value: 0.9645860972876118 and parameters: {'C': 296.3628460669566}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,173]\u001b[0m Trial 162 finished with value: 0.9645860972876118 and parameters: {'C': 327.49197351783755}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,239]\u001b[0m Trial 163 finished with value: 0.9530170998149604 and parameters: {'C': 1286.244575780518}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,307]\u001b[0m Trial 164 finished with value: 0.95989232989233 and parameters: {'C': 123.24888254914521}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,377]\u001b[0m Trial 165 finished with value: 0.9622906149921295 and parameters: {'C': 422.4810941683053}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,443]\u001b[0m Trial 166 finished with value: 0.9483045044842798 and parameters: {'C': 32.75444522417833}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,506]\u001b[0m Trial 167 finished with value: 0.9445567429675069 and parameters: {'C': 3753.4584208047804}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,582]\u001b[0m Trial 168 finished with value: 0.9445567429675069 and parameters: {'C': 18073.713595100347}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,652]\u001b[0m Trial 169 finished with value: 0.9551676374493692 and parameters: {'C': 1027.4867551650545}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,721]\u001b[0m Trial 170 finished with value: 0.9646376811594204 and parameters: {'C': 541.3606504483455}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,792]\u001b[0m Trial 171 finished with value: 0.9621856887074278 and parameters: {'C': 180.26680478942365}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,859]\u001b[0m Trial 172 finished with value: 0.9646376811594204 and parameters: {'C': 548.8121067428024}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,929]\u001b[0m Trial 173 finished with value: 0.9601400773577208 and parameters: {'C': 561.5927436707975}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:08,997]\u001b[0m Trial 174 finished with value: 0.9530170998149604 and parameters: {'C': 1845.333416777761}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,069]\u001b[0m Trial 175 finished with value: 0.9622906149921295 and parameters: {'C': 435.2218827526331}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,134]\u001b[0m Trial 176 finished with value: 0.9530992437951481 and parameters: {'C': 76.49663076935514}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,196]\u001b[0m Trial 177 finished with value: 0.9445567429675069 and parameters: {'C': 6123.222264347467}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,263]\u001b[0m Trial 178 finished with value: 0.9645860972876118 and parameters: {'C': 240.29972862205037}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,333]\u001b[0m Trial 179 finished with value: 0.9530992437951481 and parameters: {'C': 90.24011042544826}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,400]\u001b[0m Trial 180 finished with value: 0.9576366418809334 and parameters: {'C': 726.9452074284727}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,459]\u001b[0m Trial 181 finished with value: 0.9483045044842798 and parameters: {'C': 21.96941716116753}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,523]\u001b[0m Trial 182 finished with value: 0.9530170998149604 and parameters: {'C': 1668.2653461801813}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,598]\u001b[0m Trial 183 finished with value: 0.9622906149921295 and parameters: {'C': 414.97688017890243}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,667]\u001b[0m Trial 184 finished with value: 0.95989232989233 and parameters: {'C': 130.73709789945084}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,737]\u001b[0m Trial 185 finished with value: 0.9531772079061029 and parameters: {'C': 783.6292494015291}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,802]\u001b[0m Trial 186 finished with value: 0.9503967093413545 and parameters: {'C': 57.762637586484345}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,868]\u001b[0m Trial 187 finished with value: 0.9645860972876118 and parameters: {'C': 251.2508206037676}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,930]\u001b[0m Trial 188 finished with value: 0.9488578182363241 and parameters: {'C': 3079.3640882557356}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:09,997]\u001b[0m Trial 189 finished with value: 0.9621856887074278 and parameters: {'C': 201.1665975920337}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,065]\u001b[0m Trial 190 finished with value: 0.9460090221887976 and parameters: {'C': 38.50445579355626}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,132]\u001b[0m Trial 191 finished with value: 0.9645860972876118 and parameters: {'C': 375.1318526049879}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,196]\u001b[0m Trial 192 finished with value: 0.9621856887074278 and parameters: {'C': 215.54832968444683}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,262]\u001b[0m Trial 193 finished with value: 0.9645860972876118 and parameters: {'C': 373.119154935607}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,330]\u001b[0m Trial 194 finished with value: 0.9526549403095668 and parameters: {'C': 1102.7460681049959}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,401]\u001b[0m Trial 195 finished with value: 0.9645860972876118 and parameters: {'C': 281.91811712034314}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,466]\u001b[0m Trial 196 finished with value: 0.9621856887074278 and parameters: {'C': 196.64824664881056}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,535]\u001b[0m Trial 197 finished with value: 0.9530170998149604 and parameters: {'C': 1609.6764686641116}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,612]\u001b[0m Trial 198 finished with value: 0.9531525862080411 and parameters: {'C': 67.38942411122945}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,681]\u001b[0m Trial 199 finished with value: 0.9623841166284081 and parameters: {'C': 588.8009941562995}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,745]\u001b[0m Trial 200 finished with value: 0.9435534318021727 and parameters: {'C': 12.356882984297936}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,813]\u001b[0m Trial 201 finished with value: 0.95989232989233 and parameters: {'C': 133.30596579602232}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,882]\u001b[0m Trial 202 finished with value: 0.9576366418809334 and parameters: {'C': 641.7240259767896}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:10,949]\u001b[0m Trial 203 finished with value: 0.9645860972876118 and parameters: {'C': 294.27089137230763}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,018]\u001b[0m Trial 204 finished with value: 0.9645860972876118 and parameters: {'C': 272.33209180297507}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,083]\u001b[0m Trial 205 finished with value: 0.9554463099624391 and parameters: {'C': 99.05005241434972}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,146]\u001b[0m Trial 206 finished with value: 0.9488578182363241 and parameters: {'C': 2876.9102749546873}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,219]\u001b[0m Trial 207 finished with value: 0.9553277455405116 and parameters: {'C': 923.2052356080475}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,281]\u001b[0m Trial 208 finished with value: 0.9460090221887976 and parameters: {'C': 33.82393915633394}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,345]\u001b[0m Trial 209 finished with value: 0.9621856887074278 and parameters: {'C': 134.83081113576966}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,408]\u001b[0m Trial 210 finished with value: 0.9445567429675069 and parameters: {'C': 5980.51047961517}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,498]\u001b[0m Trial 211 finished with value: 0.9622906149921295 and parameters: {'C': 472.2331265792101}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,567]\u001b[0m Trial 212 finished with value: 0.9530170998149604 and parameters: {'C': 1401.0164530297623}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,645]\u001b[0m Trial 213 finished with value: 0.9597300983208029 and parameters: {'C': 224.8523869553137}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,710]\u001b[0m Trial 214 finished with value: 0.9503967093413545 and parameters: {'C': 56.718918117676935}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,786]\u001b[0m Trial 215 finished with value: 0.9623841166284081 and parameters: {'C': 584.4924884279651}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,856]\u001b[0m Trial 216 finished with value: 0.9530170998149604 and parameters: {'C': 1361.1613958601533}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,924]\u001b[0m Trial 217 finished with value: 0.9645860972876118 and parameters: {'C': 356.0790372688338}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:11,993]\u001b[0m Trial 218 finished with value: 0.9621856887074278 and parameters: {'C': 203.9590576473757}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,061]\u001b[0m Trial 219 finished with value: 0.9530170998149604 and parameters: {'C': 2266.6519936565983}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,126]\u001b[0m Trial 220 finished with value: 0.9530992437951481 and parameters: {'C': 81.68218628430462}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,192]\u001b[0m Trial 221 finished with value: 0.9646376811594204 and parameters: {'C': 502.5648509413503}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,261]\u001b[0m Trial 222 finished with value: 0.9576366418809334 and parameters: {'C': 631.3714575596952}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,326]\u001b[0m Trial 223 finished with value: 0.9645860972876118 and parameters: {'C': 239.01302750044215}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,392]\u001b[0m Trial 224 finished with value: 0.9621856887074278 and parameters: {'C': 135.21765463640438}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,468]\u001b[0m Trial 225 finished with value: 0.9646376811594204 and parameters: {'C': 511.9393968971167}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,540]\u001b[0m Trial 226 finished with value: 0.9622906149921295 and parameters: {'C': 471.11644540955245}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,653]\u001b[0m Trial 227 finished with value: 0.7050641313156418 and parameters: {'C': 0.005866173527751448}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,727]\u001b[0m Trial 228 finished with value: 0.9531772079061029 and parameters: {'C': 816.1321580241992}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,790]\u001b[0m Trial 229 finished with value: 0.9528522997279794 and parameters: {'C': 43.986105895334504}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,854]\u001b[0m Trial 230 finished with value: 0.9488578182363241 and parameters: {'C': 2777.1590925580917}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,921]\u001b[0m Trial 231 finished with value: 0.9645860972876118 and parameters: {'C': 371.92989653444283}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:12,991]\u001b[0m Trial 232 finished with value: 0.9645860972876118 and parameters: {'C': 311.89045119058335}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,061]\u001b[0m Trial 233 finished with value: 0.9526549403095668 and parameters: {'C': 1063.916251010915}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,130]\u001b[0m Trial 234 finished with value: 0.9621856887074278 and parameters: {'C': 191.04046282927774}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,195]\u001b[0m Trial 235 finished with value: 0.9530992437951481 and parameters: {'C': 89.97029931223486}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,261]\u001b[0m Trial 236 finished with value: 0.9645860972876118 and parameters: {'C': 366.02541351650865}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,330]\u001b[0m Trial 237 finished with value: 0.9646376811594204 and parameters: {'C': 509.1344173174337}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,403]\u001b[0m Trial 238 finished with value: 0.9553277455405116 and parameters: {'C': 983.8121550668556}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,470]\u001b[0m Trial 239 finished with value: 0.9530170998149604 and parameters: {'C': 1796.0175415786723}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,538]\u001b[0m Trial 240 finished with value: 0.9645860972876118 and parameters: {'C': 380.74043605159835}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,608]\u001b[0m Trial 241 finished with value: 0.9645860972876118 and parameters: {'C': 320.478572174656}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,686]\u001b[0m Trial 242 finished with value: 0.9622906149921295 and parameters: {'C': 444.3783243692069}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,756]\u001b[0m Trial 243 finished with value: 0.9554463099624391 and parameters: {'C': 106.76144700842633}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,822]\u001b[0m Trial 244 finished with value: 0.95989232989233 and parameters: {'C': 131.95531266161095}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,888]\u001b[0m Trial 245 finished with value: 0.9553277455405116 and parameters: {'C': 911.9528136523703}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:13,959]\u001b[0m Trial 246 finished with value: 0.9622906149921295 and parameters: {'C': 493.0696793812427}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,029]\u001b[0m Trial 247 finished with value: 0.9645860972876118 and parameters: {'C': 251.84694856971686}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,097]\u001b[0m Trial 248 finished with value: 0.9621856887074278 and parameters: {'C': 196.80621407411397}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,161]\u001b[0m Trial 249 finished with value: 0.9528522997279794 and parameters: {'C': 52.05169187379274}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,224]\u001b[0m Trial 250 finished with value: 0.9445567429675069 and parameters: {'C': 3833.1693443694808}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,291]\u001b[0m Trial 251 finished with value: 0.9551676374493692 and parameters: {'C': 1212.3388230479022}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,356]\u001b[0m Trial 252 finished with value: 0.9621856887074278 and parameters: {'C': 160.71126772674063}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,425]\u001b[0m Trial 253 finished with value: 0.9622906149921295 and parameters: {'C': 474.4815685543887}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,486]\u001b[0m Trial 254 finished with value: 0.9458489140976549 and parameters: {'C': 21.52953912033293}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,551]\u001b[0m Trial 255 finished with value: 0.9530170998149604 and parameters: {'C': 1882.919002161501}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,616]\u001b[0m Trial 256 finished with value: 0.9622906149921295 and parameters: {'C': 486.9652980979761}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,687]\u001b[0m Trial 257 finished with value: 0.9554996523753321 and parameters: {'C': 69.30164139257487}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,756]\u001b[0m Trial 258 finished with value: 0.9621856887074278 and parameters: {'C': 150.1917473082867}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,829]\u001b[0m Trial 259 finished with value: 0.9553277455405116 and parameters: {'C': 1014.7583171321968}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,892]\u001b[0m Trial 260 finished with value: 0.9445567429675069 and parameters: {'C': 4476.0964225725265}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:14,960]\u001b[0m Trial 261 finished with value: 0.9645860972876118 and parameters: {'C': 271.188203654987}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,029]\u001b[0m Trial 262 finished with value: 0.9445567429675069 and parameters: {'C': 33835695.32536008}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,098]\u001b[0m Trial 263 finished with value: 0.9646376811594204 and parameters: {'C': 500.1909477034351}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,163]\u001b[0m Trial 264 finished with value: 0.9554463099624391 and parameters: {'C': 94.59798055306}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,232]\u001b[0m Trial 265 finished with value: 0.9576366418809334 and parameters: {'C': 724.0725114617202}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,300]\u001b[0m Trial 266 finished with value: 0.9530170998149604 and parameters: {'C': 1956.1785989350408}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,363]\u001b[0m Trial 267 finished with value: 0.9445567429675069 and parameters: {'C': 11296.246363445147}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,428]\u001b[0m Trial 268 finished with value: 0.9645860972876118 and parameters: {'C': 335.0531416795326}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,494]\u001b[0m Trial 269 finished with value: 0.9576366418809334 and parameters: {'C': 660.9772822463514}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,564]\u001b[0m Trial 270 finished with value: 0.9645860972876118 and parameters: {'C': 357.9294917999877}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,630]\u001b[0m Trial 271 finished with value: 0.9553277455405116 and parameters: {'C': 893.4632857838703}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,695]\u001b[0m Trial 272 finished with value: 0.9645860972876118 and parameters: {'C': 347.2539312046644}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,770]\u001b[0m Trial 273 finished with value: 0.9460090221887976 and parameters: {'C': 38.59827728700047}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,834]\u001b[0m Trial 274 finished with value: 0.9530170998149604 and parameters: {'C': 1663.4952557064275}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,901]\u001b[0m Trial 275 finished with value: 0.9445567429675069 and parameters: {'C': 5402.3776830464285}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:15,964]\u001b[0m Trial 276 finished with value: 0.9577417922579213 and parameters: {'C': 113.4187026344118}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,032]\u001b[0m Trial 277 finished with value: 0.9621856887074278 and parameters: {'C': 151.27851270365016}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,095]\u001b[0m Trial 278 finished with value: 0.9484094307689814 and parameters: {'C': 40.25965461015731}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,162]\u001b[0m Trial 279 finished with value: 0.9622906149921295 and parameters: {'C': 462.96817975611816}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,224]\u001b[0m Trial 280 finished with value: 0.9488578182363241 and parameters: {'C': 2930.3012031438584}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,290]\u001b[0m Trial 281 finished with value: 0.9551676374493692 and parameters: {'C': 1034.598712274671}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,356]\u001b[0m Trial 282 finished with value: 0.9645860972876118 and parameters: {'C': 355.99107038988797}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,417]\u001b[0m Trial 283 finished with value: 0.9458489140976549 and parameters: {'C': 13.025397290188376}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,480]\u001b[0m Trial 284 finished with value: 0.9577417922579213 and parameters: {'C': 115.78484414582691}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,548]\u001b[0m Trial 285 finished with value: 0.9621856887074278 and parameters: {'C': 168.53779451472374}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,616]\u001b[0m Trial 286 finished with value: 0.9530170998149604 and parameters: {'C': 1599.5615328859988}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,681]\u001b[0m Trial 287 finished with value: 0.9554996523753321 and parameters: {'C': 68.96373974529733}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,760]\u001b[0m Trial 288 finished with value: 0.9553277455405116 and parameters: {'C': 835.1824018874723}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,868]\u001b[0m Trial 289 finished with value: 0.7050641313156418 and parameters: {'C': 3.164675685034347e-06}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,931]\u001b[0m Trial 290 finished with value: 0.9483045044842798 and parameters: {'C': 22.246223686743832}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:16,997]\u001b[0m Trial 291 finished with value: 0.9488578182363241 and parameters: {'C': 2866.0841802967934}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,071]\u001b[0m Trial 292 finished with value: 0.9645860972876118 and parameters: {'C': 282.1396230253208}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,141]\u001b[0m Trial 293 finished with value: 0.9646376811594204 and parameters: {'C': 507.3814557996733}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,208]\u001b[0m Trial 294 finished with value: 0.9553277455405116 and parameters: {'C': 852.0656441265855}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,273]\u001b[0m Trial 295 finished with value: 0.9554996523753321 and parameters: {'C': 69.07435130784316}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,339]\u001b[0m Trial 296 finished with value: 0.9645860972876118 and parameters: {'C': 290.9929475407231}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,402]\u001b[0m Trial 297 finished with value: 0.9445567429675069 and parameters: {'C': 7457.742147609695}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,470]\u001b[0m Trial 298 finished with value: 0.9621856887074278 and parameters: {'C': 148.5210370527476}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,534]\u001b[0m Trial 299 finished with value: 0.9623841166284081 and parameters: {'C': 591.7562909857718}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,604]\u001b[0m Trial 300 finished with value: 0.9622906149921295 and parameters: {'C': 432.58686304792263}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,666]\u001b[0m Trial 301 finished with value: 0.9530170998149604 and parameters: {'C': 1898.9815710115936}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,730]\u001b[0m Trial 302 finished with value: 0.9503967093413545 and parameters: {'C': 58.910865907055765}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,808]\u001b[0m Trial 303 finished with value: 0.9621856887074278 and parameters: {'C': 223.16666945631707}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,878]\u001b[0m Trial 304 finished with value: 0.9553277455405116 and parameters: {'C': 950.526088536729}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:17,942]\u001b[0m Trial 305 finished with value: 0.9488578182363241 and parameters: {'C': 3019.7137061707676}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,013]\u001b[0m Trial 306 finished with value: 0.9622906149921295 and parameters: {'C': 433.12336269201603}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,081]\u001b[0m Trial 307 finished with value: 0.9621856887074278 and parameters: {'C': 156.853105542508}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,151]\u001b[0m Trial 308 finished with value: 0.9623841166284081 and parameters: {'C': 591.2869167777749}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,214]\u001b[0m Trial 309 finished with value: 0.9530170998149604 and parameters: {'C': 1921.553721129844}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,281]\u001b[0m Trial 310 finished with value: 0.9621856887074278 and parameters: {'C': 197.98144042502892}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,346]\u001b[0m Trial 311 finished with value: 0.9554463099624391 and parameters: {'C': 106.94648071387314}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,410]\u001b[0m Trial 312 finished with value: 0.952947052947053 and parameters: {'C': 28.924764628881277}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,499]\u001b[0m Trial 313 finished with value: 0.830851418231797 and parameters: {'C': 0.16417286499987271}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,561]\u001b[0m Trial 314 finished with value: 0.9445567429675069 and parameters: {'C': 7813.177867007672}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,624]\u001b[0m Trial 315 finished with value: 0.9445567429675069 and parameters: {'C': 27789.129971293893}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,688]\u001b[0m Trial 316 finished with value: 0.9526549403095668 and parameters: {'C': 1058.8794156741178}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,757]\u001b[0m Trial 317 finished with value: 0.9530992437951481 and parameters: {'C': 84.30578524543994}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,833]\u001b[0m Trial 318 finished with value: 0.9645860972876118 and parameters: {'C': 309.2844005366437}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,905]\u001b[0m Trial 319 finished with value: 0.9526549403095668 and parameters: {'C': 1114.04575401504}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:18,967]\u001b[0m Trial 320 finished with value: 0.9488578182363241 and parameters: {'C': 2961.9785896944495}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,039]\u001b[0m Trial 321 finished with value: 0.9599837080482242 and parameters: {'C': 602.0928671471545}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,109]\u001b[0m Trial 322 finished with value: 0.9597300983208029 and parameters: {'C': 231.65282836207948}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,175]\u001b[0m Trial 323 finished with value: 0.9528522997279794 and parameters: {'C': 44.02576278690347}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,246]\u001b[0m Trial 324 finished with value: 0.9622906149921295 and parameters: {'C': 468.83454107853476}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,314]\u001b[0m Trial 325 finished with value: 0.9530170998149604 and parameters: {'C': 1432.5768069193002}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,379]\u001b[0m Trial 326 finished with value: 0.9554463099624391 and parameters: {'C': 95.7074849576314}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,451]\u001b[0m Trial 327 finished with value: 0.9622906149921295 and parameters: {'C': 416.67020538403153}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,521]\u001b[0m Trial 328 finished with value: 0.9458489140976549 and parameters: {'C': 13.423811765737877}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,587]\u001b[0m Trial 329 finished with value: 0.9621856887074278 and parameters: {'C': 201.83741654855956}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,653]\u001b[0m Trial 330 finished with value: 0.9445567429675069 and parameters: {'C': 5753.102518620884}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,723]\u001b[0m Trial 331 finished with value: 0.9621856887074278 and parameters: {'C': 167.4503761591578}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,795]\u001b[0m Trial 332 finished with value: 0.9553277455405116 and parameters: {'C': 975.7664993480832}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,905]\u001b[0m Trial 333 finished with value: 0.7050641313156418 and parameters: {'C': 0.000336586588273547}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:19,968]\u001b[0m Trial 334 finished with value: 0.9488578182363241 and parameters: {'C': 2846.4195491178584}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,040]\u001b[0m Trial 335 finished with value: 0.9599837080482242 and parameters: {'C': 598.1766162947424}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,105]\u001b[0m Trial 336 finished with value: 0.9460090221887976 and parameters: {'C': 33.72878060604043}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,173]\u001b[0m Trial 337 finished with value: 0.9621856887074278 and parameters: {'C': 206.16402809824814}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,243]\u001b[0m Trial 338 finished with value: 0.9622906149921295 and parameters: {'C': 476.8449312307359}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,310]\u001b[0m Trial 339 finished with value: 0.9530992437951481 and parameters: {'C': 79.39024436150396}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,376]\u001b[0m Trial 340 finished with value: 0.9459004979694635 and parameters: {'C': 5.704943947061006}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,447]\u001b[0m Trial 341 finished with value: 0.9551676374493692 and parameters: {'C': 1174.1969395479264}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,509]\u001b[0m Trial 342 finished with value: 0.9445567429675069 and parameters: {'C': 747432337.3957514}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,571]\u001b[0m Trial 343 finished with value: 0.952947052947053 and parameters: {'C': 28.287393937244495}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,635]\u001b[0m Trial 344 finished with value: 0.9577417922579213 and parameters: {'C': 112.37270553171271}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,702]\u001b[0m Trial 345 finished with value: 0.9645860972876118 and parameters: {'C': 303.79727568905747}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,766]\u001b[0m Trial 346 finished with value: 0.9528522997279794 and parameters: {'C': 50.453982090107765}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,844]\u001b[0m Trial 347 finished with value: 0.9645860972876118 and parameters: {'C': 251.6218794291031}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,913]\u001b[0m Trial 348 finished with value: 0.9445567429675069 and parameters: {'C': 15644.826652044412}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:20,977]\u001b[0m Trial 349 finished with value: 0.9530170998149604 and parameters: {'C': 1750.5236870604317}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,044]\u001b[0m Trial 350 finished with value: 0.9445567429675069 and parameters: {'C': 4317.5570644017425}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,109]\u001b[0m Trial 351 finished with value: 0.9553277455405116 and parameters: {'C': 873.053505644027}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,176]\u001b[0m Trial 352 finished with value: 0.9622906149921295 and parameters: {'C': 420.8166710122318}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,240]\u001b[0m Trial 353 finished with value: 0.9530170998149604 and parameters: {'C': 1852.9896896748176}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,303]\u001b[0m Trial 354 finished with value: 0.9530992437951481 and parameters: {'C': 89.085103951517}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,370]\u001b[0m Trial 355 finished with value: 0.9623841166284081 and parameters: {'C': 581.6231881561737}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,432]\u001b[0m Trial 356 finished with value: 0.95989232989233 and parameters: {'C': 132.15258724927867}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,499]\u001b[0m Trial 357 finished with value: 0.9623841166284081 and parameters: {'C': 588.4257057282136}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,564]\u001b[0m Trial 358 finished with value: 0.9621856887074278 and parameters: {'C': 179.22788396447424}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,629]\u001b[0m Trial 359 finished with value: 0.9530170998149604 and parameters: {'C': 1340.6016644460874}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,693]\u001b[0m Trial 360 finished with value: 0.9503967093413545 and parameters: {'C': 58.734852188378056}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,762]\u001b[0m Trial 361 finished with value: 0.9445567429675069 and parameters: {'C': 4604.93977744535}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,835]\u001b[0m Trial 362 finished with value: 0.9531772079061029 and parameters: {'C': 809.7612602214305}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,914]\u001b[0m Trial 363 finished with value: 0.9621856887074278 and parameters: {'C': 214.44207876355264}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:21,993]\u001b[0m Trial 364 finished with value: 0.8982238667900093 and parameters: {'C': 0.6125408986918174}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,070]\u001b[0m Trial 365 finished with value: 0.9622906149921295 and parameters: {'C': 401.6437472256061}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,139]\u001b[0m Trial 366 finished with value: 0.9530170998149604 and parameters: {'C': 2480.762506406277}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,244]\u001b[0m Trial 367 finished with value: 0.7050641313156418 and parameters: {'C': 4.696679746227247e-09}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,311]\u001b[0m Trial 368 finished with value: 0.9645860972876118 and parameters: {'C': 259.71174321471483}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,381]\u001b[0m Trial 369 finished with value: 0.9645860972876118 and parameters: {'C': 298.6217134773962}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,448]\u001b[0m Trial 370 finished with value: 0.9622906149921295 and parameters: {'C': 489.37684131187495}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,508]\u001b[0m Trial 371 finished with value: 0.9458489140976549 and parameters: {'C': 17.572795988432315}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,572]\u001b[0m Trial 372 finished with value: 0.9445567429675069 and parameters: {'C': 1574366.2334662885}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,643]\u001b[0m Trial 373 finished with value: 0.9503967093413545 and parameters: {'C': 59.48655431830796}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,715]\u001b[0m Trial 374 finished with value: 0.95989232989233 and parameters: {'C': 119.53051187570692}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,782]\u001b[0m Trial 375 finished with value: 0.9530170998149604 and parameters: {'C': 1458.84245720585}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,846]\u001b[0m Trial 376 finished with value: 0.952947052947053 and parameters: {'C': 28.348821172177498}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,920]\u001b[0m Trial 377 finished with value: 0.9445567429675069 and parameters: {'C': 10346.67772045253}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:22,991]\u001b[0m Trial 378 finished with value: 0.9621856887074278 and parameters: {'C': 220.8211825095873}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,065]\u001b[0m Trial 379 finished with value: 0.9553277455405116 and parameters: {'C': 999.4619328679576}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,133]\u001b[0m Trial 380 finished with value: 0.9530170998149604 and parameters: {'C': 2460.1907600668897}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,211]\u001b[0m Trial 381 finished with value: 0.9531525862080411 and parameters: {'C': 67.23389909837695}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,281]\u001b[0m Trial 382 finished with value: 0.9621856887074278 and parameters: {'C': 211.72314689440267}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,354]\u001b[0m Trial 383 finished with value: 0.9531772079061029 and parameters: {'C': 808.9327339200776}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,423]\u001b[0m Trial 384 finished with value: 0.9645860972876118 and parameters: {'C': 303.912065440444}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,492]\u001b[0m Trial 385 finished with value: 0.9622906149921295 and parameters: {'C': 453.13790169046337}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,562]\u001b[0m Trial 386 finished with value: 0.9445567429675069 and parameters: {'C': 73960.29910437198}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,631]\u001b[0m Trial 387 finished with value: 0.9554463099624391 and parameters: {'C': 96.36138507364502}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,698]\u001b[0m Trial 388 finished with value: 0.9445567429675069 and parameters: {'C': 5212.450908704505}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,776]\u001b[0m Trial 389 finished with value: 0.9526549403095668 and parameters: {'C': 1120.5311641483688}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,843]\u001b[0m Trial 390 finished with value: 0.95989232989233 and parameters: {'C': 123.95764736587505}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,925]\u001b[0m Trial 391 finished with value: 0.9622906149921295 and parameters: {'C': 464.92896519815815}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:23,992]\u001b[0m Trial 392 finished with value: 0.9530170998149604 and parameters: {'C': 2544.130480036646}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,059]\u001b[0m Trial 393 finished with value: 0.9460090221887976 and parameters: {'C': 36.45512003989089}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,124]\u001b[0m Trial 394 finished with value: 0.9434449075828386 and parameters: {'C': 8.142638604587683}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,190]\u001b[0m Trial 395 finished with value: 0.95989232989233 and parameters: {'C': 133.3333519356321}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,260]\u001b[0m Trial 396 finished with value: 0.9553277455405116 and parameters: {'C': 914.8498667739381}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,326]\u001b[0m Trial 397 finished with value: 0.9554996523753321 and parameters: {'C': 69.12229477064785}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,393]\u001b[0m Trial 398 finished with value: 0.9645860972876118 and parameters: {'C': 258.2440661050703}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,457]\u001b[0m Trial 399 finished with value: 0.9530170998149604 and parameters: {'C': 1492.3469434040335}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,525]\u001b[0m Trial 400 finished with value: 0.9645860972876118 and parameters: {'C': 372.91213118916045}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,598]\u001b[0m Trial 401 finished with value: 0.9646376811594204 and parameters: {'C': 536.4069809582038}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,674]\u001b[0m Trial 402 finished with value: 0.9576366418809334 and parameters: {'C': 726.9688661159561}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,737]\u001b[0m Trial 403 finished with value: 0.9445567429675069 and parameters: {'C': 4150.134714426547}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,813]\u001b[0m Trial 404 finished with value: 0.9621856887074278 and parameters: {'C': 220.28227682830988}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,879]\u001b[0m Trial 405 finished with value: 0.9530170998149604 and parameters: {'C': 1935.1921316652035}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:24,958]\u001b[0m Trial 406 finished with value: 0.9599837080482242 and parameters: {'C': 615.7349127969641}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,030]\u001b[0m Trial 407 finished with value: 0.9623841166284081 and parameters: {'C': 597.9597784397668}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,096]\u001b[0m Trial 408 finished with value: 0.9554463099624391 and parameters: {'C': 108.20670306397797}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,163]\u001b[0m Trial 409 finished with value: 0.9445567429675069 and parameters: {'C': 8638.740592812508}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,230]\u001b[0m Trial 410 finished with value: 0.9553277455405116 and parameters: {'C': 994.5471128236723}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,297]\u001b[0m Trial 411 finished with value: 0.9503967093413545 and parameters: {'C': 57.3790977542255}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,361]\u001b[0m Trial 412 finished with value: 0.9458489140976549 and parameters: {'C': 21.472547751955535}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,428]\u001b[0m Trial 413 finished with value: 0.9621856887074278 and parameters: {'C': 170.73300495054588}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,498]\u001b[0m Trial 414 finished with value: 0.9645860972876118 and parameters: {'C': 241.24005701284668}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,569]\u001b[0m Trial 415 finished with value: 0.9528522997279794 and parameters: {'C': 62.825581086051976}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,640]\u001b[0m Trial 416 finished with value: 0.9645860972876118 and parameters: {'C': 243.08811052233366}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,713]\u001b[0m Trial 417 finished with value: 0.9621856887074278 and parameters: {'C': 181.31274232283297}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,777]\u001b[0m Trial 418 finished with value: 0.9445567429675069 and parameters: {'C': 4426.086784621829}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,845]\u001b[0m Trial 419 finished with value: 0.9530170998149604 and parameters: {'C': 1803.506554076099}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:25,909]\u001b[0m Trial 420 finished with value: 0.9445567429675069 and parameters: {'C': 21022.537889630043}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,026]\u001b[0m Trial 421 finished with value: 0.7050641313156418 and parameters: {'C': 0.02376503212859481}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,100]\u001b[0m Trial 422 finished with value: 0.9531772079061029 and parameters: {'C': 750.8696465311698}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,169]\u001b[0m Trial 423 finished with value: 0.9645860972876118 and parameters: {'C': 369.9747229428439}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,236]\u001b[0m Trial 424 finished with value: 0.9530170998149604 and parameters: {'C': 2047.798192565587}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,299]\u001b[0m Trial 425 finished with value: 0.9458489140976549 and parameters: {'C': 17.699004221924028}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,371]\u001b[0m Trial 426 finished with value: 0.9622906149921295 and parameters: {'C': 389.59765958944047}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,437]\u001b[0m Trial 427 finished with value: 0.9554463099624391 and parameters: {'C': 91.31130265833197}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,503]\u001b[0m Trial 428 finished with value: 0.9530170998149604 and parameters: {'C': 1293.6377518636343}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,575]\u001b[0m Trial 429 finished with value: 0.9622906149921295 and parameters: {'C': 431.8893750366024}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,643]\u001b[0m Trial 430 finished with value: 0.9445567429675069 and parameters: {'C': 3603.902322198295}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,710]\u001b[0m Trial 431 finished with value: 0.9645860972876118 and parameters: {'C': 369.94816228218474}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,775]\u001b[0m Trial 432 finished with value: 0.9460090221887976 and parameters: {'C': 38.00017746015165}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,848]\u001b[0m Trial 433 finished with value: 0.9553277455405116 and parameters: {'C': 892.9825239696243}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:26,919]\u001b[0m Trial 434 finished with value: 0.9621856887074278 and parameters: {'C': 177.76936560210777}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,035]\u001b[0m Trial 435 finished with value: 0.7050641313156418 and parameters: {'C': 2.5937940224897748e-05}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,101]\u001b[0m Trial 436 finished with value: 0.9530170998149604 and parameters: {'C': 1780.867154683771}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,172]\u001b[0m Trial 437 finished with value: 0.9646376811594204 and parameters: {'C': 533.4314905239158}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,237]\u001b[0m Trial 438 finished with value: 0.9445567429675069 and parameters: {'C': 8126.190361824128}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,301]\u001b[0m Trial 439 finished with value: 0.9577417922579213 and parameters: {'C': 110.7790737119658}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,372]\u001b[0m Trial 440 finished with value: 0.9552362333007494 and parameters: {'C': 742.0544429282744}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,442]\u001b[0m Trial 441 finished with value: 0.9530170998149604 and parameters: {'C': 2366.6761900200354}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,507]\u001b[0m Trial 442 finished with value: 0.9528522997279794 and parameters: {'C': 44.77531228325389}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,574]\u001b[0m Trial 443 finished with value: 0.9621856887074278 and parameters: {'C': 204.5231748217193}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,641]\u001b[0m Trial 444 finished with value: 0.9622906149921295 and parameters: {'C': 485.00241602609566}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,708]\u001b[0m Trial 445 finished with value: 0.9553277455405116 and parameters: {'C': 949.5500929932005}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,775]\u001b[0m Trial 446 finished with value: 0.9434449075828386 and parameters: {'C': 7.855441635466246}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,852]\u001b[0m Trial 447 finished with value: 0.95989232989233 and parameters: {'C': 122.51398802233021}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,917]\u001b[0m Trial 448 finished with value: 0.9445567429675069 and parameters: {'C': 3821.1454946141953}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:27,993]\u001b[0m Trial 449 finished with value: 0.9646376811594204 and parameters: {'C': 518.7471769237695}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,068]\u001b[0m Trial 450 finished with value: 0.9530170998149604 and parameters: {'C': 1279.210443981063}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,139]\u001b[0m Trial 451 finished with value: 0.9576366418809334 and parameters: {'C': 693.0935953421837}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,203]\u001b[0m Trial 452 finished with value: 0.9445567429675069 and parameters: {'C': 12364.651383996657}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,270]\u001b[0m Trial 453 finished with value: 0.9621856887074278 and parameters: {'C': 192.93553961062506}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,340]\u001b[0m Trial 454 finished with value: 0.9530170998149604 and parameters: {'C': 2528.208596793729}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,409]\u001b[0m Trial 455 finished with value: 0.9645860972876118 and parameters: {'C': 354.0854560394215}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,481]\u001b[0m Trial 456 finished with value: 0.9553277455405116 and parameters: {'C': 869.5460662407315}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,549]\u001b[0m Trial 457 finished with value: 0.9528522997279794 and parameters: {'C': 62.760812285379075}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,621]\u001b[0m Trial 458 finished with value: 0.9622906149921295 and parameters: {'C': 439.22112470166167}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,690]\u001b[0m Trial 459 finished with value: 0.9530170998149604 and parameters: {'C': 1456.5699125600484}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,764]\u001b[0m Trial 460 finished with value: 0.9554463099624391 and parameters: {'C': 109.9660742670099}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,833]\u001b[0m Trial 461 finished with value: 0.9645860972876118 and parameters: {'C': 374.20447083690004}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,901]\u001b[0m Trial 462 finished with value: 0.9645860972876118 and parameters: {'C': 281.5274820929743}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:28,965]\u001b[0m Trial 463 finished with value: 0.9445567429675069 and parameters: {'C': 4301.1254711215515}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,043]\u001b[0m Trial 464 finished with value: 0.9506515706515707 and parameters: {'C': 25.28815482947088}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,111]\u001b[0m Trial 465 finished with value: 0.9204665966866712 and parameters: {'C': 2.623961255320897}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,184]\u001b[0m Trial 466 finished with value: 0.9553277455405116 and parameters: {'C': 879.2622154069376}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,248]\u001b[0m Trial 467 finished with value: 0.9445567429675069 and parameters: {'C': 225552.0680714094}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,318]\u001b[0m Trial 468 finished with value: 0.9646376811594204 and parameters: {'C': 528.946434270745}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,384]\u001b[0m Trial 469 finished with value: 0.9530170998149604 and parameters: {'C': 2381.081488396464}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,449]\u001b[0m Trial 470 finished with value: 0.9621856887074278 and parameters: {'C': 157.19394660673663}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,521]\u001b[0m Trial 471 finished with value: 0.9576366418809334 and parameters: {'C': 686.8131988179074}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,586]\u001b[0m Trial 472 finished with value: 0.9528522997279794 and parameters: {'C': 54.94446092867884}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,657]\u001b[0m Trial 473 finished with value: 0.9577417922579213 and parameters: {'C': 110.32860514768365}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,724]\u001b[0m Trial 474 finished with value: 0.9530170998149604 and parameters: {'C': 1268.0344566717213}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,791]\u001b[0m Trial 475 finished with value: 0.95989232989233 and parameters: {'C': 118.882625299251}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,856]\u001b[0m Trial 476 finished with value: 0.9445567429675069 and parameters: {'C': 7949.048054097064}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,923]\u001b[0m Trial 477 finished with value: 0.9621856887074278 and parameters: {'C': 201.87333005147082}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:29,995]\u001b[0m Trial 478 finished with value: 0.9646376811594204 and parameters: {'C': 503.2820777827237}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,072]\u001b[0m Trial 479 finished with value: 0.9530170998149604 and parameters: {'C': 1478.051573402537}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,137]\u001b[0m Trial 480 finished with value: 0.9445567429675069 and parameters: {'C': 3683.6806383112203}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,199]\u001b[0m Trial 481 finished with value: 0.9445567429675069 and parameters: {'C': 39194.75103370165}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,267]\u001b[0m Trial 482 finished with value: 0.9646376811594204 and parameters: {'C': 502.63705438044934}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,335]\u001b[0m Trial 483 finished with value: 0.9553277455405116 and parameters: {'C': 823.7209680110028}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,399]\u001b[0m Trial 484 finished with value: 0.9530170998149604 and parameters: {'C': 1599.7562341419764}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,467]\u001b[0m Trial 485 finished with value: 0.9622906149921295 and parameters: {'C': 474.42986039919583}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,536]\u001b[0m Trial 486 finished with value: 0.9445567429675069 and parameters: {'C': 6308.519237204668}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,647]\u001b[0m Trial 487 finished with value: 0.7050641313156418 and parameters: {'C': 0.0013861982305894641}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,716]\u001b[0m Trial 488 finished with value: 0.9645860972876118 and parameters: {'C': 262.03438693232874}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,786]\u001b[0m Trial 489 finished with value: 0.9646376811594204 and parameters: {'C': 529.4889131875124}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,856]\u001b[0m Trial 490 finished with value: 0.9530170998149604 and parameters: {'C': 2660.475700576944}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:30,928]\u001b[0m Trial 491 finished with value: 0.9576366418809334 and parameters: {'C': 731.8262312723239}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:31,000]\u001b[0m Trial 492 finished with value: 0.9576366418809334 and parameters: {'C': 647.4416169024141}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:31,080]\u001b[0m Trial 493 finished with value: 0.9530170998149604 and parameters: {'C': 1373.5995096495305}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:31,147]\u001b[0m Trial 494 finished with value: 0.9445567429675069 and parameters: {'C': 10537.352805086535}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:31,214]\u001b[0m Trial 495 finished with value: 0.9554996523753321 and parameters: {'C': 71.73729073819744}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:31,278]\u001b[0m Trial 496 finished with value: 0.9445567429675069 and parameters: {'C': 3473.1110374940613}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:31,346]\u001b[0m Trial 497 finished with value: 0.9622906149921295 and parameters: {'C': 436.6913488583574}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:31,415]\u001b[0m Trial 498 finished with value: 0.9621856887074278 and parameters: {'C': 149.68270302658064}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 12:59:31,488]\u001b[0m Trial 499 finished with value: 0.9530170998149604 and parameters: {'C': 1620.2939984085285}. Best is trial 128 with value: 0.9646376811594204.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_SVC = sklearn.svm.SVC(C=514.4366005927387, gamma=\"auto\",probability=True)\n",
        "optimized_SVC.fit(X_over,y_over)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvFiR1QZjBwf",
        "outputId": "bbe2db95-c057-4824-a8d8-6fc74e08df4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=514.4366005927387, gamma='auto', probability=True)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds = optimized_SVC.predict(X_over)\n",
        "f1 = f1_score(train_preds, y_over)\n",
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTNGAetQkv2f",
        "outputId": "5dba02db-a767-4413-931d-a8230b37b5d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9957264957264957"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_SVC.predict(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfBG93L-jBzC",
        "outputId": "fa508544-e824-4cc7-c55d-da86a21c2e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
              "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.where(optimized_SVC.predict(test)==0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_EKiXFBu5GM",
        "outputId": "d793659b-9a4f-4c53-e6ae-6623f534e388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 30,  33,  62,  63,  67,  82,  84,  91, 119]),)"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_SVC.predict_proba(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K88qic0jB1I",
        "outputId": "300fbd8a-ba11-4a2e-96ea-93bdbb30d161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [7.49306825e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [1.42253288e-02, 9.85774671e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [1.26041558e-07, 9.99999874e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.41333300e-06, 9.99996587e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [7.33264283e-01, 2.66735717e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [6.09729624e-01, 3.90270376e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.73054362e-14, 1.00000000e+00],\n",
              "       [1.46980956e-01, 8.53019044e-01],\n",
              "       [4.08083768e-13, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.58874116e-07, 9.99999641e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.41619416e-10, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [2.11066209e-09, 9.99999998e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [1.81336801e-06, 9.99998187e-01],\n",
              "       [4.42098075e-09, 9.99999996e-01],\n",
              "       [6.09674758e-06, 9.99993903e-01],\n",
              "       [1.55241031e-11, 1.00000000e+00],\n",
              "       [3.03482517e-02, 9.69651748e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [1.99933803e-10, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [1.96046384e-02, 9.80395362e-01],\n",
              "       [6.43206746e-01, 3.56793254e-01],\n",
              "       [5.61787806e-01, 4.38212194e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.04399041e-09, 9.99999997e-01],\n",
              "       [5.27344465e-01, 4.72655535e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [9.79293207e-06, 9.99990207e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.51104873e-07, 9.99999649e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [1.53653354e-12, 1.00000000e+00],\n",
              "       [3.98302256e-08, 9.99999960e-01],\n",
              "       [9.84181918e-03, 9.90158181e-01],\n",
              "       [1.03646299e-06, 9.99998964e-01],\n",
              "       [1.15620635e-07, 9.99999884e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [5.61554250e-03, 9.94384457e-01],\n",
              "       [8.51882147e-01, 1.48117853e-01],\n",
              "       [5.31008129e-09, 9.99999995e-01],\n",
              "       [9.40812258e-01, 5.91877417e-02],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [2.07916781e-07, 9.99999792e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [1.62559525e-06, 9.99998374e-01],\n",
              "       [8.95766783e-03, 9.91042332e-01],\n",
              "       [3.95395019e-10, 1.00000000e+00],\n",
              "       [8.93040998e-01, 1.06959002e-01],\n",
              "       [2.09150379e-10, 1.00000000e+00],\n",
              "       [4.98543009e-10, 1.00000000e+00],\n",
              "       [5.16465711e-07, 9.99999484e-01],\n",
              "       [7.06303528e-06, 9.99992937e-01],\n",
              "       [9.72154716e-02, 9.02784528e-01],\n",
              "       [1.26849721e-08, 9.99999987e-01],\n",
              "       [4.67848609e-10, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [6.72682550e-03, 9.93273174e-01],\n",
              "       [1.18027884e-05, 9.99988197e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [9.08254423e-06, 9.99990917e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [4.59410304e-13, 1.00000000e+00],\n",
              "       [4.18177829e-03, 9.95818222e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [6.63073365e-11, 1.00000000e+00],\n",
              "       [2.79566117e-08, 9.99999972e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [1.86469488e-06, 9.99998135e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [4.64384932e-02, 9.53561507e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [4.55266650e-08, 9.99999954e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [4.76938091e-07, 9.99999523e-01],\n",
              "       [7.78314299e-01, 2.21685701e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [6.75291024e-08, 9.99999932e-01],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00],\n",
              "       [3.00000090e-14, 1.00000000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_objective(trial):\n",
        "    params = {\n",
        "      'tol' : trial.suggest_uniform('tol' , 1e-6 , 1e-3),\n",
        "      'C' : trial.suggest_loguniform(\"C\", 1e-2, 1),\n",
        "      'fit_intercept' : trial.suggest_categorical('fit_intercept' , [True, False]),\n",
        "      'solver' : trial.suggest_categorical('solver' , ['lbfgs','liblinear'])\n",
        "    }\n",
        "\n",
        "    model = LogisticRegression(**params, random_state = 2020)\n",
        "    model.fit(X_over,y_over)\n",
        "    score = cross_val_score(model, X_over, y_over, cv=5, scoring=\"f1\")\n",
        "    f1_mean = score.mean()\n",
        "\n",
        "    return f1_mean\n",
        "\n"
      ],
      "metadata": {
        "id": "tK-UWKfuyUnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_study = optuna.create_study(direction='maximize')\n",
        "log_study.optimize(log_objective, n_trials=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TijNiKsyy6WG",
        "outputId": "0994a26a-2a7a-47a5-9bbd-7bf02cec446b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-01 14:04:24,310]\u001b[0m A new study created in memory with name: no-name-3a6f209e-0f4e-47d0-ae50-5c951d6d0124\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:24,451]\u001b[0m Trial 0 finished with value: 0.8734725595367937 and parameters: {'tol': 0.00043145509851018744, 'C': 0.0510977242464464, 'fit_intercept': True, 'solver': 'liblinear'}. Best is trial 0 with value: 0.8734725595367937.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:24,691]\u001b[0m Trial 1 finished with value: 0.889523144941119 and parameters: {'tol': 0.0009533316500320852, 'C': 0.44528738427186193, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.889523144941119.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:24,775]\u001b[0m Trial 2 finished with value: 0.8830815518871743 and parameters: {'tol': 4.383315284589273e-05, 'C': 0.09783447569643346, 'fit_intercept': True, 'solver': 'liblinear'}. Best is trial 1 with value: 0.889523144941119.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:24,928]\u001b[0m Trial 3 finished with value: 0.8542711769250577 and parameters: {'tol': 0.00044681224421110174, 'C': 0.023909972510260693, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.889523144941119.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:25,134]\u001b[0m Trial 4 finished with value: 0.8827901490678792 and parameters: {'tol': 0.0006422369791472657, 'C': 0.1560517589312919, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.889523144941119.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:25,319]\u001b[0m Trial 5 finished with value: 0.86966849750467 and parameters: {'tol': 0.0001451556775811162, 'C': 0.09926752617294886, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.889523144941119.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:25,546]\u001b[0m Trial 6 finished with value: 0.8827901490678792 and parameters: {'tol': 0.0003970568032633221, 'C': 0.11410379878747359, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.889523144941119.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:25,701]\u001b[0m Trial 7 finished with value: 0.8792799449862466 and parameters: {'tol': 0.00070813282874449, 'C': 0.09827619852221653, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 1 with value: 0.889523144941119.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:25,968]\u001b[0m Trial 8 finished with value: 0.8854971196503063 and parameters: {'tol': 0.0006695790689770735, 'C': 0.2292128030247614, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 1 with value: 0.889523144941119.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:26,108]\u001b[0m Trial 9 finished with value: 0.8858080955920661 and parameters: {'tol': 0.0005882787290287829, 'C': 0.1298723616685006, 'fit_intercept': True, 'solver': 'liblinear'}. Best is trial 1 with value: 0.889523144941119.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:26,429]\u001b[0m Trial 10 finished with value: 0.9003166151242586 and parameters: {'tol': 0.000998391492766345, 'C': 0.8960657910555307, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9003166151242586.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:26,722]\u001b[0m Trial 11 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0009991544317260314, 'C': 0.8608481226079361, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9003166151242586.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:27,050]\u001b[0m Trial 12 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000995572144403653, 'C': 0.8362476853748336, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 10 with value: 0.9003166151242586.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:27,474]\u001b[0m Trial 13 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0008405926414198706, 'C': 0.9734837025559959, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:27,685]\u001b[0m Trial 14 finished with value: 0.889523144941119 and parameters: {'tol': 0.000818442804371354, 'C': 0.3973586973409628, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:28,009]\u001b[0m Trial 15 finished with value: 0.889523144941119 and parameters: {'tol': 0.0008256312729247384, 'C': 0.4591337055707085, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:28,196]\u001b[0m Trial 16 finished with value: 0.8284407688531399 and parameters: {'tol': 0.0008222132766700154, 'C': 0.015932134299975392, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:28,472]\u001b[0m Trial 17 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0008928138189174305, 'C': 0.9825480200130172, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:28,595]\u001b[0m Trial 18 finished with value: 0.8874334763164828 and parameters: {'tol': 0.0008581657178229726, 'C': 0.27770761478846306, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:28,862]\u001b[0m Trial 19 finished with value: 0.8888322523491373 and parameters: {'tol': 0.0005435314033982655, 'C': 0.5553684333769459, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:29,160]\u001b[0m Trial 20 finished with value: 0.8725359763444072 and parameters: {'tol': 0.00022928817084863105, 'C': 0.05161637231651582, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:29,588]\u001b[0m Trial 21 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000880915529976859, 'C': 0.8335991190635376, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:29,872]\u001b[0m Trial 22 finished with value: 0.8934557398062081 and parameters: {'tol': 0.0007426364707526528, 'C': 0.6349726279108918, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:30,226]\u001b[0m Trial 23 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0008993979365271472, 'C': 0.996721472633928, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:30,505]\u001b[0m Trial 24 finished with value: 0.8874334763164828 and parameters: {'tol': 0.0007462242800092509, 'C': 0.29950616207275044, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:30,912]\u001b[0m Trial 25 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0008921156534275746, 'C': 0.9767559935619283, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:31,054]\u001b[0m Trial 26 finished with value: 0.8878863373306312 and parameters: {'tol': 0.0009165487446872972, 'C': 0.601012216995601, 'fit_intercept': True, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:31,351]\u001b[0m Trial 27 finished with value: 0.8900980531189908 and parameters: {'tol': 0.000779747694265348, 'C': 0.35027080412272177, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:31,691]\u001b[0m Trial 28 finished with value: 0.8855166927727709 and parameters: {'tol': 0.0003464346543290114, 'C': 0.2196931502003161, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:31,826]\u001b[0m Trial 29 finished with value: 0.8734725595367937 and parameters: {'tol': 0.0006034352975038511, 'C': 0.04888481421308718, 'fit_intercept': True, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:32,138]\u001b[0m Trial 30 finished with value: 0.8907911630037002 and parameters: {'tol': 0.0009027431150640577, 'C': 0.6101133067479411, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:32,392]\u001b[0m Trial 31 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0009186329035555991, 'C': 0.9928562391532829, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:32,714]\u001b[0m Trial 32 finished with value: 0.8974045416447269 and parameters: {'tol': 0.0009095467924355222, 'C': 0.6818317085299782, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:32,980]\u001b[0m Trial 33 finished with value: 0.889523144941119 and parameters: {'tol': 0.0007798030222199596, 'C': 0.4790783230124173, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:33,275]\u001b[0m Trial 34 finished with value: 0.8955951580974937 and parameters: {'tol': 0.0009216771977342283, 'C': 0.7253275210203561, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:33,559]\u001b[0m Trial 35 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0009434135119866552, 'C': 0.9430515161623569, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:33,887]\u001b[0m Trial 36 finished with value: 0.8886902796246263 and parameters: {'tol': 0.0006729456263198148, 'C': 0.4599100941356646, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:34,034]\u001b[0m Trial 37 finished with value: 0.8203079248793534 and parameters: {'tol': 0.0008488201229271478, 'C': 0.010504055724071523, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:34,311]\u001b[0m Trial 38 finished with value: 0.8754757978509913 and parameters: {'tol': 0.0007787036701252818, 'C': 0.06778148869147961, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:34,555]\u001b[0m Trial 39 finished with value: 0.8585524816201225 and parameters: {'tol': 1.6231863377776134e-05, 'C': 0.02831497480468284, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:34,920]\u001b[0m Trial 40 finished with value: 0.8904749786729514 and parameters: {'tol': 0.000713717230910774, 'C': 0.18889293200803478, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:35,333]\u001b[0m Trial 41 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0009598842897099969, 'C': 0.9997375029680882, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:35,780]\u001b[0m Trial 42 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0009567378643823522, 'C': 0.993884203525249, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:36,094]\u001b[0m Trial 43 finished with value: 0.8955951580974937 and parameters: {'tol': 0.0009421380803048804, 'C': 0.7323212722539977, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:36,343]\u001b[0m Trial 44 finished with value: 0.8888322523491373 and parameters: {'tol': 0.0009663515160480429, 'C': 0.5726587150971899, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:36,705]\u001b[0m Trial 45 finished with value: 0.9007611144451625 and parameters: {'tol': 0.0009686550469182881, 'C': 0.76811279164847, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:36,905]\u001b[0m Trial 46 finished with value: 0.8900980531189908 and parameters: {'tol': 0.0009628773255543292, 'C': 0.36641542997104815, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:37,319]\u001b[0m Trial 47 finished with value: 0.889523144941119 and parameters: {'tol': 0.0008858294989925038, 'C': 0.5269310395071933, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:37,709]\u001b[0m Trial 48 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0004868389358341964, 'C': 0.9774544843264622, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:37,985]\u001b[0m Trial 49 finished with value: 0.8974045416447269 and parameters: {'tol': 0.0008772882055535497, 'C': 0.695099910730843, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:38,284]\u001b[0m Trial 50 finished with value: 0.889523144941119 and parameters: {'tol': 0.0008211804373532526, 'C': 0.39366171211362483, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:38,578]\u001b[0m Trial 51 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0008496468619431948, 'C': 0.8146583247935226, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:38,914]\u001b[0m Trial 52 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0009839293173564546, 'C': 0.9591008516325089, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:39,256]\u001b[0m Trial 53 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0007948629807524079, 'C': 0.804822233857529, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:39,930]\u001b[0m Trial 54 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0009787580594203594, 'C': 0.9309363740041116, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:40,137]\u001b[0m Trial 55 finished with value: 0.889523144941119 and parameters: {'tol': 0.000999007373149159, 'C': 0.5148378616008733, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:40,347]\u001b[0m Trial 56 finished with value: 0.8954714172978095 and parameters: {'tol': 0.0009261237225919012, 'C': 0.6736059973557706, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:40,434]\u001b[0m Trial 57 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0008655770775598555, 'C': 0.8395462078042535, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:40,663]\u001b[0m Trial 58 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0001349404022580415, 'C': 0.9694499408344794, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:40,903]\u001b[0m Trial 59 finished with value: 0.8874334763164828 and parameters: {'tol': 0.00013646487286549832, 'C': 0.29531022903147613, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:41,079]\u001b[0m Trial 60 finished with value: 0.8827901490678792 and parameters: {'tol': 0.00024030923724069185, 'C': 0.13218138474549485, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:41,287]\u001b[0m Trial 61 finished with value: 0.8934557398062081 and parameters: {'tol': 0.0008987909912182646, 'C': 0.6233243308903986, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:41,577]\u001b[0m Trial 62 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00033268983658061836, 'C': 0.9827013933807962, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:41,829]\u001b[0m Trial 63 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00010051540593327556, 'C': 0.82446909658103, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:42,055]\u001b[0m Trial 64 finished with value: 0.9007611144451625 and parameters: {'tol': 0.00032267597727328673, 'C': 0.7540726509494221, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:42,295]\u001b[0m Trial 65 finished with value: 0.8860171975610068 and parameters: {'tol': 0.0002566990845565075, 'C': 0.5717014848377924, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:42,563]\u001b[0m Trial 66 finished with value: 0.8954714172978095 and parameters: {'tol': 0.0007444269390921136, 'C': 0.6712175136266932, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:42,657]\u001b[0m Trial 67 finished with value: 0.889523144941119 and parameters: {'tol': 0.000949425384814198, 'C': 0.43093722447275423, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:42,864]\u001b[0m Trial 68 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00045470421288324456, 'C': 0.9969299661871904, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:43,106]\u001b[0m Trial 69 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00045804729584366657, 'C': 0.8541711107613398, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:43,337]\u001b[0m Trial 70 finished with value: 0.8888322523491373 and parameters: {'tol': 0.00040707613073729396, 'C': 0.539746410704423, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:43,594]\u001b[0m Trial 71 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005493276999857632, 'C': 0.9973299094686712, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:43,805]\u001b[0m Trial 72 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005371905622426468, 'C': 0.9768230989726924, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:44,059]\u001b[0m Trial 73 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005472248990977499, 'C': 0.8653707319498158, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:44,221]\u001b[0m Trial 74 finished with value: 0.8955951580974937 and parameters: {'tol': 0.00018071547295200896, 'C': 0.7451829746953075, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:44,450]\u001b[0m Trial 75 finished with value: 0.8860171975610068 and parameters: {'tol': 0.0008324961135591976, 'C': 0.6276519680005601, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:44,623]\u001b[0m Trial 76 finished with value: 0.8792799449862466 and parameters: {'tol': 0.00035799349245777115, 'C': 0.08663381073382428, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:44,834]\u001b[0m Trial 77 finished with value: 0.8955951580974937 and parameters: {'tol': 0.000648348335207396, 'C': 0.7231312777362467, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:44,928]\u001b[0m Trial 78 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0009086920046872091, 'C': 0.8410176464703296, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:45,135]\u001b[0m Trial 79 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005438104364657802, 'C': 0.9007216126754005, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:45,327]\u001b[0m Trial 80 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0006059844936077554, 'C': 0.9656996144271314, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:45,573]\u001b[0m Trial 81 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004577809213412214, 'C': 0.9810339959020702, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:45,838]\u001b[0m Trial 82 finished with value: 0.9007611144451625 and parameters: {'tol': 0.0003057183994090206, 'C': 0.760154194577722, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:46,091]\u001b[0m Trial 83 finished with value: 0.8934557398062081 and parameters: {'tol': 0.0009786431169506024, 'C': 0.644523782412338, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:46,314]\u001b[0m Trial 84 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0009279604153226647, 'C': 0.8627530421995429, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:46,552]\u001b[0m Trial 85 finished with value: 0.889523144941119 and parameters: {'tol': 7.023888403923866e-05, 'C': 0.476838012581532, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:46,768]\u001b[0m Trial 86 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006083657753983092, 'C': 0.9114277254128016, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:46,974]\u001b[0m Trial 87 finished with value: 0.8974045416447269 and parameters: {'tol': 0.00019031746539686228, 'C': 0.7065019063351589, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:47,233]\u001b[0m Trial 88 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0004200462540193549, 'C': 0.7848782617410202, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:47,394]\u001b[0m Trial 89 finished with value: 0.8604692651638345 and parameters: {'tol': 0.0004975565546392797, 'C': 0.029547199273190546, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:47,647]\u001b[0m Trial 90 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00035953982475652054, 'C': 0.9119024256865954, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:47,873]\u001b[0m Trial 91 finished with value: 0.8907911630037002 and parameters: {'tol': 0.0009407119864055664, 'C': 0.595802226586912, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:48,114]\u001b[0m Trial 92 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00027729173106944786, 'C': 0.9848015922410559, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:48,301]\u001b[0m Trial 93 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00039289663957773073, 'C': 0.9950163445891609, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:48,520]\u001b[0m Trial 94 finished with value: 0.9007611144451625 and parameters: {'tol': 0.00030480417778659474, 'C': 0.776758241100885, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:48,704]\u001b[0m Trial 95 finished with value: 0.8974045416447269 and parameters: {'tol': 0.0005686345155381183, 'C': 0.6825734106786197, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:48,809]\u001b[0m Trial 96 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005200551676798994, 'C': 0.8507616452685502, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:49,006]\u001b[0m Trial 97 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0008886793780418161, 'C': 0.8923247249207151, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:49,264]\u001b[0m Trial 98 finished with value: 0.889523144941119 and parameters: {'tol': 0.0006903947744530941, 'C': 0.5131104877356284, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:49,548]\u001b[0m Trial 99 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0007979625381978066, 'C': 0.781021360101674, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:49,848]\u001b[0m Trial 100 finished with value: 0.9023084435205814 and parameters: {'tol': 0.000615872647912871, 'C': 0.959838339492155, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:50,133]\u001b[0m Trial 101 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004636074533657251, 'C': 0.9786570294442553, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:50,309]\u001b[0m Trial 102 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005715966714143544, 'C': 0.981728370350092, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:50,518]\u001b[0m Trial 103 finished with value: 0.8974045416447269 and parameters: {'tol': 0.0005255727477474631, 'C': 0.6992438624413209, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:50,784]\u001b[0m Trial 104 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0003869540707388249, 'C': 0.8204665276596521, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:50,971]\u001b[0m Trial 105 finished with value: 0.9003166151242586 and parameters: {'tol': 3.661666335997058e-06, 'C': 0.8874041934443269, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:51,255]\u001b[0m Trial 106 finished with value: 0.8860171975610068 and parameters: {'tol': 0.0009907364589329796, 'C': 0.630713256503908, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:51,429]\u001b[0m Trial 107 finished with value: 0.9007611144451625 and parameters: {'tol': 0.0008749557759551073, 'C': 0.7536622730146267, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:51,619]\u001b[0m Trial 108 finished with value: 0.8888322523491373 and parameters: {'tol': 0.0004824946241358689, 'C': 0.584094831032275, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:51,705]\u001b[0m Trial 109 finished with value: 0.8827901490678792 and parameters: {'tol': 0.0008467102893496693, 'C': 0.18282104501457894, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:51,923]\u001b[0m Trial 110 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00026653548071765463, 'C': 0.9091316134840209, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:52,113]\u001b[0m Trial 111 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004364393741359198, 'C': 0.9899902198946111, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:52,257]\u001b[0m Trial 112 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004507720824777, 'C': 0.8046787594383639, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:52,408]\u001b[0m Trial 113 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00043122187558938834, 'C': 0.9973368247522195, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:52,550]\u001b[0m Trial 114 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0009526531172400624, 'C': 0.8841117409381254, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:52,709]\u001b[0m Trial 115 finished with value: 0.8955951580974937 and parameters: {'tol': 0.0009196517476845411, 'C': 0.7167099194091455, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:52,855]\u001b[0m Trial 116 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0006289326681035434, 'C': 0.8157306624967138, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:53,002]\u001b[0m Trial 117 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005844151683893115, 'C': 0.8595802635196217, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:53,157]\u001b[0m Trial 118 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0009823996267174463, 'C': 0.9104951966180455, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:53,299]\u001b[0m Trial 119 finished with value: 0.8954714172978095 and parameters: {'tol': 0.0003354326919588332, 'C': 0.6651274289952557, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:53,460]\u001b[0m Trial 120 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0005081731645330998, 'C': 0.9914410731914335, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:53,607]\u001b[0m Trial 121 finished with value: 0.9007611144451625 and parameters: {'tol': 0.000961878816561129, 'C': 0.7577140978563445, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:53,770]\u001b[0m Trial 122 finished with value: 0.9023084435205814 and parameters: {'tol': 0.000408326090041163, 'C': 0.9834111763985357, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:53,918]\u001b[0m Trial 123 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0004018315029365481, 'C': 0.9346914238171484, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:54,070]\u001b[0m Trial 124 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00047384276187313525, 'C': 0.8116081433757857, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:54,217]\u001b[0m Trial 125 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00017767207762760002, 'C': 0.869548448132935, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:54,363]\u001b[0m Trial 126 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00038157499825107173, 'C': 0.9215058221234019, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:54,484]\u001b[0m Trial 127 finished with value: 0.8874334763164828 and parameters: {'tol': 0.00047287253338669104, 'C': 0.2512939839094523, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:54,580]\u001b[0m Trial 128 finished with value: 0.8239984070911905 and parameters: {'tol': 0.00043305027160510476, 'C': 0.01321391693609677, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:54,661]\u001b[0m Trial 129 finished with value: 0.9007611144451625 and parameters: {'tol': 0.00036124973603422327, 'C': 0.7745944761294565, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:54,802]\u001b[0m Trial 130 finished with value: 0.8974045416447269 and parameters: {'tol': 0.0005570704467820302, 'C': 0.6951984431281072, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:54,962]\u001b[0m Trial 131 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004428810231367237, 'C': 0.9857459087588505, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:55,115]\u001b[0m Trial 132 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0009003579798505338, 'C': 0.8332071711951731, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:55,277]\u001b[0m Trial 133 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004619081823373246, 'C': 0.993899359316533, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:55,427]\u001b[0m Trial 134 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0003800706896493476, 'C': 0.9948265594955755, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:55,526]\u001b[0m Trial 135 finished with value: 0.8725359763444072 and parameters: {'tol': 0.0005044164925244828, 'C': 0.048593168891105334, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:55,691]\u001b[0m Trial 136 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006003243654201756, 'C': 0.9014341349280773, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:55,832]\u001b[0m Trial 137 finished with value: 0.8955951580974937 and parameters: {'tol': 0.0008629658216698225, 'C': 0.7276075846833225, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:55,987]\u001b[0m Trial 138 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004260063345729111, 'C': 0.998477477204345, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:56,133]\u001b[0m Trial 139 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005243233952504015, 'C': 0.8978556555875838, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:56,289]\u001b[0m Trial 140 finished with value: 0.8934557398062081 and parameters: {'tol': 0.0009327123035026929, 'C': 0.6438040762696932, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:56,451]\u001b[0m Trial 141 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0003062992655014254, 'C': 0.8030798630380225, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:56,598]\u001b[0m Trial 142 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0009641915619085309, 'C': 0.864177407002584, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:56,756]\u001b[0m Trial 143 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006437908701995545, 'C': 0.937328646723266, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:56,907]\u001b[0m Trial 144 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005694024515594121, 'C': 0.9944133960060415, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:57,055]\u001b[0m Trial 145 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000282994592082733, 'C': 0.7967372312486646, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:57,207]\u001b[0m Trial 146 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0001193422841927616, 'C': 0.998883539793147, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:57,354]\u001b[0m Trial 147 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0006701934866861781, 'C': 0.9998421176586261, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:57,506]\u001b[0m Trial 148 finished with value: 0.8872395068836558 and parameters: {'tol': 0.000718149766684377, 'C': 0.7280089506998837, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:57,662]\u001b[0m Trial 149 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0004159102378894214, 'C': 0.9100648805896061, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:57,817]\u001b[0m Trial 150 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0009434089182793416, 'C': 0.845119613085346, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:57,963]\u001b[0m Trial 151 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0003836919742471404, 'C': 0.928866287076772, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:58,120]\u001b[0m Trial 152 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00044859882376049984, 'C': 0.8431187978168674, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:58,262]\u001b[0m Trial 153 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005760067410964481, 'C': 0.9936806387547897, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:58,399]\u001b[0m Trial 154 finished with value: 0.9007611144451625 and parameters: {'tol': 4.178993660825009e-05, 'C': 0.7653545469584151, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:58,543]\u001b[0m Trial 155 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00036772790090545103, 'C': 0.8987185431687615, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:58,688]\u001b[0m Trial 156 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004428561919795234, 'C': 0.9916017610406901, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:58,845]\u001b[0m Trial 157 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004717838523852949, 'C': 0.801669016006991, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:58,901]\u001b[0m Trial 158 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00042881917677674866, 'C': 0.862519429613143, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:59,039]\u001b[0m Trial 159 finished with value: 0.8974045416447269 and parameters: {'tol': 0.0004080153126469418, 'C': 0.6977614921415883, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:59,191]\u001b[0m Trial 160 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00041058053434307046, 'C': 0.9135001832700066, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:59,339]\u001b[0m Trial 161 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005406285666769314, 'C': 0.9963737747203756, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:59,487]\u001b[0m Trial 162 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006160229098229852, 'C': 0.9019256236831886, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:59,628]\u001b[0m Trial 163 finished with value: 0.9007611144451625 and parameters: {'tol': 0.0005341086623556061, 'C': 0.7780434134919672, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:59,794]\u001b[0m Trial 164 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004918483015495295, 'C': 0.8370210967830303, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:04:59,942]\u001b[0m Trial 165 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00020464961317965053, 'C': 0.9990213126753921, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:00,091]\u001b[0m Trial 166 finished with value: 0.8955951580974937 and parameters: {'tol': 0.00013288243187775358, 'C': 0.7359569531608597, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:00,241]\u001b[0m Trial 167 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005581894695168347, 'C': 0.9984119051115403, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:00,389]\u001b[0m Trial 168 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005623956516844409, 'C': 0.9995931523194274, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:00,534]\u001b[0m Trial 169 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00046612315826516544, 'C': 0.8950610543316916, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:00,698]\u001b[0m Trial 170 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00042656740063595294, 'C': 0.9102310949498429, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:00,858]\u001b[0m Trial 171 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005849488042823763, 'C': 0.9997860851169921, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:01,002]\u001b[0m Trial 172 finished with value: 0.8983191120031601 and parameters: {'tol': 8.930698001240834e-05, 'C': 0.8301681360980113, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:01,150]\u001b[0m Trial 173 finished with value: 0.9023084435205814 and parameters: {'tol': 0.000437762452989358, 'C': 0.9964787916018468, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:01,294]\u001b[0m Trial 174 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0004453846971574243, 'C': 0.9154971030090884, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:01,438]\u001b[0m Trial 175 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0006616805929201637, 'C': 0.8281814783840596, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:01,587]\u001b[0m Trial 176 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00016066621417907076, 'C': 0.9940926677189091, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:01,740]\u001b[0m Trial 177 finished with value: 0.8872395068836558 and parameters: {'tol': 0.00016172334571442772, 'C': 0.7614647672075252, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:01,912]\u001b[0m Trial 178 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005116960740067175, 'C': 0.9960835247608445, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:02,064]\u001b[0m Trial 179 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005146882841973245, 'C': 0.8735234183830171, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:02,208]\u001b[0m Trial 180 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000999424104959321, 'C': 0.7999040205853463, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:02,357]\u001b[0m Trial 181 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00011514612151607246, 'C': 0.9223150364090652, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:02,500]\u001b[0m Trial 182 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00039354462802928674, 'C': 0.9120015230992019, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:02,647]\u001b[0m Trial 183 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0006196840944046373, 'C': 0.9984987181263705, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:02,798]\u001b[0m Trial 184 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000197950067024428, 'C': 0.8476817757500941, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:02,925]\u001b[0m Trial 185 finished with value: 0.8827901490678792 and parameters: {'tol': 0.0005441606730782782, 'C': 0.12524648546225461, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:03,087]\u001b[0m Trial 186 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005631747966905889, 'C': 0.9334087916193196, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:03,235]\u001b[0m Trial 187 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005983921583758153, 'C': 0.8480693968414138, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:03,292]\u001b[0m Trial 188 finished with value: 0.9007611144451625 and parameters: {'tol': 0.00048158894345063703, 'C': 0.7552986876733895, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:03,385]\u001b[0m Trial 189 finished with value: 0.8416384388807069 and parameters: {'tol': 0.00045868077328660006, 'C': 0.01902298936633407, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:03,536]\u001b[0m Trial 190 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00034872706798134154, 'C': 0.9148008888587846, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:03,696]\u001b[0m Trial 191 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005582448252303752, 'C': 0.994196254586129, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:03,926]\u001b[0m Trial 192 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005749908376425729, 'C': 0.9997173757648443, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:04,189]\u001b[0m Trial 193 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005840987265617073, 'C': 0.933772719632063, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:04,352]\u001b[0m Trial 194 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005401672104105447, 'C': 0.9935217855827998, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:04,500]\u001b[0m Trial 195 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000666354651508415, 'C': 0.8291520643620364, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:04,734]\u001b[0m Trial 196 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005627160392041226, 'C': 0.9959199440970524, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:05,066]\u001b[0m Trial 197 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005618284628886054, 'C': 0.8797790365746898, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:05,297]\u001b[0m Trial 198 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00021504892625078247, 'C': 0.9081826468283266, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:05,459]\u001b[0m Trial 199 finished with value: 0.8673937683782397 and parameters: {'tol': 6.523580984737882e-05, 'C': 0.03891969752575647, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:05,669]\u001b[0m Trial 200 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00037542497499266653, 'C': 0.7958286169565822, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:05,899]\u001b[0m Trial 201 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005898510652437152, 'C': 0.9921165025677325, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:06,198]\u001b[0m Trial 202 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00032652382789267553, 'C': 0.8942050496951769, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:06,419]\u001b[0m Trial 203 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00043189671795913244, 'C': 0.920105885954429, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:06,713]\u001b[0m Trial 204 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00045225604920805083, 'C': 0.8316510966724315, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:06,992]\u001b[0m Trial 205 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0001437979424888389, 'C': 0.9979175061711391, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:07,231]\u001b[0m Trial 206 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0003983317280925258, 'C': 0.852326988327593, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:07,418]\u001b[0m Trial 207 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005085882064019766, 'C': 0.9900850147564448, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:07,817]\u001b[0m Trial 208 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005154444701558363, 'C': 0.9937476105770566, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:08,443]\u001b[0m Trial 209 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0005351438053255352, 'C': 0.9967204426980725, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:08,687]\u001b[0m Trial 210 finished with value: 0.8792799449862466 and parameters: {'tol': 0.0005760071274506825, 'C': 0.08123754157120762, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:08,880]\u001b[0m Trial 211 finished with value: 0.8900980531189908 and parameters: {'tol': 0.0004162994280265196, 'C': 0.3318378566738069, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:09,212]\u001b[0m Trial 212 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00013226450230702515, 'C': 0.9068100157420328, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:09,368]\u001b[0m Trial 213 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006357312187361577, 'C': 0.9081706698890925, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:09,519]\u001b[0m Trial 214 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004912713721025006, 'C': 0.7973049080292642, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:09,757]\u001b[0m Trial 215 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0002811927573952598, 'C': 0.9124477484202127, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:10,109]\u001b[0m Trial 216 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0009728252000251942, 'C': 0.9968123648309477, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:10,454]\u001b[0m Trial 217 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000617603820036616, 'C': 0.8417532263364491, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:11,004]\u001b[0m Trial 218 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005904180835100167, 'C': 0.9921403994313904, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:11,486]\u001b[0m Trial 219 finished with value: 0.8955951580974937 and parameters: {'tol': 0.00046606417179097584, 'C': 0.7486770588303772, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:11,865]\u001b[0m Trial 220 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00022982098526233845, 'C': 0.8725276936896698, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:12,290]\u001b[0m Trial 221 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00016533048053174064, 'C': 0.9843930228110308, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:12,716]\u001b[0m Trial 222 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00039439780035293643, 'C': 0.9968850352595856, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:13,083]\u001b[0m Trial 223 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00044434283932378226, 'C': 0.9068194672960174, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:13,476]\u001b[0m Trial 224 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005507166374755262, 'C': 0.9927144261726342, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:13,600]\u001b[0m Trial 225 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0004389549882936889, 'C': 0.9172642613065397, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:13,938]\u001b[0m Trial 226 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0006186998922584933, 'C': 0.998264346510442, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:14,280]\u001b[0m Trial 227 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004229029059927184, 'C': 0.8419290420230089, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:14,722]\u001b[0m Trial 228 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005339692025352787, 'C': 0.9107311894837824, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:15,129]\u001b[0m Trial 229 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0001576394851635839, 'C': 0.8134731752828221, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:15,574]\u001b[0m Trial 230 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005853854773590489, 'C': 0.9060844606653866, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:15,935]\u001b[0m Trial 231 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005612642882693105, 'C': 0.992022468388403, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:16,375]\u001b[0m Trial 232 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005997364249392292, 'C': 0.9942903434421936, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:16,664]\u001b[0m Trial 233 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0004897931340335657, 'C': 0.9077646993497722, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:16,996]\u001b[0m Trial 234 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00011846598270150815, 'C': 0.8383978070188753, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:17,338]\u001b[0m Trial 235 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005779534868748915, 'C': 0.9307899114329363, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:17,700]\u001b[0m Trial 236 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005182825022359392, 'C': 0.9909156818786561, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:18,023]\u001b[0m Trial 237 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005044190383214252, 'C': 0.8439832106017275, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:18,397]\u001b[0m Trial 238 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005507109692494941, 'C': 0.9949553482865316, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:18,650]\u001b[0m Trial 239 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005172863486229328, 'C': 0.9948623226636121, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:18,944]\u001b[0m Trial 240 finished with value: 0.9007611144451625 and parameters: {'tol': 0.000558845302528356, 'C': 0.7739316394886084, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:19,328]\u001b[0m Trial 241 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00016462011904729416, 'C': 0.9984615814108947, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:19,697]\u001b[0m Trial 242 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0003888071522918186, 'C': 0.8998426471311286, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:20,039]\u001b[0m Trial 243 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005415504668493006, 'C': 0.9161979061167519, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:20,295]\u001b[0m Trial 244 finished with value: 0.9023084435205814 and parameters: {'tol': 0.000595290900075023, 'C': 0.993216516736421, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:20,640]\u001b[0m Trial 245 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00044442410005706014, 'C': 0.8661896823222983, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:21,049]\u001b[0m Trial 246 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005726914630752941, 'C': 0.9250636564455685, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:21,387]\u001b[0m Trial 247 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0006332447932289736, 'C': 0.9938732811164895, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:21,738]\u001b[0m Trial 248 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00025801102087112465, 'C': 0.8492889239860021, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:22,154]\u001b[0m Trial 249 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0001379251581136766, 'C': 0.8889764657069502, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:22,533]\u001b[0m Trial 250 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0001504897032736759, 'C': 0.8009487532874544, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:22,924]\u001b[0m Trial 251 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0001093415164213379, 'C': 0.9129625287679259, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:23,287]\u001b[0m Trial 252 finished with value: 0.9003166151242586 and parameters: {'tol': 7.473693443747195e-05, 'C': 0.9181629347971662, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:23,690]\u001b[0m Trial 253 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005405926916294404, 'C': 0.844209803546198, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:23,947]\u001b[0m Trial 254 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005255944992202797, 'C': 0.9899945741328974, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:24,057]\u001b[0m Trial 255 finished with value: 0.9007611144451625 and parameters: {'tol': 0.0004151918821054904, 'C': 0.7656854789793976, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:24,344]\u001b[0m Trial 256 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004804447924016915, 'C': 0.9987159259437206, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:24,640]\u001b[0m Trial 257 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004650398762411586, 'C': 0.8588555776356139, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:24,910]\u001b[0m Trial 258 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0004590257030376915, 'C': 0.9182168329635673, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:25,107]\u001b[0m Trial 259 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005137797164489067, 'C': 0.9942713169067874, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:25,311]\u001b[0m Trial 260 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004318012709546149, 'C': 0.8139053496683077, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:25,564]\u001b[0m Trial 261 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005045758324517249, 'C': 0.9200702459377138, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:25,838]\u001b[0m Trial 262 finished with value: 0.8974045416447269 and parameters: {'tol': 0.00017113107308090522, 'C': 0.7019898162313777, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:26,041]\u001b[0m Trial 263 finished with value: 0.8827901490678792 and parameters: {'tol': 0.0003082849473446991, 'C': 0.20115777980503938, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:26,242]\u001b[0m Trial 264 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0006994680714167943, 'C': 0.8608585002894695, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:26,452]\u001b[0m Trial 265 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005999245011866373, 'C': 0.9161183569832064, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:26,728]\u001b[0m Trial 266 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0006189824324467703, 'C': 0.9957414231382115, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:26,994]\u001b[0m Trial 267 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004056675630218, 'C': 0.7836300429155257, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:27,195]\u001b[0m Trial 268 finished with value: 0.8827901490678792 and parameters: {'tol': 0.0006133853187738043, 'C': 0.15526813105327392, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:27,505]\u001b[0m Trial 269 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00047294185741698436, 'C': 0.9044632323676023, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:27,793]\u001b[0m Trial 270 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005616481045255713, 'C': 0.9325968679925317, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:28,078]\u001b[0m Trial 271 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0009645298500735629, 'C': 0.8203820955607257, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:28,331]\u001b[0m Trial 272 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005251485490904703, 'C': 0.9898839953462357, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:28,610]\u001b[0m Trial 273 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0009928848692877296, 'C': 0.9909960884454518, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:28,903]\u001b[0m Trial 274 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0009754917269564922, 'C': 0.9862390877526785, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:29,005]\u001b[0m Trial 275 finished with value: 0.8955951580974937 and parameters: {'tol': 0.0009970650783310247, 'C': 0.747238619317233, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:29,291]\u001b[0m Trial 276 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0006040623890802792, 'C': 0.859047047999859, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:29,560]\u001b[0m Trial 277 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0007559624893454292, 'C': 0.9173741310949527, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:29,841]\u001b[0m Trial 278 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004862863124718179, 'C': 0.8153856987306197, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:30,078]\u001b[0m Trial 279 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005827815784526708, 'C': 0.9048486809089977, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:30,371]\u001b[0m Trial 280 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004972374482708649, 'C': 0.9974476669885505, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:30,669]\u001b[0m Trial 281 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005455078567690458, 'C': 0.9122901230610654, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:30,879]\u001b[0m Trial 282 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00021007763352819835, 'C': 0.9978762813751141, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:31,095]\u001b[0m Trial 283 finished with value: 0.9007611144451625 and parameters: {'tol': 0.0002776499760641146, 'C': 0.7610654732456051, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:31,379]\u001b[0m Trial 284 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005522351143573282, 'C': 0.8505437552467073, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:31,615]\u001b[0m Trial 285 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006482650897066534, 'C': 0.928886172010945, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:31,851]\u001b[0m Trial 286 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0005933433948624179, 'C': 0.8389132173261505, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:32,106]\u001b[0m Trial 287 finished with value: 0.9003166151242586 and parameters: {'tol': 0.000576825538124389, 'C': 0.9071072419106465, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:32,421]\u001b[0m Trial 288 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005523954082554197, 'C': 0.9953098144208863, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:32,651]\u001b[0m Trial 289 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00043830401409292765, 'C': 0.8678619417162116, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:32,837]\u001b[0m Trial 290 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005285933155423719, 'C': 0.9935232125575111, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:33,064]\u001b[0m Trial 291 finished with value: 0.8982074882960307 and parameters: {'tol': 0.0005193402289920591, 'C': 0.7502735923685674, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:33,354]\u001b[0m Trial 292 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005913263112120582, 'C': 0.8097620913896298, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:33,555]\u001b[0m Trial 293 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0003461072435964049, 'C': 0.9153694046540043, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:33,813]\u001b[0m Trial 294 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0006337007599323392, 'C': 0.9968278487745255, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:33,913]\u001b[0m Trial 295 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0006842370520650099, 'C': 0.8582360089780342, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:34,088]\u001b[0m Trial 296 finished with value: 0.8955951580974937 and parameters: {'tol': 0.0006141120198603104, 'C': 0.7163954283631179, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:34,282]\u001b[0m Trial 297 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0009191198695510072, 'C': 0.8189755084246194, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:34,539]\u001b[0m Trial 298 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00024470607181754586, 'C': 0.9102120201739304, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:34,851]\u001b[0m Trial 299 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005007933982251455, 'C': 0.994293510470275, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:35,149]\u001b[0m Trial 300 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005025958166446815, 'C': 0.9150794520328381, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:35,330]\u001b[0m Trial 301 finished with value: 0.8736471667120153 and parameters: {'tol': 0.0003633523960931233, 'C': 0.059313622745237805, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:35,648]\u001b[0m Trial 302 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005509364074248402, 'C': 0.9928792719212624, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:35,943]\u001b[0m Trial 303 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005616859497608469, 'C': 0.9957441785402539, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:36,170]\u001b[0m Trial 304 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0003771291973954865, 'C': 0.8459308205125738, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:36,460]\u001b[0m Trial 305 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005180374171797676, 'C': 0.9179126049442946, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:36,738]\u001b[0m Trial 306 finished with value: 0.8872395068836558 and parameters: {'tol': 9.410746525454245e-05, 'C': 0.7715792477958912, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:36,897]\u001b[0m Trial 307 finished with value: 0.8827901490678792 and parameters: {'tol': 0.0001810700219024901, 'C': 0.10905723259960735, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:37,014]\u001b[0m Trial 308 finished with value: 0.8208061568061567 and parameters: {'tol': 0.00014970291322381375, 'C': 0.01034260957832083, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:37,276]\u001b[0m Trial 309 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00041199967881656223, 'C': 0.8679752877366979, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:37,542]\u001b[0m Trial 310 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00047994938146946606, 'C': 0.918191471144301, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:37,826]\u001b[0m Trial 311 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000574544514901275, 'C': 0.7862500165068395, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:38,046]\u001b[0m Trial 312 finished with value: 0.9023084435205814 and parameters: {'tol': 0.000480357026296466, 'C': 0.9984718245111416, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:38,328]\u001b[0m Trial 313 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004406634494261497, 'C': 0.9971794279798162, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:38,538]\u001b[0m Trial 314 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00045393600646262476, 'C': 0.8675848535696882, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:38,794]\u001b[0m Trial 315 finished with value: 0.8974045416447269 and parameters: {'tol': 0.0001235524606374758, 'C': 0.6841630530182206, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:38,911]\u001b[0m Trial 316 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006132642392268101, 'C': 0.9121872742252204, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:39,150]\u001b[0m Trial 317 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00010780700626036349, 'C': 0.997001978146654, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:39,402]\u001b[0m Trial 318 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000648237136361508, 'C': 0.7953056158104965, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:39,697]\u001b[0m Trial 319 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0006061298349448011, 'C': 0.8451407971410482, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:39,884]\u001b[0m Trial 320 finished with value: 0.8640870299374053 and parameters: {'tol': 0.0005356659282372345, 'C': 0.03173311746004978, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:40,150]\u001b[0m Trial 321 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005442195938608646, 'C': 0.9147254400247757, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:40,387]\u001b[0m Trial 322 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005691985247418211, 'C': 0.9134072847011689, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:40,592]\u001b[0m Trial 323 finished with value: 0.8874334763164828 and parameters: {'tol': 0.0005286186789432417, 'C': 0.25526702158339787, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:40,821]\u001b[0m Trial 324 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004296997379967974, 'C': 0.8450051719664059, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:41,101]\u001b[0m Trial 325 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0009832124755840338, 'C': 0.9444813606320923, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:41,293]\u001b[0m Trial 326 finished with value: 0.8955951580974937 and parameters: {'tol': 0.0005686554931388643, 'C': 0.7296768860379823, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:41,533]\u001b[0m Trial 327 finished with value: 0.8872395068836558 and parameters: {'tol': 0.000455501153377267, 'C': 0.7986519646070838, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:41,740]\u001b[0m Trial 328 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0006571283438006806, 'C': 0.9934356718393499, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:41,937]\u001b[0m Trial 329 finished with value: 0.889523144941119 and parameters: {'tol': 0.0006562995791247952, 'C': 0.4254864254219863, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:42,205]\u001b[0m Trial 330 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005317486124314072, 'C': 0.9982827978418233, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:42,400]\u001b[0m Trial 331 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005827446180990018, 'C': 0.8986682872544746, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:42,601]\u001b[0m Trial 332 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00014554898349352502, 'C': 0.9140142322618398, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:42,819]\u001b[0m Trial 333 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005840026957750654, 'C': 0.8094787649742111, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:43,061]\u001b[0m Trial 334 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004695660475993528, 'C': 0.8582520955895361, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:43,176]\u001b[0m Trial 335 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005176315256825862, 'C': 0.9102510454893138, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:43,410]\u001b[0m Trial 336 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005082371112782313, 'C': 0.9908703298142574, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:43,639]\u001b[0m Trial 337 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0002024663802586207, 'C': 0.9956392539816903, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:43,935]\u001b[0m Trial 338 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005653976914247643, 'C': 0.8653398802984299, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:44,223]\u001b[0m Trial 339 finished with value: 0.8955951580974937 and parameters: {'tol': 0.0004957340418039206, 'C': 0.7426591144186325, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:44,472]\u001b[0m Trial 340 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005433092215781149, 'C': 0.9963349145684711, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:44,670]\u001b[0m Trial 341 finished with value: 0.9023084435205814 and parameters: {'tol': 0.000522577404225841, 'C': 0.9994756912469054, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:44,933]\u001b[0m Trial 342 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005073799529562984, 'C': 0.9151360966614719, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:45,203]\u001b[0m Trial 343 finished with value: 0.8954714172978095 and parameters: {'tol': 0.0005956063837748089, 'C': 0.6700206533422449, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:45,486]\u001b[0m Trial 344 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005341158348444854, 'C': 0.8087401400408382, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:45,804]\u001b[0m Trial 345 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0002091428726670454, 'C': 0.9160993483330659, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:46,107]\u001b[0m Trial 346 finished with value: 0.8983191120031601 and parameters: {'tol': 5.5015681579874585e-05, 'C': 0.8378984276238118, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:46,302]\u001b[0m Trial 347 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0004626323069409267, 'C': 0.9246823126879072, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:46,587]\u001b[0m Trial 348 finished with value: 0.9007611144451625 and parameters: {'tol': 0.0006323633487683655, 'C': 0.7621786609758714, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:46,720]\u001b[0m Trial 349 finished with value: 0.8507588973750592 and parameters: {'tol': 0.00033276583693102145, 'C': 0.02360765441236369, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:47,002]\u001b[0m Trial 350 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0004895857002660798, 'C': 0.9230274668753228, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:47,252]\u001b[0m Trial 351 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004062144726393609, 'C': 0.8391363759761612, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:47,532]\u001b[0m Trial 352 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004761898983293259, 'C': 0.8571605223950858, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:47,754]\u001b[0m Trial 353 finished with value: 0.9003166151242586 and parameters: {'tol': 0.000419923795060928, 'C': 0.9111593210256123, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:47,870]\u001b[0m Trial 354 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0003409848199287969, 'C': 0.9954003951324301, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:47,981]\u001b[0m Trial 355 finished with value: 0.8792799449862466 and parameters: {'tol': 0.0003508289689513139, 'C': 0.08004215100219403, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:48,261]\u001b[0m Trial 356 finished with value: 0.9023084435205814 and parameters: {'tol': 0.000629586721936425, 'C': 0.999098822052543, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:48,491]\u001b[0m Trial 357 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005629298876156676, 'C': 0.908749880446239, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:48,763]\u001b[0m Trial 358 finished with value: 0.9007611144451625 and parameters: {'tol': 0.000604638301180251, 'C': 0.756163367432819, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:49,009]\u001b[0m Trial 359 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00024143026492453245, 'C': 0.8502027437078858, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:49,221]\u001b[0m Trial 360 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00018856389674600562, 'C': 0.937085208327344, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:49,453]\u001b[0m Trial 361 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005557919100696463, 'C': 0.7945463179399423, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:49,658]\u001b[0m Trial 362 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006730395081240373, 'C': 0.9294992348887933, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:49,938]\u001b[0m Trial 363 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004864772989961292, 'C': 0.8493091329444459, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:50,192]\u001b[0m Trial 364 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005457336948154162, 'C': 0.9952485832583837, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:50,392]\u001b[0m Trial 365 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005169183526906364, 'C': 0.9981502076627038, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:50,566]\u001b[0m Trial 366 finished with value: 0.8042664863014414 and parameters: {'tol': 0.0005154409373937421, 'C': 0.012163401004410964, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:50,825]\u001b[0m Trial 367 finished with value: 0.8974045416447269 and parameters: {'tol': 0.0006182808166409837, 'C': 0.7062769464675657, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:51,104]\u001b[0m Trial 368 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004280140788190556, 'C': 0.999386192423082, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:51,367]\u001b[0m Trial 369 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00038953307573573746, 'C': 0.877663731096597, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:51,548]\u001b[0m Trial 370 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004985948131370563, 'C': 0.790947328889469, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:51,651]\u001b[0m Trial 371 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00030192276202921364, 'C': 0.9102843029408896, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:51,753]\u001b[0m Trial 372 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000617335261517798, 'C': 0.8457019741663571, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:51,988]\u001b[0m Trial 373 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00018334036821577234, 'C': 0.9998020410884679, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:52,180]\u001b[0m Trial 374 finished with value: 0.8827901490678792 and parameters: {'tol': 0.0004608238603128464, 'C': 0.1619928619919813, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:52,379]\u001b[0m Trial 375 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005942849506569584, 'C': 0.915662468066357, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:52,625]\u001b[0m Trial 376 finished with value: 0.9003166151242586 and parameters: {'tol': 3.3094245556800295e-05, 'C': 0.921198199702297, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:52,854]\u001b[0m Trial 377 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0009712279876003442, 'C': 0.7836439995638175, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:53,098]\u001b[0m Trial 378 finished with value: 0.8983191120031601 and parameters: {'tol': 9.26042812712254e-05, 'C': 0.847188542283133, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:53,383]\u001b[0m Trial 379 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0001634265992169739, 'C': 0.9244463649092663, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:53,643]\u001b[0m Trial 380 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0006002947992257481, 'C': 0.9991054202083968, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:53,909]\u001b[0m Trial 381 finished with value: 0.8955951580974937 and parameters: {'tol': 0.0006354558740069798, 'C': 0.7230241976739271, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:54,206]\u001b[0m Trial 382 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004995622102537734, 'C': 0.8545793980933818, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:54,489]\u001b[0m Trial 383 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0003851234747620108, 'C': 0.9998853499018108, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:54,700]\u001b[0m Trial 384 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0005799521559732591, 'C': 0.9073951305825776, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:54,935]\u001b[0m Trial 385 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00039526014832410656, 'C': 0.8006677978025311, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:55,201]\u001b[0m Trial 386 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005598493346683568, 'C': 0.9203829704516592, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:55,438]\u001b[0m Trial 387 finished with value: 0.8954714172978095 and parameters: {'tol': 0.0008066077929399587, 'C': 0.6598472726098441, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:55,672]\u001b[0m Trial 388 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005292448723566844, 'C': 0.8649246829566444, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:55,924]\u001b[0m Trial 389 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00048463942888935603, 'C': 0.9940615603594187, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:56,460]\u001b[0m Trial 390 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005718691849781575, 'C': 0.9214986780173833, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:57,124]\u001b[0m Trial 391 finished with value: 0.8983191120031601 and parameters: {'tol': 7.928306925904181e-05, 'C': 0.8041693592083689, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:57,374]\u001b[0m Trial 392 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005370197159482409, 'C': 0.9974745757939472, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:57,543]\u001b[0m Trial 393 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005862180936413895, 'C': 0.9955081763497511, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:57,695]\u001b[0m Trial 394 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005588840605068263, 'C': 0.8621188746579231, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:57,849]\u001b[0m Trial 395 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0004916042180419205, 'C': 0.9129079877048656, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:57,997]\u001b[0m Trial 396 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0007099623239441895, 'C': 0.9946881540389912, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:58,144]\u001b[0m Trial 397 finished with value: 0.8982074882960307 and parameters: {'tol': 0.0004762103233694639, 'C': 0.7500331850022519, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:58,294]\u001b[0m Trial 398 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005134566791176632, 'C': 0.9981074785495391, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:58,454]\u001b[0m Trial 399 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004540004495404003, 'C': 0.8385405990674301, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:58,602]\u001b[0m Trial 400 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006608417223416349, 'C': 0.9119305057141088, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:58,736]\u001b[0m Trial 401 finished with value: 0.8900980531189908 and parameters: {'tol': 0.0005340676074979024, 'C': 0.3698907237674154, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:58,810]\u001b[0m Trial 402 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000409761521159892, 'C': 0.8472215057284095, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:58,966]\u001b[0m Trial 403 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0002265693692015826, 'C': 0.9201476025315612, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:59,131]\u001b[0m Trial 404 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0004971538450752023, 'C': 0.76175799399785, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:59,305]\u001b[0m Trial 405 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006050704590575685, 'C': 0.9158698325831889, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:59,472]\u001b[0m Trial 406 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00047093132653463154, 'C': 0.9981594808293833, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:59,624]\u001b[0m Trial 407 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00048024229186489496, 'C': 0.8072624886740885, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:59,784]\u001b[0m Trial 408 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006809933873073315, 'C': 0.9224696407496236, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:05:59,932]\u001b[0m Trial 409 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00043747848989415523, 'C': 0.8546949225568331, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:00,106]\u001b[0m Trial 410 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006183887065163108, 'C': 0.9247237397915601, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:00,263]\u001b[0m Trial 411 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00012854786479947074, 'C': 0.9964675197111938, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:00,414]\u001b[0m Trial 412 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005844267496588404, 'C': 0.8567456183893695, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:00,476]\u001b[0m Trial 413 finished with value: 0.8900980531189908 and parameters: {'tol': 0.0005100160656895834, 'C': 0.32156783718887944, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:00,636]\u001b[0m Trial 414 finished with value: 0.9007611144451625 and parameters: {'tol': 0.00042291877505990573, 'C': 0.7772469400025654, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:00,789]\u001b[0m Trial 415 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005493787233941903, 'C': 0.9165021359730957, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:00,953]\u001b[0m Trial 416 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0007748361302019066, 'C': 0.996638859301444, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:01,110]\u001b[0m Trial 417 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005863009427679197, 'C': 0.8463765363121086, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:01,253]\u001b[0m Trial 418 finished with value: 0.8974045416447269 and parameters: {'tol': 0.0005358101018975414, 'C': 0.7063195726682188, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:01,414]\u001b[0m Trial 419 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0007141661985827631, 'C': 0.926933312109235, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:01,583]\u001b[0m Trial 420 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00043261122117345814, 'C': 0.9992461424905065, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:01,737]\u001b[0m Trial 421 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005332755340117007, 'C': 0.9961457963420725, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:01,899]\u001b[0m Trial 422 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005735397312420352, 'C': 0.8571295264226508, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:01,963]\u001b[0m Trial 423 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0003740592639226162, 'C': 0.9996960441968392, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:02,028]\u001b[0m Trial 424 finished with value: 0.8934557398062081 and parameters: {'tol': 0.0003650124045491771, 'C': 0.6164695693503883, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:02,198]\u001b[0m Trial 425 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0006249577525005792, 'C': 0.8985538205158784, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:02,307]\u001b[0m Trial 426 finished with value: 0.8701516846193019 and parameters: {'tol': 0.00048335896892459017, 'C': 0.04208968836637004, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:02,410]\u001b[0m Trial 427 finished with value: 0.838102713437765 and parameters: {'tol': 0.0005486424456652903, 'C': 0.018122335155243138, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:02,579]\u001b[0m Trial 428 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000643905164071401, 'C': 0.7973025180271072, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:02,734]\u001b[0m Trial 429 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00039253268072371824, 'C': 0.9082179915847386, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:02,891]\u001b[0m Trial 430 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00041615640168879214, 'C': 0.8107205526977931, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:02,962]\u001b[0m Trial 431 finished with value: 0.8955951580974937 and parameters: {'tol': 0.00021179404406064447, 'C': 0.7466043031018057, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:03,133]\u001b[0m Trial 432 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00018456078111879606, 'C': 0.9254925151168109, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:03,284]\u001b[0m Trial 433 finished with value: 0.9023084435205814 and parameters: {'tol': 0.000984338301806269, 'C': 0.9975318219041028, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:03,438]\u001b[0m Trial 434 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005862791231961942, 'C': 0.8570391849171387, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:03,604]\u001b[0m Trial 435 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005112730333704274, 'C': 0.9100599466737513, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:03,761]\u001b[0m Trial 436 finished with value: 0.9023084435205814 and parameters: {'tol': 0.000550559139022238, 'C': 0.9946958097508621, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:03,912]\u001b[0m Trial 437 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0001615584335557928, 'C': 0.8625049785282998, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:04,065]\u001b[0m Trial 438 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005511516913004795, 'C': 0.7867873201758783, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:04,217]\u001b[0m Trial 439 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006182707469262609, 'C': 0.9137493659476157, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:04,373]\u001b[0m Trial 440 finished with value: 0.9023084435205814 and parameters: {'tol': 0.000565855899298674, 'C': 0.9981506147026314, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:04,438]\u001b[0m Trial 441 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004454150007128101, 'C': 0.9960450756960694, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:04,505]\u001b[0m Trial 442 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00045025785729756613, 'C': 0.8572601170803218, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:04,647]\u001b[0m Trial 443 finished with value: 0.8689197206884774 and parameters: {'tol': 0.0001159403299420706, 'C': 0.09380718682170662, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:04,811]\u001b[0m Trial 444 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006008133885470522, 'C': 0.9135886582711903, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:04,962]\u001b[0m Trial 445 finished with value: 0.8955951580974937 and parameters: {'tol': 0.00045428138540696894, 'C': 0.7223505610249973, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:05,123]\u001b[0m Trial 446 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005693161562310516, 'C': 0.9192049279966198, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:05,278]\u001b[0m Trial 447 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000476217678332383, 'C': 0.8003508903797855, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:05,345]\u001b[0m Trial 448 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00032215319968095243, 'C': 0.838158224062045, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:05,497]\u001b[0m Trial 449 finished with value: 0.9003166151242586 and parameters: {'tol': 9.549897236515584e-05, 'C': 0.9232839683164409, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:05,655]\u001b[0m Trial 450 finished with value: 0.8954714172978095 and parameters: {'tol': 0.0005127551375395836, 'C': 0.6575048869562881, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:05,823]\u001b[0m Trial 451 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0007588381905353113, 'C': 0.9984084728933744, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:05,986]\u001b[0m Trial 452 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004520460847299594, 'C': 0.9993336143274506, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:06,148]\u001b[0m Trial 453 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0008189674832392942, 'C': 0.9963634326237462, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:06,311]\u001b[0m Trial 454 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005882117679263249, 'C': 0.8407188836917666, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:06,467]\u001b[0m Trial 455 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0002080481329269912, 'C': 0.9127728948511746, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:06,632]\u001b[0m Trial 456 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00026320191172834903, 'C': 0.9180023641923728, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:06,759]\u001b[0m Trial 457 finished with value: 0.8792799449862466 and parameters: {'tol': 0.0004660180805018791, 'C': 0.07125165600323882, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:06,918]\u001b[0m Trial 458 finished with value: 0.8955951580974937 and parameters: {'tol': 0.0006437456605420318, 'C': 0.7456039961363177, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:07,056]\u001b[0m Trial 459 finished with value: 0.8855166927727709 and parameters: {'tol': 0.0004327933747603022, 'C': 0.21892131989384891, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:07,129]\u001b[0m Trial 460 finished with value: 0.8983191120031601 and parameters: {'tol': 0.00037489920230011133, 'C': 0.8032355262040577, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:07,283]\u001b[0m Trial 461 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0006077656626076871, 'C': 0.8787099315279883, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:07,449]\u001b[0m Trial 462 finished with value: 0.8872395068836558 and parameters: {'tol': 0.0006069597855793252, 'C': 0.9978978669300668, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:07,515]\u001b[0m Trial 463 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005655708079791985, 'C': 0.8580137787181987, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:07,698]\u001b[0m Trial 464 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005310168930192352, 'C': 0.9193148382791396, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:07,853]\u001b[0m Trial 465 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005270085732192786, 'C': 0.993995836225101, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:08,013]\u001b[0m Trial 466 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0005203069116422537, 'C': 0.9204342457203195, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:08,168]\u001b[0m Trial 467 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005322639266897555, 'C': 0.7912889729541455, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:08,324]\u001b[0m Trial 468 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0004005087827882695, 'C': 0.8202919372946227, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:08,393]\u001b[0m Trial 469 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00038540832722546104, 'C': 0.9945103442364468, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:08,457]\u001b[0m Trial 470 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0003599210563414049, 'C': 0.9224720447704892, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:08,607]\u001b[0m Trial 471 finished with value: 0.8974045416447269 and parameters: {'tol': 0.0004973297358336036, 'C': 0.7067483111179277, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:08,770]\u001b[0m Trial 472 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0002953689481013259, 'C': 0.8678637559103326, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:08,924]\u001b[0m Trial 473 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00022407675452002974, 'C': 0.9155224404503403, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:09,081]\u001b[0m Trial 474 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004886276610713129, 'C': 0.9976267993306148, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:09,228]\u001b[0m Trial 475 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0002476908375267893, 'C': 0.7913518817349894, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:09,378]\u001b[0m Trial 476 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005028126491618457, 'C': 0.8552975270155518, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:09,534]\u001b[0m Trial 477 finished with value: 0.9003166151242586 and parameters: {'tol': 5.8856320595743834e-05, 'C': 0.9258980840128829, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:09,689]\u001b[0m Trial 478 finished with value: 0.9003166151242586 and parameters: {'tol': 0.000493898683644616, 'C': 0.9223052342874261, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:09,833]\u001b[0m Trial 479 finished with value: 0.8827901490678792 and parameters: {'tol': 0.0004716196128875151, 'C': 0.1362735630105993, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:09,919]\u001b[0m Trial 480 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000552884021404727, 'C': 0.8553473181535894, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:10,072]\u001b[0m Trial 481 finished with value: 0.9003166151242586 and parameters: {'tol': 0.0006788378683116484, 'C': 0.9256331019407897, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:10,230]\u001b[0m Trial 482 finished with value: 0.8872395068836558 and parameters: {'tol': 0.00045733993830892244, 'C': 0.7550133270757089, 'fit_intercept': True, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:10,379]\u001b[0m Trial 483 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0008169610639285556, 'C': 0.8262607957455553, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:10,532]\u001b[0m Trial 484 finished with value: 0.9023084435205814 and parameters: {'tol': 0.000157857234438286, 'C': 0.9980183533727399, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:10,690]\u001b[0m Trial 485 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00044077357825402934, 'C': 0.9970929260787328, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:10,873]\u001b[0m Trial 486 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0004084253089060557, 'C': 0.9947292122820779, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:10,941]\u001b[0m Trial 487 finished with value: 0.889523144941119 and parameters: {'tol': 0.0004096010006309616, 'C': 0.5303506392024722, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:11,108]\u001b[0m Trial 488 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00048382045847600644, 'C': 0.9997300595342672, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:11,268]\u001b[0m Trial 489 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005217115618582715, 'C': 0.8764450423496523, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:11,423]\u001b[0m Trial 490 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00017746021947557253, 'C': 0.9120228327660659, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:11,573]\u001b[0m Trial 491 finished with value: 0.9007611144451625 and parameters: {'tol': 0.0007403482953781235, 'C': 0.772043673761729, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:11,738]\u001b[0m Trial 492 finished with value: 0.8983191120031601 and parameters: {'tol': 0.000747384530266165, 'C': 0.8438836642506348, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:11,907]\u001b[0m Trial 493 finished with value: 0.8955951580974937 and parameters: {'tol': 0.0007048559211984924, 'C': 0.7169707054140715, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:12,067]\u001b[0m Trial 494 finished with value: 0.9003166151242586 and parameters: {'tol': 0.00043121339239124765, 'C': 0.9211241971460097, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:12,228]\u001b[0m Trial 495 finished with value: 0.9023084435205814 and parameters: {'tol': 0.00019383441396631778, 'C': 0.9954150628058009, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:12,291]\u001b[0m Trial 496 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0008668052932961587, 'C': 0.998643618809022, 'fit_intercept': False, 'solver': 'liblinear'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:12,448]\u001b[0m Trial 497 finished with value: 0.8983191120031601 and parameters: {'tol': 0.0005483644276991717, 'C': 0.8550883026298297, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:12,608]\u001b[0m Trial 498 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0005890538761546051, 'C': 0.9965044449198429, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n",
            "\u001b[32m[I 2022-03-01 14:06:12,769]\u001b[0m Trial 499 finished with value: 0.9023084435205814 and parameters: {'tol': 0.0006667115884043779, 'C': 0.9968559689380907, 'fit_intercept': False, 'solver': 'lbfgs'}. Best is trial 13 with value: 0.9023084435205814.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_log = LogisticRegression(tol= 0.0008405926414198706, C= 0.9734837025559959, fit_intercept= False, solver='lbfgs', random_state = 2020)\n",
        "optimized_log.fit(X_over,y_over)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ_hL92nzHrk",
        "outputId": "189c3b24-6d6a-41c6-9d63-cb6fcb4468d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.9734837025559959, fit_intercept=False, random_state=2020,\n",
              "                   tol=0.0008405926414198706)"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_preds = optimized_log.predict(X_over)\n",
        "f1 = f1_score(train_preds, y_over)\n",
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-pEDndpzrKi",
        "outputId": "e592c4fa-514b-49fe-b1dc-2f5948492ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9125799573560767"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_log.predict(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHLSxdb3zwfN",
        "outputId": "2079452f-bb7d-4df6-a113-2c92320e165d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
              "       1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
              "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
              "       0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.where(optimized_log.predict(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkzCR511z1gr",
        "outputId": "df95bf00-8c17-423c-d3b4-9a85f7294daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  0,   1,   4,   5,   6,   7,   9,  10,  12,  14,  16,  17,  18,\n",
              "         19,  22,  23,  24,  25,  26,  27,  28,  29,  31,  32,  33,  34,\n",
              "         35,  36,  37,  38,  39,  41,  42,  43,  44,  45,  46,  47,  49,\n",
              "         50,  51,  53,  54,  55,  57,  59,  61,  62,  64,  65,  66,  69,\n",
              "         70,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
              "         86,  87,  89,  90,  92,  93,  94,  95,  96,  97,  98, 100, 101,\n",
              "        103, 105, 107, 108, 109, 111, 114, 115, 116, 118, 119, 123, 125]),)"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "def LGB_objective(trial):\n",
        "    dtrain = lgb.Dataset(X_over, label=y_over)\n",
        "    param = {\n",
        "        \"objective\": \"binary\",\n",
        "        \"metric\": \"binary_logloss\",\n",
        "        \"verbosity\": -1,\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)\n",
        "    }\n",
        "\n",
        "    model = lgb.LGBMRegressor(**param)\n",
        "    model.fit(X_over,y_over)\n",
        "    score = cross_val_score(model, X_over, y_over, cv=5, scoring=\"f1\")\n",
        "    f1_mean = score.mean()\n",
        "\n",
        "    return f1_mean\n",
        "\n"
      ],
      "metadata": {
        "id": "q9ctwv1hlYGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SZ4fZH_Azvy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LGB_study = optuna.create_study(direction='maximize')\n",
        "LGB_study.optimize(LGB_objective, n_trials=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0nehzCFXlYIy",
        "outputId": "95e6b4fe-a90f-4126-948a-d5d7e8c10642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-03-01 13:34:16,244]\u001b[0m A new study created in memory with name: no-name-f2315d4c-a443-4b2e-863c-df3f230e18f5\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=7.005711010718167, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.005711010718167\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6759787597113704, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6759787597113704\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6010677957890904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6010677957890904\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.927859601485823e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.927859601485823e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.005711010718167, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.005711010718167\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6759787597113704, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6759787597113704\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6010677957890904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6010677957890904\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.927859601485823e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.927859601485823e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.005711010718167, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.005711010718167\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6759787597113704, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6759787597113704\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6010677957890904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6010677957890904\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.927859601485823e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.927859601485823e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.005711010718167, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.005711010718167\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6759787597113704, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6759787597113704\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6010677957890904, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6010677957890904\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.927859601485823e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.927859601485823e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:16,974]\u001b[0m Trial 0 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.6108501411288904e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6108501411288904e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6334878686489814, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6334878686489814\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9647883853154553, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9647883853154553\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.37126138055702e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.37126138055702e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.6108501411288904e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6108501411288904e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6334878686489814, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6334878686489814\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9647883853154553, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9647883853154553\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.37126138055702e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.37126138055702e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.6108501411288904e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6108501411288904e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6334878686489814, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6334878686489814\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9647883853154553, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9647883853154553\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.37126138055702e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.37126138055702e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.6108501411288904e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6108501411288904e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6334878686489814, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6334878686489814\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9647883853154553, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9647883853154553\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.37126138055702e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.37126138055702e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.6108501411288904e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6108501411288904e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6334878686489814, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6334878686489814\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9647883853154553, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9647883853154553\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.37126138055702e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.37126138055702e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:17,661]\u001b[0m Trial 1 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.671101040565796e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.671101040565796e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5633695159067948, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5633695159067948\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6063763921960411, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6063763921960411\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.23265902962404397, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23265902962404397\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.671101040565796e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.671101040565796e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5633695159067948, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5633695159067948\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6063763921960411, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6063763921960411\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.23265902962404397, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23265902962404397\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.671101040565796e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.671101040565796e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5633695159067948, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5633695159067948\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6063763921960411, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6063763921960411\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.23265902962404397, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23265902962404397\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.671101040565796e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.671101040565796e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5633695159067948, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5633695159067948\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6063763921960411, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6063763921960411\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.23265902962404397, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23265902962404397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:18,310]\u001b[0m Trial 2 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.671101040565796e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.671101040565796e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5633695159067948, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5633695159067948\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6063763921960411, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6063763921960411\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.23265902962404397, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.23265902962404397\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0012628974535198285, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0012628974535198285\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5901818192061911, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5901818192061911\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5377002585036396, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5377002585036396\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3377690675722629, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3377690675722629\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0012628974535198285, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0012628974535198285\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5901818192061911, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5901818192061911\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5377002585036396, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5377002585036396\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3377690675722629, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3377690675722629\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0012628974535198285, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0012628974535198285\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5901818192061911, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5901818192061911\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5377002585036396, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5377002585036396\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3377690675722629, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3377690675722629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:19,035]\u001b[0m Trial 3 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0012628974535198285, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0012628974535198285\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5901818192061911, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5901818192061911\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5377002585036396, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5377002585036396\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3377690675722629, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3377690675722629\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0012628974535198285, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0012628974535198285\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5901818192061911, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5901818192061911\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5377002585036396, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5377002585036396\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3377690675722629, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3377690675722629\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.863254360342912e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.863254360342912e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8794381736782555, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8794381736782555\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5398880319067325, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5398880319067325\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.4067751395675725, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4067751395675725\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.863254360342912e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.863254360342912e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8794381736782555, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8794381736782555\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5398880319067325, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5398880319067325\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.4067751395675725, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4067751395675725\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.863254360342912e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.863254360342912e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8794381736782555, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8794381736782555\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5398880319067325, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5398880319067325\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.4067751395675725, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4067751395675725\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.863254360342912e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.863254360342912e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8794381736782555, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8794381736782555\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5398880319067325, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5398880319067325\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.4067751395675725, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4067751395675725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:19,670]\u001b[0m Trial 4 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=4.863254360342912e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.863254360342912e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8794381736782555, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8794381736782555\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5398880319067325, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5398880319067325\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.4067751395675725, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4067751395675725\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.354523133981429, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.354523133981429\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45048433322883913, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45048433322883913\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8095038336988205, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8095038336988205\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00010403769950791446, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010403769950791446\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.354523133981429, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.354523133981429\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45048433322883913, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45048433322883913\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8095038336988205, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8095038336988205\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00010403769950791446, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010403769950791446\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.354523133981429, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.354523133981429\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45048433322883913, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45048433322883913\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8095038336988205, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8095038336988205\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00010403769950791446, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010403769950791446\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.354523133981429, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.354523133981429\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45048433322883913, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45048433322883913\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8095038336988205, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8095038336988205\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00010403769950791446, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010403769950791446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:20,231]\u001b[0m Trial 5 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=5.354523133981429, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.354523133981429\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45048433322883913, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45048433322883913\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8095038336988205, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8095038336988205\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00010403769950791446, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00010403769950791446\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.545906841775079, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.545906841775079\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.714093651982386, subsample=1.0 will be ignored. Current value: bagging_fraction=0.714093651982386\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.43183212741924687, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.43183212741924687\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.776915903327686e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.776915903327686e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.545906841775079, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.545906841775079\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.714093651982386, subsample=1.0 will be ignored. Current value: bagging_fraction=0.714093651982386\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.43183212741924687, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.43183212741924687\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.776915903327686e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.776915903327686e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.545906841775079, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.545906841775079\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.714093651982386, subsample=1.0 will be ignored. Current value: bagging_fraction=0.714093651982386\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.43183212741924687, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.43183212741924687\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.776915903327686e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.776915903327686e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.545906841775079, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.545906841775079\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.714093651982386, subsample=1.0 will be ignored. Current value: bagging_fraction=0.714093651982386\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.43183212741924687, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.43183212741924687\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.776915903327686e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.776915903327686e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:20,769]\u001b[0m Trial 6 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=7.545906841775079, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.545906841775079\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.714093651982386, subsample=1.0 will be ignored. Current value: bagging_fraction=0.714093651982386\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.43183212741924687, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.43183212741924687\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.776915903327686e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.776915903327686e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.585109464456373e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.585109464456373e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7430101705986275, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7430101705986275\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8985132800052927, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8985132800052927\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0005394618308081015, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005394618308081015\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.585109464456373e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.585109464456373e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7430101705986275, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7430101705986275\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8985132800052927, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8985132800052927\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0005394618308081015, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005394618308081015\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.585109464456373e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.585109464456373e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7430101705986275, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7430101705986275\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8985132800052927, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8985132800052927\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0005394618308081015, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005394618308081015\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.585109464456373e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.585109464456373e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7430101705986275, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7430101705986275\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8985132800052927, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8985132800052927\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0005394618308081015, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005394618308081015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:21,394]\u001b[0m Trial 7 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=5.585109464456373e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.585109464456373e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7430101705986275, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7430101705986275\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8985132800052927, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8985132800052927\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0005394618308081015, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0005394618308081015\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0045038431571923, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0045038431571923\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5231907325045867, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5231907325045867\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5367197850412356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5367197850412356\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.002564239799027514, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.002564239799027514\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0045038431571923, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0045038431571923\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5231907325045867, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5231907325045867\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5367197850412356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5367197850412356\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.002564239799027514, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.002564239799027514\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0045038431571923, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0045038431571923\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5231907325045867, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5231907325045867\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5367197850412356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5367197850412356\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.002564239799027514, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.002564239799027514\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0045038431571923, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0045038431571923\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5231907325045867, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5231907325045867\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5367197850412356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5367197850412356\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.002564239799027514, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.002564239799027514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:22,289]\u001b[0m Trial 8 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0045038431571923, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0045038431571923\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5231907325045867, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5231907325045867\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5367197850412356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5367197850412356\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.002564239799027514, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.002564239799027514\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7134611691608168e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7134611691608168e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5880111742364726, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5880111742364726\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9005871802959988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9005871802959988\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3681342097675633e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3681342097675633e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7134611691608168e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7134611691608168e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5880111742364726, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5880111742364726\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9005871802959988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9005871802959988\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3681342097675633e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3681342097675633e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7134611691608168e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7134611691608168e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5880111742364726, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5880111742364726\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9005871802959988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9005871802959988\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3681342097675633e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3681342097675633e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7134611691608168e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7134611691608168e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5880111742364726, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5880111742364726\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9005871802959988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9005871802959988\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3681342097675633e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3681342097675633e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7134611691608168e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7134611691608168e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5880111742364726, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5880111742364726\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9005871802959988, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9005871802959988\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3681342097675633e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3681342097675633e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:22,641]\u001b[0m Trial 9 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.3601605349446307e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3601605349446307e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6938949529328127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6938949529328127\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.405824016951869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.405824016951869\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.457262047216155e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.457262047216155e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3601605349446307e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3601605349446307e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6938949529328127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6938949529328127\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.405824016951869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.405824016951869\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.457262047216155e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.457262047216155e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3601605349446307e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3601605349446307e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6938949529328127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6938949529328127\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.405824016951869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.405824016951869\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.457262047216155e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.457262047216155e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3601605349446307e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3601605349446307e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6938949529328127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6938949529328127\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.405824016951869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.405824016951869\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.457262047216155e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.457262047216155e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3601605349446307e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3601605349446307e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6938949529328127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6938949529328127\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.405824016951869, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.405824016951869\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.457262047216155e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.457262047216155e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:23,985]\u001b[0m Trial 10 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.004150830692488761, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004150830692488761\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8215620272359911, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8215620272359911\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7316070971526354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7316070971526354\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.1310106064042693, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1310106064042693\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004150830692488761, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004150830692488761\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8215620272359911, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8215620272359911\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7316070971526354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7316070971526354\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.1310106064042693, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1310106064042693\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004150830692488761, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004150830692488761\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8215620272359911, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8215620272359911\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7316070971526354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7316070971526354\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.1310106064042693, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1310106064042693\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004150830692488761, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004150830692488761\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8215620272359911, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8215620272359911\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7316070971526354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7316070971526354\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.1310106064042693, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1310106064042693\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004150830692488761, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004150830692488761\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8215620272359911, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8215620272359911\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7316070971526354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7316070971526354\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.1310106064042693, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.1310106064042693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:24,616]\u001b[0m Trial 11 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.1650078981604227e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1650078981604227e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5479133284050903, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5479133284050903\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.764252613525923, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.764252613525923\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7629348889060822, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7629348889060822\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.1650078981604227e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1650078981604227e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5479133284050903, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5479133284050903\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.764252613525923, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.764252613525923\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7629348889060822, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7629348889060822\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.1650078981604227e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1650078981604227e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5479133284050903, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5479133284050903\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.764252613525923, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.764252613525923\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7629348889060822, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7629348889060822\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.1650078981604227e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1650078981604227e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5479133284050903, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5479133284050903\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.764252613525923, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.764252613525923\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7629348889060822, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7629348889060822\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.1650078981604227e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1650078981604227e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5479133284050903, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5479133284050903\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.764252613525923, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.764252613525923\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7629348889060822, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7629348889060822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:25,131]\u001b[0m Trial 12 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=3.976278621345777e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.976278621345777e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5233908706074951, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5233908706074951\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.858354143841068, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.858354143841068\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.06463104061443879, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06463104061443879\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.976278621345777e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.976278621345777e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5233908706074951, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5233908706074951\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.858354143841068, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.858354143841068\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.06463104061443879, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06463104061443879\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.976278621345777e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.976278621345777e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5233908706074951, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5233908706074951\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.858354143841068, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.858354143841068\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.06463104061443879, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06463104061443879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:25,582]\u001b[0m Trial 13 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=3.976278621345777e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.976278621345777e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5233908706074951, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5233908706074951\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.858354143841068, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.858354143841068\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.06463104061443879, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06463104061443879\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.976278621345777e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.976278621345777e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5233908706074951, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5233908706074951\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.858354143841068, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.858354143841068\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.06463104061443879, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06463104061443879\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0013195150499493498, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013195150499493498\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5921757649283766, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5921757649283766\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8919110622655716, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8919110622655716\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.508096927403047e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.508096927403047e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0013195150499493498, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013195150499493498\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5921757649283766, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5921757649283766\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8919110622655716, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8919110622655716\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.508096927403047e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.508096927403047e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0013195150499493498, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013195150499493498\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5921757649283766, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5921757649283766\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8919110622655716, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8919110622655716\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.508096927403047e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.508096927403047e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0013195150499493498, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013195150499493498\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5921757649283766, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5921757649283766\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8919110622655716, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8919110622655716\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.508096927403047e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.508096927403047e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:26,166]\u001b[0m Trial 14 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0013195150499493498, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0013195150499493498\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5921757649283766, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5921757649283766\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8919110622655716, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8919110622655716\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.508096927403047e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.508096927403047e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3410456619836566, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3410456619836566\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4416452698616635, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4416452698616635\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.616853483875008, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.616853483875008\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0012139364698898877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0012139364698898877\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3410456619836566, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3410456619836566\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4416452698616635, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4416452698616635\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.616853483875008, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.616853483875008\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0012139364698898877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0012139364698898877\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3410456619836566, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3410456619836566\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4416452698616635, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4416452698616635\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.616853483875008, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.616853483875008\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0012139364698898877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0012139364698898877\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3410456619836566, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3410456619836566\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4416452698616635, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4416452698616635\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.616853483875008, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.616853483875008\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0012139364698898877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0012139364698898877\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3410456619836566, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3410456619836566\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4416452698616635, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4416452698616635\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.616853483875008, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.616853483875008\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0012139364698898877, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0012139364698898877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:26,848]\u001b[0m Trial 15 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=8.23447799906212e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.23447799906212e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8050302900434663, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8050302900434663\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7471495116191265, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7471495116191265\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00018333107840614362, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00018333107840614362\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.23447799906212e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.23447799906212e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8050302900434663, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8050302900434663\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7471495116191265, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7471495116191265\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00018333107840614362, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00018333107840614362\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.23447799906212e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.23447799906212e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8050302900434663, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8050302900434663\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7471495116191265, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7471495116191265\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00018333107840614362, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00018333107840614362\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.23447799906212e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.23447799906212e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8050302900434663, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8050302900434663\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7471495116191265, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7471495116191265\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00018333107840614362, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00018333107840614362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:27,537]\u001b[0m Trial 16 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=8.23447799906212e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.23447799906212e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8050302900434663, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8050302900434663\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7471495116191265, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7471495116191265\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00018333107840614362, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00018333107840614362\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0007167090765675955, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0007167090765675955\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.590329778303803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.590329778303803\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.972579915211713, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.972579915211713\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014653903596319214, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014653903596319214\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0007167090765675955, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0007167090765675955\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.590329778303803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.590329778303803\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.972579915211713, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.972579915211713\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014653903596319214, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014653903596319214\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0007167090765675955, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0007167090765675955\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.590329778303803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.590329778303803\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.972579915211713, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.972579915211713\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014653903596319214, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014653903596319214\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0007167090765675955, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0007167090765675955\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.590329778303803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.590329778303803\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.972579915211713, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.972579915211713\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014653903596319214, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014653903596319214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:28,571]\u001b[0m Trial 17 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0007167090765675955, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0007167090765675955\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.590329778303803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.590329778303803\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.972579915211713, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.972579915211713\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014653903596319214, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014653903596319214\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.192207161236657e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.192207161236657e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9938114361306506, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9938114361306506\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9829118053016669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9829118053016669\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.424977574278306e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.424977574278306e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.192207161236657e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.192207161236657e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9938114361306506, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9938114361306506\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9829118053016669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9829118053016669\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.424977574278306e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.424977574278306e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.192207161236657e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.192207161236657e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9938114361306506, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9938114361306506\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9829118053016669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9829118053016669\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.424977574278306e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.424977574278306e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:29,217]\u001b[0m Trial 18 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=4.192207161236657e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.192207161236657e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9938114361306506, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9938114361306506\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9829118053016669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9829118053016669\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.424977574278306e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.424977574278306e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.192207161236657e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.192207161236657e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9938114361306506, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9938114361306506\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9829118053016669, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9829118053016669\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.424977574278306e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.424977574278306e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004392109066620835, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004392109066620835\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7769098027773826, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7769098027773826\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8014475779711223, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8014475779711223\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3923184011374678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3923184011374678e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004392109066620835, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004392109066620835\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7769098027773826, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7769098027773826\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8014475779711223, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8014475779711223\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3923184011374678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3923184011374678e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004392109066620835, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004392109066620835\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7769098027773826, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7769098027773826\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8014475779711223, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8014475779711223\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3923184011374678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3923184011374678e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004392109066620835, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004392109066620835\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7769098027773826, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7769098027773826\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8014475779711223, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8014475779711223\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3923184011374678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3923184011374678e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:29,791]\u001b[0m Trial 19 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.004392109066620835, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004392109066620835\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7769098027773826, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7769098027773826\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8014475779711223, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8014475779711223\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3923184011374678e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3923184011374678e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7303308775974486e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7303308775974486e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.43208352058541943, subsample=1.0 will be ignored. Current value: bagging_fraction=0.43208352058541943\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6886275732249143, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6886275732249143\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.369040746189325e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.369040746189325e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7303308775974486e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7303308775974486e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.43208352058541943, subsample=1.0 will be ignored. Current value: bagging_fraction=0.43208352058541943\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6886275732249143, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6886275732249143\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.369040746189325e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.369040746189325e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7303308775974486e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7303308775974486e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.43208352058541943, subsample=1.0 will be ignored. Current value: bagging_fraction=0.43208352058541943\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6886275732249143, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6886275732249143\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.369040746189325e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.369040746189325e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7303308775974486e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7303308775974486e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.43208352058541943, subsample=1.0 will be ignored. Current value: bagging_fraction=0.43208352058541943\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6886275732249143, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6886275732249143\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.369040746189325e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.369040746189325e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7303308775974486e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7303308775974486e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.43208352058541943, subsample=1.0 will be ignored. Current value: bagging_fraction=0.43208352058541943\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6886275732249143, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6886275732249143\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.369040746189325e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.369040746189325e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:30,575]\u001b[0m Trial 20 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=4.799881506054054e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.799881506054054e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9035973397672787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9035973397672787\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7701794583238983, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7701794583238983\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5112163739870884e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5112163739870884e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.799881506054054e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.799881506054054e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9035973397672787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9035973397672787\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7701794583238983, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7701794583238983\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5112163739870884e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5112163739870884e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.799881506054054e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.799881506054054e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9035973397672787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9035973397672787\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7701794583238983, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7701794583238983\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5112163739870884e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5112163739870884e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.799881506054054e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.799881506054054e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9035973397672787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9035973397672787\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7701794583238983, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7701794583238983\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5112163739870884e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5112163739870884e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.799881506054054e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.799881506054054e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9035973397672787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9035973397672787\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7701794583238983, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7701794583238983\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5112163739870884e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5112163739870884e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:31,328]\u001b[0m Trial 21 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.46476372900828294, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.46476372900828294\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6037268705708041, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6037268705708041\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5293631155381856, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5293631155381856\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.369993448986627e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.369993448986627e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.46476372900828294, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.46476372900828294\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6037268705708041, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6037268705708041\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5293631155381856, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5293631155381856\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.369993448986627e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.369993448986627e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.46476372900828294, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.46476372900828294\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6037268705708041, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6037268705708041\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5293631155381856, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5293631155381856\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.369993448986627e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.369993448986627e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:31,787]\u001b[0m Trial 22 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.46476372900828294, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.46476372900828294\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6037268705708041, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6037268705708041\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5293631155381856, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5293631155381856\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.369993448986627e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.369993448986627e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.46476372900828294, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.46476372900828294\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6037268705708041, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6037268705708041\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5293631155381856, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5293631155381856\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.369993448986627e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.369993448986627e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.002359345746342706, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.002359345746342706\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8335732969874923, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8335732969874923\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8588504018650353, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8588504018650353\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.241352242608886e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.241352242608886e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.002359345746342706, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.002359345746342706\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8335732969874923, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8335732969874923\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8588504018650353, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8588504018650353\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.241352242608886e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.241352242608886e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.002359345746342706, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.002359345746342706\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8335732969874923, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8335732969874923\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8588504018650353, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8588504018650353\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.241352242608886e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.241352242608886e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.002359345746342706, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.002359345746342706\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8335732969874923, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8335732969874923\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8588504018650353, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8588504018650353\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.241352242608886e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.241352242608886e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.002359345746342706, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.002359345746342706\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8335732969874923, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8335732969874923\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8588504018650353, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8588504018650353\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.241352242608886e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.241352242608886e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:33,595]\u001b[0m Trial 23 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.1285173445364396e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1285173445364396e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9180255151581658, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9180255151581658\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.647020091916547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.647020091916547\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.05499975863340716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.05499975863340716\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1285173445364396e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1285173445364396e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9180255151581658, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9180255151581658\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.647020091916547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.647020091916547\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.05499975863340716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.05499975863340716\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1285173445364396e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1285173445364396e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9180255151581658, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9180255151581658\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.647020091916547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.647020091916547\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.05499975863340716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.05499975863340716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:33,960]\u001b[0m Trial 24 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.1285173445364396e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1285173445364396e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9180255151581658, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9180255151581658\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.647020091916547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.647020091916547\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.05499975863340716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.05499975863340716\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1285173445364396e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1285173445364396e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9180255151581658, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9180255151581658\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.647020091916547, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.647020091916547\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.05499975863340716, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.05499975863340716\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0015204629889979926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0015204629889979926\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8168681508449448, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8168681508449448\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9629945692169403, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9629945692169403\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.772334526913796e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.772334526913796e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:34,278]\u001b[0m Trial 25 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0015204629889979926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0015204629889979926\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8168681508449448, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8168681508449448\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9629945692169403, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9629945692169403\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.772334526913796e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.772334526913796e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0015204629889979926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0015204629889979926\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8168681508449448, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8168681508449448\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9629945692169403, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9629945692169403\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.772334526913796e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.772334526913796e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0015204629889979926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0015204629889979926\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8168681508449448, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8168681508449448\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9629945692169403, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9629945692169403\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.772334526913796e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.772334526913796e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0015204629889979926, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0015204629889979926\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8168681508449448, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8168681508449448\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9629945692169403, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9629945692169403\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.772334526913796e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.772334526913796e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.6374206254140596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6374206254140596\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8927120746120764, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8927120746120764\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6490015428772974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6490015428772974\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.7188241226906474e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7188241226906474e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.6374206254140596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6374206254140596\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8927120746120764, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8927120746120764\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6490015428772974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6490015428772974\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.7188241226906474e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7188241226906474e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.6374206254140596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6374206254140596\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8927120746120764, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8927120746120764\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6490015428772974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6490015428772974\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.7188241226906474e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7188241226906474e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.6374206254140596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6374206254140596\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8927120746120764, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8927120746120764\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6490015428772974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6490015428772974\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.7188241226906474e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7188241226906474e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.6374206254140596, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.6374206254140596\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8927120746120764, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8927120746120764\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6490015428772974, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6490015428772974\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.7188241226906474e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7188241226906474e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:34,523]\u001b[0m Trial 26 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:34,693]\u001b[0m Trial 27 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.659669206948155, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.659669206948155\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8451686005430122, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8451686005430122\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.48325736397678165, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48325736397678165\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.929506332573827e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.929506332573827e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.659669206948155, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.659669206948155\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8451686005430122, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8451686005430122\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.48325736397678165, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48325736397678165\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.929506332573827e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.929506332573827e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.659669206948155, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.659669206948155\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8451686005430122, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8451686005430122\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.48325736397678165, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48325736397678165\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.929506332573827e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.929506332573827e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.659669206948155, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.659669206948155\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8451686005430122, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8451686005430122\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.48325736397678165, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48325736397678165\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.929506332573827e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.929506332573827e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.659669206948155, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.659669206948155\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8451686005430122, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8451686005430122\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.48325736397678165, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48325736397678165\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.929506332573827e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.929506332573827e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.9659525799124782, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9659525799124782\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5346796328743448, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5346796328743448\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6961442147304358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6961442147304358\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.37703266520242185, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37703266520242185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:34,850]\u001b[0m Trial 28 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.9659525799124782, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9659525799124782\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5346796328743448, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5346796328743448\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6961442147304358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6961442147304358\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.37703266520242185, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37703266520242185\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.9659525799124782, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9659525799124782\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5346796328743448, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5346796328743448\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6961442147304358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6961442147304358\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.37703266520242185, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37703266520242185\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.9659525799124782, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9659525799124782\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5346796328743448, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5346796328743448\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6961442147304358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6961442147304358\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.37703266520242185, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37703266520242185\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.9659525799124782, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9659525799124782\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5346796328743448, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5346796328743448\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6961442147304358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6961442147304358\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.37703266520242185, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37703266520242185\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.15361653650043e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.15361653650043e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4277683009923156, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4277683009923156\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5104008643233023, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5104008643233023\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00018224016367095989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00018224016367095989\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.15361653650043e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.15361653650043e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4277683009923156, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4277683009923156\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5104008643233023, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5104008643233023\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00018224016367095989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00018224016367095989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:35,074]\u001b[0m Trial 29 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=3.15361653650043e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.15361653650043e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4277683009923156, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4277683009923156\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5104008643233023, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5104008643233023\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00018224016367095989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00018224016367095989\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.15361653650043e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.15361653650043e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4277683009923156, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4277683009923156\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5104008643233023, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5104008643233023\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00018224016367095989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00018224016367095989\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.15361653650043e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.15361653650043e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4277683009923156, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4277683009923156\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5104008643233023, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5104008643233023\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00018224016367095989, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00018224016367095989\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0057967767167868825, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0057967767167868825\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9595427895136516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9595427895136516\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.751141360039594, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.751141360039594\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.246248289025873e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.246248289025873e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:35,390]\u001b[0m Trial 30 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0057967767167868825, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0057967767167868825\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9595427895136516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9595427895136516\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.751141360039594, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.751141360039594\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.246248289025873e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.246248289025873e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0057967767167868825, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0057967767167868825\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9595427895136516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9595427895136516\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.751141360039594, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.751141360039594\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.246248289025873e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.246248289025873e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0057967767167868825, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0057967767167868825\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9595427895136516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9595427895136516\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.751141360039594, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.751141360039594\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.246248289025873e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.246248289025873e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0057967767167868825, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0057967767167868825\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9595427895136516, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9595427895136516\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.751141360039594, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.751141360039594\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.246248289025873e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.246248289025873e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.670574935289152e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.670574935289152e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8931283235757876, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8931283235757876\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:35,563]\u001b[0m Trial 31 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4854087030796036, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4854087030796036\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.8982689348573985e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8982689348573985e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.670574935289152e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.670574935289152e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8931283235757876, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8931283235757876\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4854087030796036, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4854087030796036\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.8982689348573985e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8982689348573985e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.670574935289152e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.670574935289152e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8931283235757876, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8931283235757876\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4854087030796036, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4854087030796036\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.8982689348573985e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8982689348573985e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.670574935289152e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.670574935289152e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8931283235757876, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8931283235757876\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4854087030796036, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4854087030796036\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.8982689348573985e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8982689348573985e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.670574935289152e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.670574935289152e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8931283235757876, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8931283235757876\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4854087030796036, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4854087030796036\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.8982689348573985e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8982689348573985e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.8493553480159787, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8493553480159787\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9177011579770934, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9177011579770934\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8631722017850298, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8631722017850298\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07566274015349181, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07566274015349181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:35,877]\u001b[0m Trial 32 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.8493553480159787, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8493553480159787\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9177011579770934, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9177011579770934\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8631722017850298, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8631722017850298\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07566274015349181, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07566274015349181\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.8493553480159787, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8493553480159787\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9177011579770934, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9177011579770934\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8631722017850298, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8631722017850298\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07566274015349181, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07566274015349181\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.8493553480159787, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8493553480159787\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9177011579770934, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9177011579770934\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8631722017850298, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8631722017850298\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07566274015349181, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07566274015349181\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.8493553480159787, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.8493553480159787\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9177011579770934, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9177011579770934\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8631722017850298, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8631722017850298\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07566274015349181, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07566274015349181\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009165080207508197, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009165080207508197\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5084109733796429, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5084109733796429\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7047928694533354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7047928694533354\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.795094152114844e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.795094152114844e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:36,030]\u001b[0m Trial 33 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.009165080207508197, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009165080207508197\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5084109733796429, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5084109733796429\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7047928694533354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7047928694533354\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.795094152114844e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.795094152114844e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009165080207508197, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009165080207508197\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5084109733796429, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5084109733796429\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7047928694533354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7047928694533354\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.795094152114844e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.795094152114844e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009165080207508197, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009165080207508197\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5084109733796429, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5084109733796429\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7047928694533354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7047928694533354\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.795094152114844e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.795094152114844e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009165080207508197, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009165080207508197\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5084109733796429, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5084109733796429\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7047928694533354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7047928694533354\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.795094152114844e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.795094152114844e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.72144221957796e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.72144221957796e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46192023943279115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46192023943279115\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.44662640776137025, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.44662640776137025\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014050093351288444, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014050093351288444\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.72144221957796e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.72144221957796e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46192023943279115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46192023943279115\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.44662640776137025, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.44662640776137025\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014050093351288444, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014050093351288444\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.72144221957796e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.72144221957796e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46192023943279115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46192023943279115\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.44662640776137025, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.44662640776137025\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014050093351288444, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014050093351288444\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.72144221957796e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.72144221957796e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46192023943279115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46192023943279115\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.44662640776137025, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.44662640776137025\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014050093351288444, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014050093351288444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:36,185]\u001b[0m Trial 34 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=6.72144221957796e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.72144221957796e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46192023943279115, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46192023943279115\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.44662640776137025, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.44662640776137025\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014050093351288444, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014050093351288444\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3987445456052406e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3987445456052406e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8743498881627764, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8743498881627764\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5068277289801508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5068277289801508\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.004573736637468515, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.004573736637468515\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3987445456052406e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3987445456052406e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8743498881627764, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8743498881627764\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5068277289801508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5068277289801508\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.004573736637468515, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.004573736637468515\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3987445456052406e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3987445456052406e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8743498881627764, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8743498881627764\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5068277289801508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5068277289801508\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.004573736637468515, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.004573736637468515\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3987445456052406e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3987445456052406e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8743498881627764, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8743498881627764\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5068277289801508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5068277289801508\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.004573736637468515, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.004573736637468515\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3987445456052406e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3987445456052406e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8743498881627764, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8743498881627764\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5068277289801508, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5068277289801508\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.004573736637468515, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.004573736637468515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:36,619]\u001b[0m Trial 35 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=4.318129740045953, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.318129740045953\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.824157627969935, subsample=1.0 will be ignored. Current value: bagging_fraction=0.824157627969935\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4875167383906369, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4875167383906369\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.24924877033768647, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24924877033768647\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.318129740045953, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.318129740045953\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.824157627969935, subsample=1.0 will be ignored. Current value: bagging_fraction=0.824157627969935\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4875167383906369, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4875167383906369\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.24924877033768647, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24924877033768647\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.318129740045953, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.318129740045953\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.824157627969935, subsample=1.0 will be ignored. Current value: bagging_fraction=0.824157627969935\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4875167383906369, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4875167383906369\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.24924877033768647, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24924877033768647\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.318129740045953, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.318129740045953\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.824157627969935, subsample=1.0 will be ignored. Current value: bagging_fraction=0.824157627969935\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4875167383906369, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4875167383906369\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.24924877033768647, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24924877033768647\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.318129740045953, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.318129740045953\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.824157627969935, subsample=1.0 will be ignored. Current value: bagging_fraction=0.824157627969935\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:36,878]\u001b[0m Trial 36 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:37,020]\u001b[0m Trial 37 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] feature_fraction is set=0.4875167383906369, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4875167383906369\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.24924877033768647, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.24924877033768647\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.400410972593438e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.400410972593438e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4414571151115726, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4414571151115726\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.959078551240173, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.959078551240173\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.001029192819814741, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001029192819814741\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.400410972593438e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.400410972593438e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4414571151115726, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4414571151115726\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.959078551240173, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.959078551240173\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.001029192819814741, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001029192819814741\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.400410972593438e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.400410972593438e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4414571151115726, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4414571151115726\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.959078551240173, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.959078551240173\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.001029192819814741, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001029192819814741\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.400410972593438e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.400410972593438e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4414571151115726, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4414571151115726\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.959078551240173, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.959078551240173\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.001029192819814741, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001029192819814741\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.400410972593438e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.400410972593438e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4414571151115726, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4414571151115726\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.959078551240173, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.959078551240173\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.001029192819814741, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001029192819814741\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.5800692852762905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5800692852762905\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9139090859509915, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9139090859509915\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5727154769161096, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5727154769161096\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.975275181668026e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.975275181668026e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:37,326]\u001b[0m Trial 38 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.5800692852762905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5800692852762905\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9139090859509915, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9139090859509915\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5727154769161096, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5727154769161096\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.975275181668026e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.975275181668026e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.5800692852762905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5800692852762905\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9139090859509915, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9139090859509915\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5727154769161096, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5727154769161096\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.975275181668026e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.975275181668026e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.5800692852762905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5800692852762905\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9139090859509915, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9139090859509915\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5727154769161096, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5727154769161096\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.975275181668026e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.975275181668026e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.5800692852762905, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5800692852762905\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9139090859509915, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9139090859509915\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5727154769161096, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5727154769161096\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.975275181668026e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.975275181668026e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.506583633035981e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.506583633035981e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9648916193212214, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9648916193212214\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6381250665523168, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6381250665523168\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.562865841974505, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.562865841974505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:37,638]\u001b[0m Trial 39 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=8.506583633035981e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.506583633035981e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9648916193212214, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9648916193212214\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6381250665523168, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6381250665523168\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.562865841974505, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.562865841974505\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.506583633035981e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.506583633035981e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9648916193212214, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9648916193212214\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6381250665523168, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6381250665523168\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.562865841974505, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.562865841974505\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.506583633035981e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.506583633035981e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9648916193212214, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9648916193212214\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6381250665523168, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6381250665523168\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.562865841974505, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.562865841974505\n",
            "[LightGBM] [Warning] lambda_l1 is set=8.506583633035981e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=8.506583633035981e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9648916193212214, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9648916193212214\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6381250665523168, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6381250665523168\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.562865841974505, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.562865841974505\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.3324619415582528, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3324619415582528\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4277749601848127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4277749601848127\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.646533369003018, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.646533369003018\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.05216896135219292, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.05216896135219292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:37,754]\u001b[0m Trial 40 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.3324619415582528, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3324619415582528\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4277749601848127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4277749601848127\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.646533369003018, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.646533369003018\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.05216896135219292, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.05216896135219292\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.3324619415582528, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3324619415582528\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4277749601848127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4277749601848127\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.646533369003018, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.646533369003018\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.05216896135219292, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.05216896135219292\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.3324619415582528, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3324619415582528\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4277749601848127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4277749601848127\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.646533369003018, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.646533369003018\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.05216896135219292, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.05216896135219292\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.3324619415582528, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.3324619415582528\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4277749601848127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4277749601848127\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.646533369003018, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.646533369003018\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.05216896135219292, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.05216896135219292\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.786358171856948e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.786358171856948e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5434991503496809, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5434991503496809\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6229656440388138, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6229656440388138\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.3415127731980463, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.3415127731980463\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.786358171856948e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.786358171856948e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5434991503496809, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5434991503496809\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6229656440388138, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6229656440388138\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.3415127731980463, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.3415127731980463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:38,062]\u001b[0m Trial 41 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=7.786358171856948e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.786358171856948e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5434991503496809, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5434991503496809\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6229656440388138, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6229656440388138\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.3415127731980463, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.3415127731980463\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.786358171856948e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.786358171856948e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5434991503496809, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5434991503496809\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6229656440388138, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6229656440388138\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.3415127731980463, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.3415127731980463\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.786358171856948e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.786358171856948e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5434991503496809, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5434991503496809\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6229656440388138, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6229656440388138\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.3415127731980463, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.3415127731980463\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1500837281174265, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1500837281174265\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7163332362018355, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7163332362018355\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6163998033201019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6163998033201019\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.003955256793753266, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.003955256793753266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:38,247]\u001b[0m Trial 42 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.1500837281174265, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1500837281174265\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7163332362018355, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7163332362018355\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6163998033201019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6163998033201019\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.003955256793753266, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.003955256793753266\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1500837281174265, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1500837281174265\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7163332362018355, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7163332362018355\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6163998033201019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6163998033201019\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.003955256793753266, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.003955256793753266\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1500837281174265, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1500837281174265\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7163332362018355, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7163332362018355\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6163998033201019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6163998033201019\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.003955256793753266, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.003955256793753266\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1500837281174265, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1500837281174265\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7163332362018355, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7163332362018355\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6163998033201019, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6163998033201019\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.003955256793753266, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.003955256793753266\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.4400631945473436e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.4400631945473436e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5130823020440998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5130823020440998\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8646244684137923, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8646244684137923\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.14558415900929e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.14558415900929e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.4400631945473436e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.4400631945473436e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5130823020440998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5130823020440998\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8646244684137923, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8646244684137923\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.14558415900929e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.14558415900929e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.4400631945473436e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.4400631945473436e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5130823020440998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5130823020440998\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8646244684137923, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8646244684137923\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.14558415900929e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.14558415900929e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.4400631945473436e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.4400631945473436e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5130823020440998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5130823020440998\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8646244684137923, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8646244684137923\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.14558415900929e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.14558415900929e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.4400631945473436e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.4400631945473436e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5130823020440998, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5130823020440998\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8646244684137923, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8646244684137923\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.14558415900929e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.14558415900929e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:38,684]\u001b[0m Trial 43 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:38,823]\u001b[0m Trial 44 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.3780514722413764e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3780514722413764e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5384358944355196, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5384358944355196\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.988931037965999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.988931037965999\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00038081869646087275, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00038081869646087275\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3780514722413764e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3780514722413764e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5384358944355196, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5384358944355196\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.988931037965999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.988931037965999\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00038081869646087275, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00038081869646087275\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3780514722413764e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3780514722413764e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5384358944355196, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5384358944355196\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.988931037965999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.988931037965999\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00038081869646087275, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00038081869646087275\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3780514722413764e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3780514722413764e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5384358944355196, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5384358944355196\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.988931037965999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.988931037965999\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00038081869646087275, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00038081869646087275\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3780514722413764e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3780514722413764e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5384358944355196, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5384358944355196\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.988931037965999, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.988931037965999\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00038081869646087275, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00038081869646087275\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.091654701956873e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.091654701956873e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.44149875693970886, subsample=1.0 will be ignored. Current value: bagging_fraction=0.44149875693970886\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.510297946700283, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.510297946700283\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014205736800566133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014205736800566133\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.091654701956873e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.091654701956873e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.44149875693970886, subsample=1.0 will be ignored. Current value: bagging_fraction=0.44149875693970886\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.510297946700283, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.510297946700283\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014205736800566133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014205736800566133\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.091654701956873e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.091654701956873e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.44149875693970886, subsample=1.0 will be ignored. Current value: bagging_fraction=0.44149875693970886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:38,953]\u001b[0m Trial 45 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:39,106]\u001b[0m Trial 46 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.510297946700283, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.510297946700283\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014205736800566133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014205736800566133\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.091654701956873e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.091654701956873e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.44149875693970886, subsample=1.0 will be ignored. Current value: bagging_fraction=0.44149875693970886\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.510297946700283, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.510297946700283\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014205736800566133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014205736800566133\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.091654701956873e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.091654701956873e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.44149875693970886, subsample=1.0 will be ignored. Current value: bagging_fraction=0.44149875693970886\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.510297946700283, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.510297946700283\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.014205736800566133, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.014205736800566133\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0329789500298874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0329789500298874\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6772207042770233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6772207042770233\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7295374452878745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7295374452878745\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.000286928561538722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.000286928561538722\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0329789500298874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0329789500298874\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6772207042770233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6772207042770233\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7295374452878745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7295374452878745\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.000286928561538722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.000286928561538722\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0329789500298874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0329789500298874\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6772207042770233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6772207042770233\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7295374452878745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7295374452878745\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.000286928561538722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.000286928561538722\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0329789500298874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0329789500298874\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6772207042770233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6772207042770233\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7295374452878745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7295374452878745\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.000286928561538722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.000286928561538722\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0329789500298874, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0329789500298874\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6772207042770233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6772207042770233\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7295374452878745, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7295374452878745\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.000286928561538722, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.000286928561538722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:39,272]\u001b[0m Trial 47 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.0483699750750278e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0483699750750278e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7432210955388969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7432210955388969\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6365099960946141, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6365099960946141\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.8881793878188288e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8881793878188288e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0483699750750278e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0483699750750278e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7432210955388969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7432210955388969\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6365099960946141, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6365099960946141\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.8881793878188288e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8881793878188288e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0483699750750278e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0483699750750278e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7432210955388969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7432210955388969\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6365099960946141, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6365099960946141\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.8881793878188288e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8881793878188288e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0483699750750278e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0483699750750278e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7432210955388969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7432210955388969\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6365099960946141, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6365099960946141\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.8881793878188288e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8881793878188288e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0483699750750278e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0483699750750278e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7432210955388969, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7432210955388969\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6365099960946141, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6365099960946141\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.8881793878188288e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8881793878188288e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0488952370110474e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0488952370110474e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9119025502337239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9119025502337239\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8797430955331813, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8797430955331813\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003300675049889485, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003300675049889485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:39,517]\u001b[0m Trial 48 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.0488952370110474e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0488952370110474e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9119025502337239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9119025502337239\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8797430955331813, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8797430955331813\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003300675049889485, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003300675049889485\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0488952370110474e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0488952370110474e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9119025502337239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9119025502337239\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8797430955331813, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8797430955331813\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003300675049889485, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003300675049889485\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0488952370110474e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0488952370110474e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9119025502337239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9119025502337239\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8797430955331813, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8797430955331813\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003300675049889485, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003300675049889485\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0488952370110474e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0488952370110474e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9119025502337239, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9119025502337239\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8797430955331813, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8797430955331813\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003300675049889485, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003300675049889485\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.3241879175201973e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.3241879175201973e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45399198969254706, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45399198969254706\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6229726258032546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6229726258032546\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.572718790372748e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.572718790372748e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:39,699]\u001b[0m Trial 49 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.3241879175201973e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.3241879175201973e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45399198969254706, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45399198969254706\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6229726258032546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6229726258032546\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.572718790372748e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.572718790372748e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.3241879175201973e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.3241879175201973e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45399198969254706, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45399198969254706\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6229726258032546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6229726258032546\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.572718790372748e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.572718790372748e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.3241879175201973e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.3241879175201973e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45399198969254706, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45399198969254706\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6229726258032546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6229726258032546\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.572718790372748e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.572718790372748e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.3241879175201973e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.3241879175201973e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45399198969254706, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45399198969254706\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6229726258032546, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6229726258032546\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.572718790372748e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.572718790372748e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.13118146664113245, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13118146664113245\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.439954234529217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.439954234529217\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7446963141981323, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7446963141981323\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7017439575789948, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7017439575789948\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.13118146664113245, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13118146664113245\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.439954234529217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.439954234529217\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7446963141981323, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7446963141981323\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7017439575789948, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7017439575789948\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.13118146664113245, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13118146664113245\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.439954234529217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.439954234529217\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7446963141981323, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7446963141981323\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7017439575789948, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7017439575789948\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.13118146664113245, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13118146664113245\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.439954234529217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.439954234529217\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7446963141981323, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7446963141981323\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7017439575789948, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7017439575789948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:39,829]\u001b[0m Trial 50 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.13118146664113245, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.13118146664113245\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.439954234529217, subsample=1.0 will be ignored. Current value: bagging_fraction=0.439954234529217\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7446963141981323, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7446963141981323\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.7017439575789948, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.7017439575789948\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.562430621638707e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.562430621638707e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7236480910650003, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7236480910650003\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8983029979187949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8983029979187949\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.348980391113621e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.348980391113621e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.562430621638707e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.562430621638707e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7236480910650003, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7236480910650003\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8983029979187949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8983029979187949\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.348980391113621e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.348980391113621e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.562430621638707e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.562430621638707e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7236480910650003, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7236480910650003\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8983029979187949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8983029979187949\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.348980391113621e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.348980391113621e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.562430621638707e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.562430621638707e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7236480910650003, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7236480910650003\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8983029979187949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8983029979187949\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.348980391113621e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.348980391113621e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:40,064]\u001b[0m Trial 51 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.562430621638707e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.562430621638707e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7236480910650003, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7236480910650003\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8983029979187949, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8983029979187949\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.348980391113621e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.348980391113621e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0327327880416363, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0327327880416363\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.824959232427899, subsample=1.0 will be ignored. Current value: bagging_fraction=0.824959232427899\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.46476364825063193, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.46476364825063193\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.989960973804734e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.989960973804734e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0327327880416363, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0327327880416363\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.824959232427899, subsample=1.0 will be ignored. Current value: bagging_fraction=0.824959232427899\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.46476364825063193, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.46476364825063193\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.989960973804734e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.989960973804734e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0327327880416363, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0327327880416363\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.824959232427899, subsample=1.0 will be ignored. Current value: bagging_fraction=0.824959232427899\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.46476364825063193, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.46476364825063193\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.989960973804734e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.989960973804734e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0327327880416363, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0327327880416363\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.824959232427899, subsample=1.0 will be ignored. Current value: bagging_fraction=0.824959232427899\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.46476364825063193, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.46476364825063193\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.989960973804734e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.989960973804734e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0327327880416363, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0327327880416363\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.824959232427899, subsample=1.0 will be ignored. Current value: bagging_fraction=0.824959232427899\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.46476364825063193, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.46476364825063193\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.989960973804734e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.989960973804734e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:40,248]\u001b[0m Trial 52 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:40,454]\u001b[0m Trial 53 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=4.4732772973124285e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.4732772973124285e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4334994302215477, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4334994302215477\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9473854891596631, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9473854891596631\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003342454452312702, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003342454452312702\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.4732772973124285e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.4732772973124285e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4334994302215477, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4334994302215477\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9473854891596631, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9473854891596631\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003342454452312702, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003342454452312702\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.4732772973124285e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.4732772973124285e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4334994302215477, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4334994302215477\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9473854891596631, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9473854891596631\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003342454452312702, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003342454452312702\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.4732772973124285e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.4732772973124285e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4334994302215477, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4334994302215477\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9473854891596631, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9473854891596631\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003342454452312702, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003342454452312702\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.4732772973124285e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.4732772973124285e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4334994302215477, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4334994302215477\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9473854891596631, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9473854891596631\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003342454452312702, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003342454452312702\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0012910358863577505, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0012910358863577505\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9305655562186087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9305655562186087\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6637234830768235, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637234830768235\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0647505453251889e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0647505453251889e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:40,792]\u001b[0m Trial 54 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0012910358863577505, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0012910358863577505\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9305655562186087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9305655562186087\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6637234830768235, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637234830768235\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0647505453251889e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0647505453251889e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0012910358863577505, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0012910358863577505\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9305655562186087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9305655562186087\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6637234830768235, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637234830768235\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0647505453251889e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0647505453251889e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0012910358863577505, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0012910358863577505\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9305655562186087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9305655562186087\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6637234830768235, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637234830768235\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0647505453251889e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0647505453251889e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0012910358863577505, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0012910358863577505\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9305655562186087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9305655562186087\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6637234830768235, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6637234830768235\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0647505453251889e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0647505453251889e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.2835743973091634e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.2835743973091634e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8641745376270875, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641745376270875\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8920237013217447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8920237013217447\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00012903311595001532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012903311595001532\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.2835743973091634e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.2835743973091634e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8641745376270875, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641745376270875\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8920237013217447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8920237013217447\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00012903311595001532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012903311595001532\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.2835743973091634e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.2835743973091634e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8641745376270875, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641745376270875\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8920237013217447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8920237013217447\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00012903311595001532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012903311595001532\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.2835743973091634e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.2835743973091634e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8641745376270875, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641745376270875\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8920237013217447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8920237013217447\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00012903311595001532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012903311595001532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:41,080]\u001b[0m Trial 55 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.2835743973091634e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.2835743973091634e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8641745376270875, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8641745376270875\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8920237013217447, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8920237013217447\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00012903311595001532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012903311595001532\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0005211585860904173, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005211585860904173\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8674083528204608, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8674083528204608\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5765230193684482, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5765230193684482\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.8358846295252631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8358846295252631\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0005211585860904173, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005211585860904173\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8674083528204608, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8674083528204608\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5765230193684482, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5765230193684482\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.8358846295252631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8358846295252631\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0005211585860904173, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005211585860904173\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8674083528204608, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8674083528204608\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5765230193684482, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5765230193684482\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.8358846295252631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8358846295252631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:41,365]\u001b[0m Trial 56 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0005211585860904173, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005211585860904173\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8674083528204608, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8674083528204608\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5765230193684482, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5765230193684482\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.8358846295252631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8358846295252631\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0005211585860904173, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0005211585860904173\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8674083528204608, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8674083528204608\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5765230193684482, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5765230193684482\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.8358846295252631, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.8358846295252631\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4091749298155674e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4091749298155674e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7702474182917478, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7702474182917478\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5152318379050693, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5152318379050693\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.09447601881671948, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.09447601881671948\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4091749298155674e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4091749298155674e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7702474182917478, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7702474182917478\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5152318379050693, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5152318379050693\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.09447601881671948, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.09447601881671948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:41,613]\u001b[0m Trial 57 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.4091749298155674e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4091749298155674e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7702474182917478, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7702474182917478\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5152318379050693, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5152318379050693\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.09447601881671948, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.09447601881671948\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4091749298155674e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4091749298155674e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7702474182917478, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7702474182917478\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5152318379050693, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5152318379050693\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.09447601881671948, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.09447601881671948\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4091749298155674e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4091749298155674e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7702474182917478, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7702474182917478\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5152318379050693, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5152318379050693\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.09447601881671948, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.09447601881671948\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0036186714571209634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0036186714571209634\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9355556249822822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9355556249822822\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.729523394406691, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.729523394406691\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0014041630690928323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0014041630690928323\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0036186714571209634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0036186714571209634\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9355556249822822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9355556249822822\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.729523394406691, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.729523394406691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:41,824]\u001b[0m Trial 58 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l2 is set=0.0014041630690928323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0014041630690928323\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0036186714571209634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0036186714571209634\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9355556249822822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9355556249822822\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.729523394406691, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.729523394406691\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0014041630690928323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0014041630690928323\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0036186714571209634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0036186714571209634\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9355556249822822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9355556249822822\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.729523394406691, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.729523394406691\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0014041630690928323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0014041630690928323\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0036186714571209634, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0036186714571209634\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9355556249822822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9355556249822822\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.729523394406691, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.729523394406691\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0014041630690928323, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0014041630690928323\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0002275611264231221, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002275611264231221\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6531021537882993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6531021537882993\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9341334236859653, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9341334236859653\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.4642702919464061, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4642702919464061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:42,073]\u001b[0m Trial 59 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0002275611264231221, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002275611264231221\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6531021537882993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6531021537882993\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9341334236859653, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9341334236859653\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.4642702919464061, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4642702919464061\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0002275611264231221, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002275611264231221\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6531021537882993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6531021537882993\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9341334236859653, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9341334236859653\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.4642702919464061, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4642702919464061\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0002275611264231221, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002275611264231221\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6531021537882993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6531021537882993\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9341334236859653, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9341334236859653\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.4642702919464061, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4642702919464061\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0002275611264231221, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002275611264231221\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6531021537882993, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6531021537882993\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9341334236859653, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9341334236859653\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.4642702919464061, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.4642702919464061\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00012592143338950663, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00012592143338950663\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8788942286124215, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8788942286124215\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.649051710480371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.649051710480371\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.102125748594767e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.102125748594767e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:42,259]\u001b[0m Trial 60 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.00012592143338950663, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00012592143338950663\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8788942286124215, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8788942286124215\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.649051710480371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.649051710480371\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.102125748594767e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.102125748594767e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00012592143338950663, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00012592143338950663\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8788942286124215, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8788942286124215\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.649051710480371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.649051710480371\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.102125748594767e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.102125748594767e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00012592143338950663, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00012592143338950663\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8788942286124215, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8788942286124215\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.649051710480371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.649051710480371\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.102125748594767e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.102125748594767e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00012592143338950663, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00012592143338950663\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8788942286124215, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8788942286124215\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.649051710480371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.649051710480371\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.102125748594767e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.102125748594767e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1250323370570083, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1250323370570083\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9580775565025109, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9580775565025109\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5947347097671356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5947347097671356\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5481450814836097, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5481450814836097\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1250323370570083, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1250323370570083\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9580775565025109, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9580775565025109\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5947347097671356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5947347097671356\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5481450814836097, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5481450814836097\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1250323370570083, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1250323370570083\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9580775565025109, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9580775565025109\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5947347097671356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5947347097671356\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5481450814836097, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5481450814836097\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1250323370570083, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1250323370570083\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9580775565025109, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9580775565025109\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5947347097671356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5947347097671356\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5481450814836097, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5481450814836097\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1250323370570083, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1250323370570083\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9580775565025109, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9580775565025109\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5947347097671356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5947347097671356\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5481450814836097, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5481450814836097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:42,636]\u001b[0m Trial 61 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:42,783]\u001b[0m Trial 62 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.0954624278295675e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.0954624278295675e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.520207095366588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.520207095366588\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.908236634950767, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.908236634950767\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.1859974672019828e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1859974672019828e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.0954624278295675e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.0954624278295675e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.520207095366588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.520207095366588\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.908236634950767, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.908236634950767\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.1859974672019828e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1859974672019828e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.0954624278295675e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.0954624278295675e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.520207095366588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.520207095366588\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.908236634950767, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.908236634950767\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.1859974672019828e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1859974672019828e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.0954624278295675e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.0954624278295675e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.520207095366588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.520207095366588\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.908236634950767, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.908236634950767\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.1859974672019828e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1859974672019828e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.0954624278295675e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.0954624278295675e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.520207095366588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.520207095366588\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.908236634950767, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.908236634950767\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.1859974672019828e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.1859974672019828e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.419438015268215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.419438015268215\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5724533409492142, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5724533409492142\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.48148278929073796, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48148278929073796\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.003884873822599, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.003884873822599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:42,992]\u001b[0m Trial 63 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.419438015268215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.419438015268215\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5724533409492142, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5724533409492142\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.48148278929073796, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48148278929073796\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.003884873822599, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.003884873822599\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.419438015268215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.419438015268215\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5724533409492142, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5724533409492142\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.48148278929073796, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48148278929073796\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.003884873822599, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.003884873822599\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.419438015268215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.419438015268215\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5724533409492142, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5724533409492142\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.48148278929073796, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48148278929073796\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.003884873822599, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.003884873822599\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.419438015268215, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.419438015268215\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5724533409492142, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5724533409492142\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.48148278929073796, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48148278929073796\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.003884873822599, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.003884873822599\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1881884892756107, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1881884892756107\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6389381938831484, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6389381938831484\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9339033196893256, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9339033196893256\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.015055151944426276, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.015055151944426276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:43,214]\u001b[0m Trial 64 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.1881884892756107, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1881884892756107\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6389381938831484, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6389381938831484\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9339033196893256, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9339033196893256\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.015055151944426276, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.015055151944426276\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1881884892756107, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1881884892756107\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6389381938831484, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6389381938831484\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9339033196893256, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9339033196893256\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.015055151944426276, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.015055151944426276\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1881884892756107, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1881884892756107\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6389381938831484, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6389381938831484\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9339033196893256, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9339033196893256\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.015055151944426276, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.015055151944426276\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.1881884892756107, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1881884892756107\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6389381938831484, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6389381938831484\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9339033196893256, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9339033196893256\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.015055151944426276, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.015055151944426276\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.1649575634625194e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1649575634625194e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7479396759725361, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7479396759725361\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.829140777846578, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.829140777846578\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.000642930906244234, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.000642930906244234\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.1649575634625194e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1649575634625194e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7479396759725361, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7479396759725361\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.829140777846578, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.829140777846578\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.000642930906244234, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.000642930906244234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:43,366]\u001b[0m Trial 65 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.1649575634625194e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1649575634625194e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7479396759725361, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7479396759725361\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.829140777846578, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.829140777846578\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.000642930906244234, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.000642930906244234\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.1649575634625194e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1649575634625194e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7479396759725361, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7479396759725361\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.829140777846578, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.829140777846578\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.000642930906244234, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.000642930906244234\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.1649575634625194e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1649575634625194e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7479396759725361, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7479396759725361\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.829140777846578, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.829140777846578\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.000642930906244234, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.000642930906244234\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0051377380823976315, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0051377380823976315\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8458346613508971, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8458346613508971\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5245546997182425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5245546997182425\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.6504960991881988, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6504960991881988\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0051377380823976315, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0051377380823976315\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8458346613508971, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8458346613508971\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5245546997182425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5245546997182425\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.6504960991881988, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6504960991881988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:43,670]\u001b[0m Trial 66 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0051377380823976315, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0051377380823976315\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8458346613508971, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8458346613508971\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5245546997182425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5245546997182425\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.6504960991881988, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6504960991881988\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0051377380823976315, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0051377380823976315\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8458346613508971, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8458346613508971\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5245546997182425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5245546997182425\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.6504960991881988, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6504960991881988\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0051377380823976315, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0051377380823976315\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8458346613508971, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8458346613508971\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5245546997182425, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5245546997182425\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.6504960991881988, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6504960991881988\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4665359912895019e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4665359912895019e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8814478225425654, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8814478225425654\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5858821498620077, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5858821498620077\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3146768119412597e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3146768119412597e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:43,879]\u001b[0m Trial 67 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.4665359912895019e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4665359912895019e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8814478225425654, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8814478225425654\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5858821498620077, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5858821498620077\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3146768119412597e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3146768119412597e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4665359912895019e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4665359912895019e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8814478225425654, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8814478225425654\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5858821498620077, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5858821498620077\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3146768119412597e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3146768119412597e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4665359912895019e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4665359912895019e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8814478225425654, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8814478225425654\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5858821498620077, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5858821498620077\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3146768119412597e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3146768119412597e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4665359912895019e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4665359912895019e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8814478225425654, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8814478225425654\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5858821498620077, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5858821498620077\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.3146768119412597e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.3146768119412597e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.41758252595817186, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41758252595817186\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5245336658185058, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5245336658185058\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8913674134589791, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8913674134589791\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.623755521842877e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.623755521842877e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.41758252595817186, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41758252595817186\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5245336658185058, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5245336658185058\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8913674134589791, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8913674134589791\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.623755521842877e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.623755521842877e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.41758252595817186, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41758252595817186\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5245336658185058, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5245336658185058\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8913674134589791, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8913674134589791\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.623755521842877e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.623755521842877e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.41758252595817186, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41758252595817186\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5245336658185058, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5245336658185058\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8913674134589791, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8913674134589791\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.623755521842877e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.623755521842877e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.41758252595817186, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.41758252595817186\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5245336658185058, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5245336658185058\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8913674134589791, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8913674134589791\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.623755521842877e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.623755521842877e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:44,227]\u001b[0m Trial 68 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:44,402]\u001b[0m Trial 69 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.09189671956783156, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09189671956783156\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7911444141645163, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7911444141645163\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8231229890089875, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8231229890089875\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.41736240524330953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41736240524330953\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.09189671956783156, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09189671956783156\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7911444141645163, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7911444141645163\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8231229890089875, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8231229890089875\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.41736240524330953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41736240524330953\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.09189671956783156, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09189671956783156\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7911444141645163, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7911444141645163\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8231229890089875, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8231229890089875\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.41736240524330953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41736240524330953\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.09189671956783156, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09189671956783156\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7911444141645163, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7911444141645163\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8231229890089875, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8231229890089875\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.41736240524330953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41736240524330953\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.09189671956783156, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09189671956783156\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7911444141645163, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7911444141645163\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8231229890089875, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8231229890089875\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.41736240524330953, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.41736240524330953\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.7644840177133437e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.7644840177133437e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7459180902090986, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7459180902090986\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.592892829184689, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.592892829184689\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0205154872198563, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0205154872198563\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.7644840177133437e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.7644840177133437e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7459180902090986, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7459180902090986\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.592892829184689, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.592892829184689\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0205154872198563, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0205154872198563\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.7644840177133437e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.7644840177133437e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7459180902090986, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7459180902090986\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.592892829184689, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.592892829184689\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0205154872198563, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0205154872198563\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.7644840177133437e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.7644840177133437e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7459180902090986, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7459180902090986\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.592892829184689, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.592892829184689\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0205154872198563, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0205154872198563\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.7644840177133437e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.7644840177133437e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:44,815]\u001b[0m Trial 70 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] bagging_fraction is set=0.7459180902090986, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7459180902090986\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.592892829184689, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.592892829184689\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0205154872198563, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0205154872198563\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.14724246300136026, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.14724246300136026\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7214884529018004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7214884529018004\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5117488759727131, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5117488759727131\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.026397590845211337, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.026397590845211337\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.14724246300136026, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.14724246300136026\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7214884529018004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7214884529018004\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5117488759727131, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5117488759727131\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.026397590845211337, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.026397590845211337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:45,086]\u001b[0m Trial 71 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.14724246300136026, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.14724246300136026\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7214884529018004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7214884529018004\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5117488759727131, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5117488759727131\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.026397590845211337, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.026397590845211337\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.14724246300136026, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.14724246300136026\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7214884529018004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7214884529018004\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5117488759727131, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5117488759727131\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.026397590845211337, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.026397590845211337\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.14724246300136026, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.14724246300136026\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7214884529018004, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7214884529018004\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5117488759727131, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5117488759727131\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.026397590845211337, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.026397590845211337\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0016449010147036476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0016449010147036476\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8497894238407552, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497894238407552\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7438009299907651, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7438009299907651\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.769259183560122e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.769259183560122e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0016449010147036476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0016449010147036476\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8497894238407552, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497894238407552\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7438009299907651, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7438009299907651\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.769259183560122e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.769259183560122e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:45,263]\u001b[0m Trial 72 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0016449010147036476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0016449010147036476\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8497894238407552, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497894238407552\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7438009299907651, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7438009299907651\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.769259183560122e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.769259183560122e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0016449010147036476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0016449010147036476\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8497894238407552, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497894238407552\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7438009299907651, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7438009299907651\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.769259183560122e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.769259183560122e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0016449010147036476, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0016449010147036476\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8497894238407552, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8497894238407552\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7438009299907651, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7438009299907651\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.769259183560122e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.769259183560122e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.028723369306679115, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.028723369306679115\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9845488397244039, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9845488397244039\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.79196929863192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.79196929863192\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006302011589882346, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006302011589882346\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.028723369306679115, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.028723369306679115\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9845488397244039, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9845488397244039\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.79196929863192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.79196929863192\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006302011589882346, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006302011589882346\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.028723369306679115, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.028723369306679115\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9845488397244039, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9845488397244039\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.79196929863192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.79196929863192\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006302011589882346, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006302011589882346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:45,477]\u001b[0m Trial 73 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.028723369306679115, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.028723369306679115\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9845488397244039, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9845488397244039\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.79196929863192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.79196929863192\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006302011589882346, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006302011589882346\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.028723369306679115, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.028723369306679115\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9845488397244039, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9845488397244039\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.79196929863192, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.79196929863192\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006302011589882346, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006302011589882346\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.013706211742786e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.013706211742786e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4762157639231171, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4762157639231171\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6971206384739344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6971206384739344\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.17312471942064805, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17312471942064805\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.013706211742786e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.013706211742786e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4762157639231171, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4762157639231171\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6971206384739344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6971206384739344\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.17312471942064805, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17312471942064805\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.013706211742786e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.013706211742786e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4762157639231171, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4762157639231171\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6971206384739344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6971206384739344\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.17312471942064805, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17312471942064805\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.013706211742786e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.013706211742786e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4762157639231171, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4762157639231171\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6971206384739344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6971206384739344\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.17312471942064805, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17312471942064805\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.013706211742786e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.013706211742786e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4762157639231171, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4762157639231171\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6971206384739344, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6971206384739344\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.17312471942064805, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.17312471942064805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:45,884]\u001b[0m Trial 74 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.0250324888030002e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0250324888030002e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6632293121508386, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6632293121508386\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9062932096083042, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9062932096083042\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00012835321626584605, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012835321626584605\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0250324888030002e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0250324888030002e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6632293121508386, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6632293121508386\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9062932096083042, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9062932096083042\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00012835321626584605, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012835321626584605\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0250324888030002e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0250324888030002e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6632293121508386, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6632293121508386\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9062932096083042, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9062932096083042\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00012835321626584605, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012835321626584605\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0250324888030002e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0250324888030002e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6632293121508386, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6632293121508386\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9062932096083042, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9062932096083042\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00012835321626584605, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012835321626584605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:46,152]\u001b[0m Trial 75 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.0250324888030002e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0250324888030002e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6632293121508386, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6632293121508386\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9062932096083042, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9062932096083042\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00012835321626584605, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00012835321626584605\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00021900778027470584, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00021900778027470584\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7614632869169216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7614632869169216\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7172687709231218, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7172687709231218\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0001747544032225741, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0001747544032225741\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00021900778027470584, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00021900778027470584\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7614632869169216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7614632869169216\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7172687709231218, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7172687709231218\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0001747544032225741, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0001747544032225741\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00021900778027470584, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00021900778027470584\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7614632869169216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7614632869169216\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7172687709231218, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7172687709231218\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0001747544032225741, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0001747544032225741\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00021900778027470584, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00021900778027470584\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7614632869169216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7614632869169216\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7172687709231218, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7172687709231218\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0001747544032225741, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0001747544032225741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:46,379]\u001b[0m Trial 76 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.00021900778027470584, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00021900778027470584\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7614632869169216, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7614632869169216\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7172687709231218, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7172687709231218\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0001747544032225741, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0001747544032225741\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.578356115263919, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.578356115263919\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8601471116958836, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8601471116958836\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6323503635823273, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6323503635823273\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0013673932546227668, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0013673932546227668\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.578356115263919, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.578356115263919\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8601471116958836, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8601471116958836\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6323503635823273, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6323503635823273\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0013673932546227668, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0013673932546227668\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.578356115263919, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.578356115263919\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8601471116958836, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8601471116958836\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6323503635823273, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6323503635823273\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0013673932546227668, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0013673932546227668\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.578356115263919, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.578356115263919\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8601471116958836, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8601471116958836\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6323503635823273, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6323503635823273\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0013673932546227668, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0013673932546227668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:46,605]\u001b[0m Trial 77 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:46,770]\u001b[0m Trial 78 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=7.578356115263919, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.578356115263919\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8601471116958836, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8601471116958836\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6323503635823273, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6323503635823273\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0013673932546227668, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0013673932546227668\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4801841457005037e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4801841457005037e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4132772142110368, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4132772142110368\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7480144341572681, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7480144341572681\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.430699617817438e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.430699617817438e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4801841457005037e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4801841457005037e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4132772142110368, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4132772142110368\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7480144341572681, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7480144341572681\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.430699617817438e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.430699617817438e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4801841457005037e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4801841457005037e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4132772142110368, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4132772142110368\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7480144341572681, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7480144341572681\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.430699617817438e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.430699617817438e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4801841457005037e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4801841457005037e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4132772142110368, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4132772142110368\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7480144341572681, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7480144341572681\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.430699617817438e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.430699617817438e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.4801841457005037e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.4801841457005037e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4132772142110368, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4132772142110368\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7480144341572681, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7480144341572681\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.430699617817438e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.430699617817438e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.6922244894456685e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6922244894456685e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9739685050052227, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9739685050052227\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4921057709698654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4921057709698654\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.662135311273007, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.662135311273007\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.6922244894456685e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6922244894456685e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9739685050052227, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9739685050052227\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4921057709698654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4921057709698654\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.662135311273007, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.662135311273007\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.6922244894456685e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6922244894456685e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9739685050052227, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9739685050052227\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4921057709698654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4921057709698654\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.662135311273007, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.662135311273007\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.6922244894456685e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6922244894456685e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9739685050052227, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9739685050052227\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4921057709698654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4921057709698654\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.662135311273007, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.662135311273007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:47,074]\u001b[0m Trial 79 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.6922244894456685e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6922244894456685e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9739685050052227, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9739685050052227\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4921057709698654, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4921057709698654\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.662135311273007, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.662135311273007\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004891540471771684, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004891540471771684\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8421903253311225, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8421903253311225\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5389150710289549, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5389150710289549\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.4378054767296764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4378054767296764\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004891540471771684, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004891540471771684\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8421903253311225, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8421903253311225\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5389150710289549, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5389150710289549\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.4378054767296764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4378054767296764\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004891540471771684, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004891540471771684\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8421903253311225, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8421903253311225\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5389150710289549, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5389150710289549\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.4378054767296764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4378054767296764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:47,357]\u001b[0m Trial 80 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.004891540471771684, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004891540471771684\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8421903253311225, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8421903253311225\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5389150710289549, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5389150710289549\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.4378054767296764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4378054767296764\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.004891540471771684, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.004891540471771684\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8421903253311225, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8421903253311225\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5389150710289549, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5389150710289549\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.4378054767296764, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4378054767296764\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00018103869693427186, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00018103869693427186\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6242147351071261, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6242147351071261\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.951618020025997, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.951618020025997\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.4028314615501428e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4028314615501428e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00018103869693427186, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00018103869693427186\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6242147351071261, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6242147351071261\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.951618020025997, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.951618020025997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:47,654]\u001b[0m Trial 81 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l2 is set=2.4028314615501428e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4028314615501428e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00018103869693427186, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00018103869693427186\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6242147351071261, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6242147351071261\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.951618020025997, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.951618020025997\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.4028314615501428e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4028314615501428e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00018103869693427186, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00018103869693427186\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6242147351071261, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6242147351071261\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.951618020025997, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.951618020025997\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.4028314615501428e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4028314615501428e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00018103869693427186, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00018103869693427186\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6242147351071261, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6242147351071261\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.951618020025997, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.951618020025997\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.4028314615501428e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4028314615501428e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.002316531822058845, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.002316531822058845\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9144623323572186, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9144623323572186\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4143043929080593, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4143043929080593\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07843375910879978, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07843375910879978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:47,844]\u001b[0m Trial 82 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.002316531822058845, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.002316531822058845\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9144623323572186, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9144623323572186\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4143043929080593, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4143043929080593\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07843375910879978, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07843375910879978\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.002316531822058845, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.002316531822058845\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9144623323572186, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9144623323572186\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4143043929080593, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4143043929080593\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07843375910879978, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07843375910879978\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.002316531822058845, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.002316531822058845\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9144623323572186, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9144623323572186\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4143043929080593, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4143043929080593\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07843375910879978, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07843375910879978\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.002316531822058845, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.002316531822058845\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9144623323572186, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9144623323572186\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4143043929080593, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4143043929080593\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07843375910879978, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07843375910879978\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.1777397122129125, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.1777397122129125\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7357585721747397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7357585721747397\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5568527426261418, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5568527426261418\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.0349720621046672e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.0349720621046672e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.1777397122129125, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.1777397122129125\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7357585721747397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7357585721747397\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5568527426261418, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5568527426261418\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.0349720621046672e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.0349720621046672e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.1777397122129125, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.1777397122129125\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7357585721747397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7357585721747397\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5568527426261418, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5568527426261418\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.0349720621046672e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.0349720621046672e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:48,017]\u001b[0m Trial 83 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=5.1777397122129125, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.1777397122129125\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7357585721747397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7357585721747397\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5568527426261418, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5568527426261418\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.0349720621046672e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.0349720621046672e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.1777397122129125, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.1777397122129125\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7357585721747397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7357585721747397\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5568527426261418, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5568527426261418\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.0349720621046672e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.0349720621046672e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.284837853639649, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.284837853639649\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9165723682920693, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9165723682920693\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.49760286000482595, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.49760286000482595\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.694001885733276e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.694001885733276e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.284837853639649, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.284837853639649\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9165723682920693, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9165723682920693\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.49760286000482595, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.49760286000482595\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.694001885733276e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.694001885733276e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.284837853639649, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.284837853639649\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9165723682920693, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9165723682920693\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.49760286000482595, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.49760286000482595\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.694001885733276e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.694001885733276e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.284837853639649, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.284837853639649\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9165723682920693, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9165723682920693\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.49760286000482595, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.49760286000482595\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.694001885733276e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.694001885733276e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:48,228]\u001b[0m Trial 84 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.284837853639649, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.284837853639649\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9165723682920693, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9165723682920693\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.49760286000482595, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.49760286000482595\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.694001885733276e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.694001885733276e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.003227981579636282, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.003227981579636282\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6334677085420336, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6334677085420336\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8885971481592967, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8885971481592967\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.03181535815944061, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03181535815944061\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.003227981579636282, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.003227981579636282\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6334677085420336, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6334677085420336\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8885971481592967, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8885971481592967\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.03181535815944061, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03181535815944061\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.003227981579636282, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.003227981579636282\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6334677085420336, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6334677085420336\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8885971481592967, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8885971481592967\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.03181535815944061, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03181535815944061\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.003227981579636282, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.003227981579636282\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6334677085420336, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6334677085420336\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8885971481592967, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8885971481592967\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.03181535815944061, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03181535815944061\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.003227981579636282, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.003227981579636282\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6334677085420336, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6334677085420336\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8885971481592967, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8885971481592967\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.03181535815944061, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.03181535815944061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:48,404]\u001b[0m Trial 85 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:48,597]\u001b[0m Trial 86 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.5422595734844289, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5422595734844289\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8254543981082787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254543981082787\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5120335910920275, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5120335910920275\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.013155516280127828, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.013155516280127828\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.5422595734844289, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5422595734844289\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8254543981082787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254543981082787\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5120335910920275, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5120335910920275\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.013155516280127828, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.013155516280127828\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.5422595734844289, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5422595734844289\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8254543981082787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254543981082787\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5120335910920275, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5120335910920275\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.013155516280127828, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.013155516280127828\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.5422595734844289, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5422595734844289\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8254543981082787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254543981082787\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5120335910920275, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5120335910920275\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.013155516280127828, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.013155516280127828\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.5422595734844289, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.5422595734844289\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8254543981082787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8254543981082787\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5120335910920275, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5120335910920275\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.013155516280127828, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.013155516280127828\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00024485995415992506, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00024485995415992506\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6942465096071171, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6942465096071171\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6136433226778306, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6136433226778306\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00011906509048652146, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011906509048652146\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00024485995415992506, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00024485995415992506\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6942465096071171, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6942465096071171\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6136433226778306, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6136433226778306\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00011906509048652146, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011906509048652146\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00024485995415992506, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00024485995415992506\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6942465096071171, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6942465096071171\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6136433226778306, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6136433226778306\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00011906509048652146, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011906509048652146\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00024485995415992506, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00024485995415992506\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6942465096071171, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6942465096071171\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6136433226778306, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6136433226778306\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00011906509048652146, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011906509048652146\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00024485995415992506, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00024485995415992506\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6942465096071171, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6942465096071171\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6136433226778306, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6136433226778306\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00011906509048652146, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011906509048652146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:49,041]\u001b[0m Trial 87 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0014071088618995567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0014071088618995567\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.550575245630765, subsample=1.0 will be ignored. Current value: bagging_fraction=0.550575245630765\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8389900665664523, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8389900665664523\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.5750694844407944e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.5750694844407944e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0014071088618995567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0014071088618995567\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.550575245630765, subsample=1.0 will be ignored. Current value: bagging_fraction=0.550575245630765\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8389900665664523, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8389900665664523\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.5750694844407944e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.5750694844407944e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0014071088618995567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0014071088618995567\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.550575245630765, subsample=1.0 will be ignored. Current value: bagging_fraction=0.550575245630765\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8389900665664523, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8389900665664523\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.5750694844407944e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.5750694844407944e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0014071088618995567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0014071088618995567\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.550575245630765, subsample=1.0 will be ignored. Current value: bagging_fraction=0.550575245630765\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8389900665664523, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8389900665664523\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.5750694844407944e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.5750694844407944e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:49,300]\u001b[0m Trial 88 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0014071088618995567, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0014071088618995567\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.550575245630765, subsample=1.0 will be ignored. Current value: bagging_fraction=0.550575245630765\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8389900665664523, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8389900665664523\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.5750694844407944e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.5750694844407944e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.1681431346590351, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1681431346590351\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6906643787495146, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6906643787495146\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.45976298216358963, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45976298216358963\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.69771991130485e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.69771991130485e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.1681431346590351, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1681431346590351\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6906643787495146, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6906643787495146\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.45976298216358963, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45976298216358963\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.69771991130485e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.69771991130485e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.1681431346590351, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1681431346590351\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6906643787495146, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6906643787495146\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.45976298216358963, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45976298216358963\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.69771991130485e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.69771991130485e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:49,555]\u001b[0m Trial 89 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.1681431346590351, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1681431346590351\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6906643787495146, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6906643787495146\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.45976298216358963, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45976298216358963\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.69771991130485e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.69771991130485e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.1681431346590351, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1681431346590351\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6906643787495146, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6906643787495146\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.45976298216358963, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45976298216358963\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.69771991130485e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.69771991130485e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.515925002041667, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.515925002041667\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.43465685634926327, subsample=1.0 will be ignored. Current value: bagging_fraction=0.43465685634926327\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4403294315596689, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4403294315596689\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.010817161659383383, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.010817161659383383\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.515925002041667, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.515925002041667\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.43465685634926327, subsample=1.0 will be ignored. Current value: bagging_fraction=0.43465685634926327\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4403294315596689, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4403294315596689\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.010817161659383383, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.010817161659383383\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.515925002041667, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.515925002041667\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.43465685634926327, subsample=1.0 will be ignored. Current value: bagging_fraction=0.43465685634926327\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4403294315596689, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4403294315596689\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.010817161659383383, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.010817161659383383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:49,764]\u001b[0m Trial 90 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=5.515925002041667, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.515925002041667\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.43465685634926327, subsample=1.0 will be ignored. Current value: bagging_fraction=0.43465685634926327\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4403294315596689, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4403294315596689\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.010817161659383383, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.010817161659383383\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.515925002041667, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.515925002041667\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.43465685634926327, subsample=1.0 will be ignored. Current value: bagging_fraction=0.43465685634926327\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4403294315596689, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4403294315596689\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.010817161659383383, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.010817161659383383\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.972857150992297e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.972857150992297e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8384450135328982, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8384450135328982\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6201413367384823, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6201413367384823\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0004216249290579318, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004216249290579318\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.972857150992297e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.972857150992297e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8384450135328982, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8384450135328982\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6201413367384823, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6201413367384823\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0004216249290579318, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004216249290579318\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.972857150992297e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.972857150992297e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8384450135328982, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8384450135328982\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6201413367384823, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6201413367384823\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0004216249290579318, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004216249290579318\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.972857150992297e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.972857150992297e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8384450135328982, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8384450135328982\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6201413367384823, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6201413367384823\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0004216249290579318, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004216249290579318\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.972857150992297e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.972857150992297e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8384450135328982, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8384450135328982\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6201413367384823, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6201413367384823\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0004216249290579318, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0004216249290579318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:50,151]\u001b[0m Trial 91 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:50,313]\u001b[0m Trial 92 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.5861936267859265, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5861936267859265\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7549761253502341, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7549761253502341\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4043502075336086, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4043502075336086\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0032089300899836087, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0032089300899836087\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.5861936267859265, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5861936267859265\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7549761253502341, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7549761253502341\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4043502075336086, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4043502075336086\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0032089300899836087, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0032089300899836087\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.5861936267859265, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5861936267859265\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7549761253502341, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7549761253502341\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4043502075336086, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4043502075336086\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0032089300899836087, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0032089300899836087\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.5861936267859265, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5861936267859265\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7549761253502341, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7549761253502341\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4043502075336086, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4043502075336086\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0032089300899836087, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0032089300899836087\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.5861936267859265, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5861936267859265\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7549761253502341, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7549761253502341\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4043502075336086, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4043502075336086\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0032089300899836087, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0032089300899836087\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7065439151841236e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7065439151841236e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7201459736585927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7201459736585927\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4480203469065587, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4480203469065587\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.06575103313005981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06575103313005981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:50,500]\u001b[0m Trial 93 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.7065439151841236e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7065439151841236e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7201459736585927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7201459736585927\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4480203469065587, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4480203469065587\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.06575103313005981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06575103313005981\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7065439151841236e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7065439151841236e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7201459736585927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7201459736585927\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4480203469065587, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4480203469065587\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.06575103313005981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06575103313005981\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7065439151841236e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7065439151841236e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7201459736585927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7201459736585927\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4480203469065587, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4480203469065587\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.06575103313005981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06575103313005981\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.7065439151841236e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.7065439151841236e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7201459736585927, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7201459736585927\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4480203469065587, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4480203469065587\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.06575103313005981, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.06575103313005981\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.164274275228499e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.164274275228499e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8487335096062323, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8487335096062323\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8635911250689194, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8635911250689194\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.005678452849966785, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005678452849966785\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.164274275228499e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.164274275228499e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8487335096062323, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8487335096062323\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8635911250689194, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8635911250689194\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.005678452849966785, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005678452849966785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:50,695]\u001b[0m Trial 94 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.164274275228499e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.164274275228499e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8487335096062323, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8487335096062323\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8635911250689194, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8635911250689194\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.005678452849966785, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005678452849966785\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.164274275228499e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.164274275228499e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8487335096062323, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8487335096062323\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8635911250689194, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8635911250689194\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.005678452849966785, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005678452849966785\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.164274275228499e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.164274275228499e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8487335096062323, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8487335096062323\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8635911250689194, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8635911250689194\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.005678452849966785, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.005678452849966785\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.027638209146313, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.027638209146313\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5543292425616424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5543292425616424\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8597042016350044, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8597042016350044\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.595510384653271, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.595510384653271\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.027638209146313, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.027638209146313\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5543292425616424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5543292425616424\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8597042016350044, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8597042016350044\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.595510384653271, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.595510384653271\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.027638209146313, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.027638209146313\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5543292425616424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5543292425616424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:50,891]\u001b[0m Trial 95 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8597042016350044, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8597042016350044\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.595510384653271, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.595510384653271\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.027638209146313, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.027638209146313\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5543292425616424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5543292425616424\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8597042016350044, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8597042016350044\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.595510384653271, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.595510384653271\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.027638209146313, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.027638209146313\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5543292425616424, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5543292425616424\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8597042016350044, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8597042016350044\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.595510384653271, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.595510384653271\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.2568507876729615, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.2568507876729615\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9114634687645671, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9114634687645671\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5512912249680817, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5512912249680817\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3374301236376492e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3374301236376492e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.2568507876729615, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.2568507876729615\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9114634687645671, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9114634687645671\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5512912249680817, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5512912249680817\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3374301236376492e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3374301236376492e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.2568507876729615, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.2568507876729615\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9114634687645671, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9114634687645671\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5512912249680817, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5512912249680817\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3374301236376492e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3374301236376492e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:51,103]\u001b[0m Trial 96 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=7.2568507876729615, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.2568507876729615\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9114634687645671, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9114634687645671\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5512912249680817, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5512912249680817\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3374301236376492e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3374301236376492e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.2568507876729615, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.2568507876729615\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9114634687645671, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9114634687645671\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5512912249680817, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5512912249680817\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.3374301236376492e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3374301236376492e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0004296493072180262, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004296493072180262\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8670474935159594, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8670474935159594\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7012776425151648, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7012776425151648\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.970251356212208e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.970251356212208e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0004296493072180262, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004296493072180262\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8670474935159594, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8670474935159594\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7012776425151648, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7012776425151648\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.970251356212208e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.970251356212208e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0004296493072180262, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004296493072180262\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8670474935159594, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8670474935159594\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7012776425151648, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7012776425151648\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.970251356212208e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.970251356212208e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0004296493072180262, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004296493072180262\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8670474935159594, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8670474935159594\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7012776425151648, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7012776425151648\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.970251356212208e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.970251356212208e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:51,724]\u001b[0m Trial 97 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0004296493072180262, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0004296493072180262\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8670474935159594, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8670474935159594\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7012776425151648, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7012776425151648\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.970251356212208e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.970251356212208e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.32242256689526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.32242256689526\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5801095465785108, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5801095465785108\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6653809387895178, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6653809387895178\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.658349526478865e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.658349526478865e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.32242256689526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.32242256689526\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5801095465785108, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5801095465785108\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6653809387895178, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6653809387895178\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.658349526478865e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.658349526478865e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.32242256689526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.32242256689526\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5801095465785108, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5801095465785108\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6653809387895178, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6653809387895178\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.658349526478865e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.658349526478865e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:51,893]\u001b[0m Trial 98 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.32242256689526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.32242256689526\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5801095465785108, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5801095465785108\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6653809387895178, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6653809387895178\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.658349526478865e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.658349526478865e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.32242256689526, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.32242256689526\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5801095465785108, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5801095465785108\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6653809387895178, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6653809387895178\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.658349526478865e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.658349526478865e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.065430393718525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.065430393718525\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8926300009523438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8926300009523438\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5077911306853097, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5077911306853097\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.367077693596213e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.367077693596213e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.065430393718525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.065430393718525\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8926300009523438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8926300009523438\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5077911306853097, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5077911306853097\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.367077693596213e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.367077693596213e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.065430393718525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.065430393718525\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8926300009523438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8926300009523438\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5077911306853097, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5077911306853097\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.367077693596213e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.367077693596213e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.065430393718525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.065430393718525\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8926300009523438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8926300009523438\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5077911306853097, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5077911306853097\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.367077693596213e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.367077693596213e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.065430393718525, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.065430393718525\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8926300009523438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8926300009523438\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5077911306853097, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5077911306853097\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.367077693596213e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.367077693596213e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:52,060]\u001b[0m Trial 99 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.9147546058998367, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9147546058998367\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6648353602477364, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6648353602477364\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.853390575603389, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.853390575603389\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.09031945033789629, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.09031945033789629\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.9147546058998367, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9147546058998367\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6648353602477364, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6648353602477364\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.853390575603389, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.853390575603389\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.09031945033789629, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.09031945033789629\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.9147546058998367, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9147546058998367\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6648353602477364, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6648353602477364\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.853390575603389, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.853390575603389\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.09031945033789629, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.09031945033789629\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.9147546058998367, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9147546058998367\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6648353602477364, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6648353602477364\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.853390575603389, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.853390575603389\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.09031945033789629, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.09031945033789629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:52,343]\u001b[0m Trial 100 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.9147546058998367, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.9147546058998367\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6648353602477364, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6648353602477364\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.853390575603389, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.853390575603389\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.09031945033789629, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.09031945033789629\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00010428189616116047, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00010428189616116047\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8942903969887732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8942903969887732\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5135714290288315, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135714290288315\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.861442912453146, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.861442912453146\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00010428189616116047, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00010428189616116047\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8942903969887732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8942903969887732\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5135714290288315, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135714290288315\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.861442912453146, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.861442912453146\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00010428189616116047, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00010428189616116047\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8942903969887732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8942903969887732\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5135714290288315, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135714290288315\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.861442912453146, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.861442912453146\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00010428189616116047, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00010428189616116047\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8942903969887732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8942903969887732\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5135714290288315, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135714290288315\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.861442912453146, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.861442912453146\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00010428189616116047, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00010428189616116047\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8942903969887732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8942903969887732\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5135714290288315, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5135714290288315\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.861442912453146, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.861442912453146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:52,528]\u001b[0m Trial 101 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0033306393973443105, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0033306393973443105\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9688956177830859, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9688956177830859\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7421396923829151, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7421396923829151\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.371141356579069e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.371141356579069e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0033306393973443105, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0033306393973443105\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9688956177830859, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9688956177830859\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7421396923829151, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7421396923829151\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.371141356579069e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.371141356579069e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0033306393973443105, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0033306393973443105\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9688956177830859, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9688956177830859\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7421396923829151, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7421396923829151\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.371141356579069e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.371141356579069e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0033306393973443105, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0033306393973443105\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9688956177830859, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9688956177830859\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7421396923829151, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7421396923829151\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.371141356579069e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.371141356579069e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0033306393973443105, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0033306393973443105\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9688956177830859, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9688956177830859\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7421396923829151, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7421396923829151\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.371141356579069e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.371141356579069e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:52,986]\u001b[0m Trial 102 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.15493696910942337, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15493696910942337\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6733924541379741, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6733924541379741\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.745656762105718, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.745656762105718\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0006597213733608887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006597213733608887\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15493696910942337, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15493696910942337\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6733924541379741, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6733924541379741\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.745656762105718, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.745656762105718\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0006597213733608887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006597213733608887\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15493696910942337, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15493696910942337\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6733924541379741, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6733924541379741\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.745656762105718, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.745656762105718\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0006597213733608887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006597213733608887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:53,348]\u001b[0m Trial 103 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.15493696910942337, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15493696910942337\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6733924541379741, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6733924541379741\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.745656762105718, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.745656762105718\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0006597213733608887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006597213733608887\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.15493696910942337, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.15493696910942337\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6733924541379741, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6733924541379741\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.745656762105718, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.745656762105718\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0006597213733608887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0006597213733608887\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.006675931024852272, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.006675931024852272\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8858855382908916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8858855382908916\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9668512964341858, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9668512964341858\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.7809889109173659e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7809889109173659e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:53,689]\u001b[0m Trial 104 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.006675931024852272, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.006675931024852272\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8858855382908916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8858855382908916\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9668512964341858, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9668512964341858\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.7809889109173659e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7809889109173659e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.006675931024852272, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.006675931024852272\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8858855382908916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8858855382908916\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9668512964341858, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9668512964341858\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.7809889109173659e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7809889109173659e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.006675931024852272, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.006675931024852272\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8858855382908916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8858855382908916\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9668512964341858, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9668512964341858\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.7809889109173659e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7809889109173659e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.006675931024852272, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.006675931024852272\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8858855382908916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8858855382908916\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9668512964341858, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9668512964341858\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.7809889109173659e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.7809889109173659e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.138892634022461, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.138892634022461\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9914820831981058, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9914820831981058\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8177627447185406, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8177627447185406\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0562867261895028, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0562867261895028\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.138892634022461, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.138892634022461\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9914820831981058, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9914820831981058\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8177627447185406, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8177627447185406\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0562867261895028, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0562867261895028\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.138892634022461, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.138892634022461\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9914820831981058, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9914820831981058\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8177627447185406, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8177627447185406\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0562867261895028, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0562867261895028\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.138892634022461, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.138892634022461\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9914820831981058, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9914820831981058\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8177627447185406, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8177627447185406\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0562867261895028, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0562867261895028\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.138892634022461, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.138892634022461\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9914820831981058, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9914820831981058\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8177627447185406, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8177627447185406\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.0562867261895028, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0562867261895028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:53,912]\u001b[0m Trial 105 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:54,086]\u001b[0m Trial 106 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=5.404822208470038e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.404822208470038e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7848926729124162, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7848926729124162\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5822672487610983, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5822672487610983\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00011403164458929739, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011403164458929739\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.404822208470038e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.404822208470038e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7848926729124162, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7848926729124162\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5822672487610983, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5822672487610983\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00011403164458929739, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011403164458929739\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.404822208470038e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.404822208470038e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7848926729124162, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7848926729124162\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5822672487610983, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5822672487610983\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00011403164458929739, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011403164458929739\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.404822208470038e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.404822208470038e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7848926729124162, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7848926729124162\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5822672487610983, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5822672487610983\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00011403164458929739, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011403164458929739\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.404822208470038e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.404822208470038e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7848926729124162, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7848926729124162\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5822672487610983, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5822672487610983\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00011403164458929739, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00011403164458929739\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.04635291840641358, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04635291840641358\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5506249321803713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5506249321803713\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4337973945074202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4337973945074202\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0031484321314997404, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0031484321314997404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:54,248]\u001b[0m Trial 107 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.04635291840641358, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04635291840641358\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5506249321803713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5506249321803713\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4337973945074202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4337973945074202\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0031484321314997404, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0031484321314997404\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.04635291840641358, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04635291840641358\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5506249321803713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5506249321803713\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4337973945074202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4337973945074202\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0031484321314997404, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0031484321314997404\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.04635291840641358, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04635291840641358\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5506249321803713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5506249321803713\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4337973945074202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4337973945074202\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0031484321314997404, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0031484321314997404\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.04635291840641358, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04635291840641358\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5506249321803713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5506249321803713\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4337973945074202, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4337973945074202\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0031484321314997404, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0031484321314997404\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.029419712884483112, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.029419712884483112\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.985591051830605, subsample=1.0 will be ignored. Current value: bagging_fraction=0.985591051830605\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7902161189194067, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902161189194067\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.562469498172686e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.562469498172686e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.029419712884483112, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.029419712884483112\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.985591051830605, subsample=1.0 will be ignored. Current value: bagging_fraction=0.985591051830605\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7902161189194067, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902161189194067\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.562469498172686e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.562469498172686e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.029419712884483112, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.029419712884483112\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.985591051830605, subsample=1.0 will be ignored. Current value: bagging_fraction=0.985591051830605\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7902161189194067, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902161189194067\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.562469498172686e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.562469498172686e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.029419712884483112, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.029419712884483112\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.985591051830605, subsample=1.0 will be ignored. Current value: bagging_fraction=0.985591051830605\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7902161189194067, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902161189194067\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.562469498172686e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.562469498172686e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:54,719]\u001b[0m Trial 108 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.029419712884483112, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.029419712884483112\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.985591051830605, subsample=1.0 will be ignored. Current value: bagging_fraction=0.985591051830605\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7902161189194067, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7902161189194067\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.562469498172686e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.562469498172686e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.964376825547721e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.964376825547721e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7524249762143803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7524249762143803\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6856517959066297, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6856517959066297\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.16506320207358288, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16506320207358288\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.964376825547721e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.964376825547721e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7524249762143803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7524249762143803\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6856517959066297, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6856517959066297\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.16506320207358288, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16506320207358288\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.964376825547721e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.964376825547721e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7524249762143803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7524249762143803\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6856517959066297, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6856517959066297\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.16506320207358288, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16506320207358288\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.964376825547721e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.964376825547721e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7524249762143803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7524249762143803\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6856517959066297, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6856517959066297\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.16506320207358288, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16506320207358288\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.964376825547721e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.964376825547721e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7524249762143803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7524249762143803\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6856517959066297, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6856517959066297\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.16506320207358288, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.16506320207358288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:55,059]\u001b[0m Trial 109 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:55,204]\u001b[0m Trial 110 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.02628495102673974, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.02628495102673974\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4090799860934169, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4090799860934169\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7518798693756463, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7518798693756463\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.527312976756112e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.527312976756112e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.02628495102673974, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.02628495102673974\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4090799860934169, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4090799860934169\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7518798693756463, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7518798693756463\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.527312976756112e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.527312976756112e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.02628495102673974, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.02628495102673974\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4090799860934169, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4090799860934169\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7518798693756463, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7518798693756463\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.527312976756112e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.527312976756112e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.02628495102673974, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.02628495102673974\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4090799860934169, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4090799860934169\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7518798693756463, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7518798693756463\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.527312976756112e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.527312976756112e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.02628495102673974, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.02628495102673974\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4090799860934169, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4090799860934169\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7518798693756463, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7518798693756463\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.527312976756112e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.527312976756112e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.77632548974306e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.77632548974306e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4702246792726996, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4702246792726996\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5168962449377887, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5168962449377887\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.341546175244675e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.341546175244675e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.77632548974306e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.77632548974306e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4702246792726996, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4702246792726996\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5168962449377887, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5168962449377887\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.341546175244675e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.341546175244675e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:55,331]\u001b[0m Trial 111 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=5.77632548974306e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.77632548974306e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4702246792726996, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4702246792726996\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5168962449377887, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5168962449377887\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.341546175244675e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.341546175244675e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.77632548974306e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.77632548974306e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4702246792726996, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4702246792726996\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5168962449377887, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5168962449377887\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.341546175244675e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.341546175244675e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.77632548974306e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.77632548974306e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4702246792726996, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4702246792726996\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5168962449377887, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5168962449377887\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.341546175244675e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.341546175244675e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.4204695203772387e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.4204695203772387e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8301097795358121, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8301097795358121\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9215364116381104, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9215364116381104\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5062172863182516e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5062172863182516e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.4204695203772387e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.4204695203772387e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8301097795358121, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8301097795358121\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9215364116381104, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9215364116381104\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5062172863182516e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5062172863182516e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.4204695203772387e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.4204695203772387e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8301097795358121, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8301097795358121\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9215364116381104, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9215364116381104\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5062172863182516e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5062172863182516e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.4204695203772387e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.4204695203772387e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8301097795358121, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8301097795358121\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9215364116381104, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9215364116381104\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5062172863182516e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5062172863182516e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:55,552]\u001b[0m Trial 112 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.4204695203772387e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.4204695203772387e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8301097795358121, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8301097795358121\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9215364116381104, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9215364116381104\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.5062172863182516e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.5062172863182516e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.163968648841865e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.163968648841865e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6045720105729517, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6045720105729517\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7244568400888767, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7244568400888767\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.8892927611798904e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.8892927611798904e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.163968648841865e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.163968648841865e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6045720105729517, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6045720105729517\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7244568400888767, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7244568400888767\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.8892927611798904e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.8892927611798904e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.163968648841865e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.163968648841865e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6045720105729517, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6045720105729517\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7244568400888767, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7244568400888767\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.8892927611798904e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.8892927611798904e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.163968648841865e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.163968648841865e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6045720105729517, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6045720105729517\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7244568400888767, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7244568400888767\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.8892927611798904e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.8892927611798904e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:55,774]\u001b[0m Trial 113 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=4.163968648841865e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.163968648841865e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6045720105729517, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6045720105729517\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7244568400888767, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7244568400888767\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.8892927611798904e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.8892927611798904e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.822634746587423e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.822634746587423e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9440171786778676, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9440171786778676\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5819805890575398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5819805890575398\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006624516212956048, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006624516212956048\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.822634746587423e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.822634746587423e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9440171786778676, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9440171786778676\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5819805890575398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5819805890575398\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006624516212956048, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006624516212956048\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.822634746587423e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.822634746587423e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9440171786778676, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9440171786778676\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5819805890575398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5819805890575398\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006624516212956048, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006624516212956048\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.822634746587423e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.822634746587423e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9440171786778676, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9440171786778676\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5819805890575398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5819805890575398\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006624516212956048, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006624516212956048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:56,358]\u001b[0m Trial 114 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.822634746587423e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.822634746587423e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9440171786778676, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9440171786778676\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5819805890575398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5819805890575398\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006624516212956048, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006624516212956048\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.307884045239744e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.307884045239744e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7112048620956603, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112048620956603\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4416645841765985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4416645841765985\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.371696390536379e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.371696390536379e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.307884045239744e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.307884045239744e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7112048620956603, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112048620956603\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4416645841765985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4416645841765985\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.371696390536379e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.371696390536379e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:56,645]\u001b[0m Trial 115 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=3.307884045239744e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.307884045239744e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7112048620956603, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112048620956603\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4416645841765985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4416645841765985\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.371696390536379e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.371696390536379e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.307884045239744e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.307884045239744e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7112048620956603, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112048620956603\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4416645841765985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4416645841765985\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.371696390536379e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.371696390536379e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.307884045239744e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.307884045239744e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7112048620956603, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7112048620956603\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4416645841765985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4416645841765985\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.371696390536379e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.371696390536379e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.05689088093487245, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.05689088093487245\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8161047511086215, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8161047511086215\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8405650563495621, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8405650563495621\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.4849289830058743e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.4849289830058743e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:56,862]\u001b[0m Trial 116 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.05689088093487245, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.05689088093487245\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8161047511086215, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8161047511086215\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8405650563495621, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8405650563495621\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.4849289830058743e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.4849289830058743e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.05689088093487245, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.05689088093487245\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8161047511086215, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8161047511086215\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8405650563495621, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8405650563495621\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.4849289830058743e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.4849289830058743e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.05689088093487245, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.05689088093487245\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8161047511086215, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8161047511086215\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8405650563495621, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8405650563495621\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.4849289830058743e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.4849289830058743e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.05689088093487245, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.05689088093487245\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8161047511086215, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8161047511086215\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8405650563495621, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8405650563495621\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.4849289830058743e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.4849289830058743e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.822677606997023e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.822677606997023e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.840759069178958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.840759069178958\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6869128258335734, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6869128258335734\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0020015347923342777, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020015347923342777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:57,079]\u001b[0m Trial 117 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=5.822677606997023e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.822677606997023e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.840759069178958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.840759069178958\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6869128258335734, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6869128258335734\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0020015347923342777, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020015347923342777\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.822677606997023e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.822677606997023e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.840759069178958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.840759069178958\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6869128258335734, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6869128258335734\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0020015347923342777, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020015347923342777\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.822677606997023e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.822677606997023e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.840759069178958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.840759069178958\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6869128258335734, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6869128258335734\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0020015347923342777, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020015347923342777\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.822677606997023e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.822677606997023e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.840759069178958, subsample=1.0 will be ignored. Current value: bagging_fraction=0.840759069178958\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6869128258335734, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6869128258335734\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0020015347923342777, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0020015347923342777\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00014080284736237134, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00014080284736237134\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46015710606160787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46015710606160787\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.51209965894894, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.51209965894894\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.2678599899958674e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.2678599899958674e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:57,309]\u001b[0m Trial 118 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.00014080284736237134, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00014080284736237134\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46015710606160787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46015710606160787\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.51209965894894, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.51209965894894\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.2678599899958674e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.2678599899958674e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00014080284736237134, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00014080284736237134\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46015710606160787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46015710606160787\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.51209965894894, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.51209965894894\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.2678599899958674e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.2678599899958674e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00014080284736237134, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00014080284736237134\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46015710606160787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46015710606160787\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.51209965894894, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.51209965894894\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.2678599899958674e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.2678599899958674e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00014080284736237134, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00014080284736237134\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46015710606160787, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46015710606160787\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.51209965894894, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.51209965894894\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.2678599899958674e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.2678599899958674e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.2835365920389856, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2835365920389856\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6714961817187046, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6714961817187046\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8176109219188351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8176109219188351\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.4765962339319028e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4765962339319028e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.2835365920389856, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2835365920389856\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6714961817187046, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6714961817187046\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8176109219188351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8176109219188351\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.4765962339319028e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4765962339319028e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.2835365920389856, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2835365920389856\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6714961817187046, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6714961817187046\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8176109219188351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8176109219188351\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.4765962339319028e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4765962339319028e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.2835365920389856, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2835365920389856\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6714961817187046, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6714961817187046\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8176109219188351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8176109219188351\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.4765962339319028e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4765962339319028e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.2835365920389856, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.2835365920389856\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6714961817187046, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6714961817187046\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8176109219188351, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8176109219188351\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.4765962339319028e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.4765962339319028e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:57,663]\u001b[0m Trial 119 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:57,862]\u001b[0m Trial 120 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.009308723535089266, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009308723535089266\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9814005968442122, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9814005968442122\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5024807304792012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5024807304792012\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.8217360970165335, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.8217360970165335\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009308723535089266, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009308723535089266\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9814005968442122, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9814005968442122\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5024807304792012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5024807304792012\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.8217360970165335, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.8217360970165335\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009308723535089266, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009308723535089266\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9814005968442122, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9814005968442122\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5024807304792012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5024807304792012\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.8217360970165335, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.8217360970165335\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009308723535089266, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009308723535089266\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9814005968442122, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9814005968442122\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5024807304792012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5024807304792012\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.8217360970165335, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.8217360970165335\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009308723535089266, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009308723535089266\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9814005968442122, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9814005968442122\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5024807304792012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5024807304792012\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.8217360970165335, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.8217360970165335\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.048665651427641426, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.048665651427641426\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5185634025255469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5185634025255469\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5598282260044072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5598282260044072\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.018952012758748433, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018952012758748433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:58,153]\u001b[0m Trial 121 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.048665651427641426, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.048665651427641426\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5185634025255469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5185634025255469\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5598282260044072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5598282260044072\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.018952012758748433, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018952012758748433\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.048665651427641426, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.048665651427641426\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5185634025255469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5185634025255469\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5598282260044072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5598282260044072\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.018952012758748433, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018952012758748433\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.048665651427641426, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.048665651427641426\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5185634025255469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5185634025255469\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5598282260044072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5598282260044072\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.018952012758748433, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018952012758748433\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.048665651427641426, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.048665651427641426\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5185634025255469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5185634025255469\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5598282260044072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5598282260044072\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.018952012758748433, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018952012758748433\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.805573422658158e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.805573422658158e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9390977292997008, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9390977292997008\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8394638061098829, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8394638061098829\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.027346592784503958, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.027346592784503958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:58,422]\u001b[0m Trial 122 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=4.805573422658158e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.805573422658158e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9390977292997008, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9390977292997008\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8394638061098829, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8394638061098829\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.027346592784503958, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.027346592784503958\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.805573422658158e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.805573422658158e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9390977292997008, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9390977292997008\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8394638061098829, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8394638061098829\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.027346592784503958, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.027346592784503958\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.805573422658158e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.805573422658158e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9390977292997008, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9390977292997008\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8394638061098829, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8394638061098829\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.027346592784503958, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.027346592784503958\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.805573422658158e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.805573422658158e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9390977292997008, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9390977292997008\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8394638061098829, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8394638061098829\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.027346592784503958, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.027346592784503958\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.483852720524246e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.483852720524246e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7420241111803663, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7420241111803663\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5545362727245554, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5545362727245554\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.9240040097432626e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9240040097432626e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:58,710]\u001b[0m Trial 123 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.483852720524246e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.483852720524246e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7420241111803663, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7420241111803663\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5545362727245554, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5545362727245554\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.9240040097432626e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9240040097432626e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.483852720524246e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.483852720524246e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7420241111803663, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7420241111803663\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5545362727245554, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5545362727245554\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.9240040097432626e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9240040097432626e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.483852720524246e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.483852720524246e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7420241111803663, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7420241111803663\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5545362727245554, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5545362727245554\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.9240040097432626e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9240040097432626e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.483852720524246e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.483852720524246e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7420241111803663, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7420241111803663\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5545362727245554, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5545362727245554\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.9240040097432626e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9240040097432626e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.04216038694420011, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04216038694420011\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6365907003131257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6365907003131257\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5281970808932961, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5281970808932961\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.863720468494119e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.863720468494119e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.04216038694420011, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04216038694420011\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6365907003131257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6365907003131257\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5281970808932961, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5281970808932961\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.863720468494119e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.863720468494119e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.04216038694420011, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04216038694420011\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6365907003131257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6365907003131257\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5281970808932961, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5281970808932961\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.863720468494119e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.863720468494119e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.04216038694420011, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04216038694420011\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6365907003131257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6365907003131257\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5281970808932961, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5281970808932961\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.863720468494119e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.863720468494119e-07\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.04216038694420011, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.04216038694420011\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6365907003131257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6365907003131257\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5281970808932961, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5281970808932961\n",
            "[LightGBM] [Warning] lambda_l2 is set=4.863720468494119e-07, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.863720468494119e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:59,062]\u001b[0m Trial 124 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:59,261]\u001b[0m Trial 125 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0006743428410744672, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0006743428410744672\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9747148507918528, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9747148507918528\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5969969369656497, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5969969369656497\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.635446758297593e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.635446758297593e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0006743428410744672, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0006743428410744672\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9747148507918528, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9747148507918528\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5969969369656497, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5969969369656497\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.635446758297593e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.635446758297593e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0006743428410744672, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0006743428410744672\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9747148507918528, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9747148507918528\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5969969369656497, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5969969369656497\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.635446758297593e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.635446758297593e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0006743428410744672, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0006743428410744672\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9747148507918528, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9747148507918528\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5969969369656497, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5969969369656497\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.635446758297593e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.635446758297593e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0006743428410744672, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0006743428410744672\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9747148507918528, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9747148507918528\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5969969369656497, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5969969369656497\n",
            "[LightGBM] [Warning] lambda_l2 is set=7.635446758297593e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.635446758297593e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.33546698952648635, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.33546698952648635\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6411381667659088, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6411381667659088\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9333002766695928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9333002766695928\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00042905840869323503, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00042905840869323503\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.33546698952648635, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.33546698952648635\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6411381667659088, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6411381667659088\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9333002766695928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9333002766695928\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00042905840869323503, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00042905840869323503\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.33546698952648635, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.33546698952648635\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6411381667659088, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6411381667659088\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9333002766695928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9333002766695928\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00042905840869323503, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00042905840869323503\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.33546698952648635, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.33546698952648635\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6411381667659088, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6411381667659088\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9333002766695928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9333002766695928\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00042905840869323503, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00042905840869323503\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.33546698952648635, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.33546698952648635\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6411381667659088, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6411381667659088\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9333002766695928, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9333002766695928\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00042905840869323503, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00042905840869323503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:34:59,639]\u001b[0m Trial 126 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:34:59,831]\u001b[0m Trial 127 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.4340100545370421, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4340100545370421\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5822698604904051, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5822698604904051\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6564139210681499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6564139210681499\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.712871485905894e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.712871485905894e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.4340100545370421, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4340100545370421\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5822698604904051, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5822698604904051\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6564139210681499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6564139210681499\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.712871485905894e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.712871485905894e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.4340100545370421, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4340100545370421\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5822698604904051, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5822698604904051\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6564139210681499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6564139210681499\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.712871485905894e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.712871485905894e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.4340100545370421, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4340100545370421\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5822698604904051, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5822698604904051\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6564139210681499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6564139210681499\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.712871485905894e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.712871485905894e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.4340100545370421, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4340100545370421\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5822698604904051, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5822698604904051\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6564139210681499, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6564139210681499\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.712871485905894e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.712871485905894e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.112789131582302e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.112789131582302e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5241571021145732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5241571021145732\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.483815728909125, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.483815728909125\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.018222236186381385, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018222236186381385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:00,022]\u001b[0m Trial 128 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=6.112789131582302e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.112789131582302e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5241571021145732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5241571021145732\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.483815728909125, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.483815728909125\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.018222236186381385, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018222236186381385\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.112789131582302e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.112789131582302e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5241571021145732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5241571021145732\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.483815728909125, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.483815728909125\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.018222236186381385, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018222236186381385\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.112789131582302e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.112789131582302e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5241571021145732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5241571021145732\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.483815728909125, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.483815728909125\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.018222236186381385, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018222236186381385\n",
            "[LightGBM] [Warning] lambda_l1 is set=6.112789131582302e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=6.112789131582302e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5241571021145732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5241571021145732\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.483815728909125, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.483815728909125\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.018222236186381385, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.018222236186381385\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3035871201212165e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3035871201212165e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.547819083181862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.547819083181862\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5274039072082426, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5274039072082426\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.7609832937368244e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.7609832937368244e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:00,289]\u001b[0m Trial 129 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.3035871201212165e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3035871201212165e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.547819083181862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.547819083181862\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5274039072082426, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5274039072082426\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.7609832937368244e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.7609832937368244e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3035871201212165e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3035871201212165e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.547819083181862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.547819083181862\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5274039072082426, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5274039072082426\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.7609832937368244e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.7609832937368244e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3035871201212165e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3035871201212165e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.547819083181862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.547819083181862\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5274039072082426, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5274039072082426\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.7609832937368244e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.7609832937368244e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.3035871201212165e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.3035871201212165e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.547819083181862, subsample=1.0 will be ignored. Current value: bagging_fraction=0.547819083181862\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5274039072082426, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5274039072082426\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.7609832937368244e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.7609832937368244e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00167847907392581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00167847907392581\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9767167269502163, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9767167269502163\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9230141980926356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9230141980926356\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0017790387432878691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0017790387432878691\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00167847907392581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00167847907392581\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9767167269502163, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9767167269502163\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9230141980926356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9230141980926356\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0017790387432878691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0017790387432878691\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00167847907392581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00167847907392581\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9767167269502163, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9767167269502163\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9230141980926356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9230141980926356\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0017790387432878691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0017790387432878691\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00167847907392581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00167847907392581\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9767167269502163, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9767167269502163\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9230141980926356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9230141980926356\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0017790387432878691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0017790387432878691\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00167847907392581, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00167847907392581\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9767167269502163, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9767167269502163\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9230141980926356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9230141980926356\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0017790387432878691, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0017790387432878691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:01,011]\u001b[0m Trial 130 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.037663322748630074, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.037663322748630074\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8848901754076949, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8848901754076949\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6150808455231033, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6150808455231033\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.1373393221263903, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.1373393221263903\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.037663322748630074, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.037663322748630074\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8848901754076949, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8848901754076949\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6150808455231033, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6150808455231033\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.1373393221263903, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.1373393221263903\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.037663322748630074, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.037663322748630074\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8848901754076949, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8848901754076949\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6150808455231033, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6150808455231033\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.1373393221263903, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.1373393221263903\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.037663322748630074, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.037663322748630074\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8848901754076949, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8848901754076949\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6150808455231033, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6150808455231033\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.1373393221263903, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.1373393221263903\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.037663322748630074, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.037663322748630074\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8848901754076949, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8848901754076949\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6150808455231033, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6150808455231033\n",
            "[LightGBM] [Warning] lambda_l2 is set=3.1373393221263903, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.1373393221263903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:01,261]\u001b[0m Trial 131 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0001902634873921605, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001902634873921605\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9955736274586491, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9955736274586491\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5130725326374553, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5130725326374553\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00019608557028955872, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00019608557028955872\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0001902634873921605, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001902634873921605\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9955736274586491, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9955736274586491\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5130725326374553, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5130725326374553\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00019608557028955872, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00019608557028955872\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0001902634873921605, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001902634873921605\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9955736274586491, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9955736274586491\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5130725326374553, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5130725326374553\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00019608557028955872, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00019608557028955872\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0001902634873921605, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001902634873921605\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9955736274586491, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9955736274586491\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5130725326374553, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5130725326374553\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00019608557028955872, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00019608557028955872\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0001902634873921605, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0001902634873921605\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9955736274586491, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9955736274586491\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5130725326374553, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5130725326374553\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.00019608557028955872, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.00019608557028955872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:01,785]\u001b[0m Trial 132 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:35:01,959]\u001b[0m Trial 133 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.012151242685521379, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012151242685521379\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.711321626145426, subsample=1.0 will be ignored. Current value: bagging_fraction=0.711321626145426\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4535292058366024, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4535292058366024\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.082198125874709e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.082198125874709e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.012151242685521379, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012151242685521379\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.711321626145426, subsample=1.0 will be ignored. Current value: bagging_fraction=0.711321626145426\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4535292058366024, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4535292058366024\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.082198125874709e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.082198125874709e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.012151242685521379, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012151242685521379\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.711321626145426, subsample=1.0 will be ignored. Current value: bagging_fraction=0.711321626145426\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4535292058366024, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4535292058366024\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.082198125874709e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.082198125874709e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.012151242685521379, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012151242685521379\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.711321626145426, subsample=1.0 will be ignored. Current value: bagging_fraction=0.711321626145426\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4535292058366024, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4535292058366024\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.082198125874709e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.082198125874709e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.012151242685521379, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.012151242685521379\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.711321626145426, subsample=1.0 will be ignored. Current value: bagging_fraction=0.711321626145426\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4535292058366024, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4535292058366024\n",
            "[LightGBM] [Warning] lambda_l2 is set=8.082198125874709e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=8.082198125874709e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00017671893738378298, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00017671893738378298\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7774206747649373, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7774206747649373\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.43981067106802507, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.43981067106802507\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0029581081264180333, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0029581081264180333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:02,282]\u001b[0m Trial 134 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.00017671893738378298, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00017671893738378298\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7774206747649373, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7774206747649373\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.43981067106802507, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.43981067106802507\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0029581081264180333, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0029581081264180333\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00017671893738378298, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00017671893738378298\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7774206747649373, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7774206747649373\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.43981067106802507, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.43981067106802507\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0029581081264180333, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0029581081264180333\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00017671893738378298, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00017671893738378298\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7774206747649373, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7774206747649373\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.43981067106802507, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.43981067106802507\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0029581081264180333, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0029581081264180333\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00017671893738378298, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00017671893738378298\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7774206747649373, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7774206747649373\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.43981067106802507, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.43981067106802507\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0029581081264180333, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0029581081264180333\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0022696593950097023, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0022696593950097023\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7701215109399544, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7701215109399544\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6145602948738322, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6145602948738322\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.714883636325403, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.714883636325403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:02,506]\u001b[0m Trial 135 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0022696593950097023, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0022696593950097023\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7701215109399544, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7701215109399544\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6145602948738322, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6145602948738322\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.714883636325403, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.714883636325403\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0022696593950097023, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0022696593950097023\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7701215109399544, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7701215109399544\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6145602948738322, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6145602948738322\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.714883636325403, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.714883636325403\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0022696593950097023, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0022696593950097023\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7701215109399544, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7701215109399544\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6145602948738322, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6145602948738322\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.714883636325403, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.714883636325403\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0022696593950097023, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0022696593950097023\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7701215109399544, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7701215109399544\n",
            "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6145602948738322, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6145602948738322\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.714883636325403, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.714883636325403\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.7935606939155273e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.7935606939155273e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7162054526543498, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7162054526543498\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5427991037257801, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5427991037257801\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.499636254008386e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.499636254008386e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:02,738]\u001b[0m Trial 136 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=4.7935606939155273e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.7935606939155273e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7162054526543498, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7162054526543498\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5427991037257801, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5427991037257801\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.499636254008386e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.499636254008386e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.7935606939155273e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.7935606939155273e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7162054526543498, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7162054526543498\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5427991037257801, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5427991037257801\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.499636254008386e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.499636254008386e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.7935606939155273e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.7935606939155273e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7162054526543498, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7162054526543498\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5427991037257801, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5427991037257801\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.499636254008386e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.499636254008386e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.7935606939155273e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.7935606939155273e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7162054526543498, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7162054526543498\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5427991037257801, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5427991037257801\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.499636254008386e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.499636254008386e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.292055265752529e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.292055265752529e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6986615748756679, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6986615748756679\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6504809148656052, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6504809148656052\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.1836382275402494e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1836382275402494e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.292055265752529e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.292055265752529e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6986615748756679, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6986615748756679\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6504809148656052, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6504809148656052\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.1836382275402494e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1836382275402494e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:02,898]\u001b[0m Trial 137 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=3.292055265752529e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.292055265752529e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6986615748756679, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6986615748756679\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6504809148656052, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6504809148656052\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.1836382275402494e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1836382275402494e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.292055265752529e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.292055265752529e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6986615748756679, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6986615748756679\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6504809148656052, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6504809148656052\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.1836382275402494e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1836382275402494e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.292055265752529e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.292055265752529e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6986615748756679, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6986615748756679\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6504809148656052, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6504809148656052\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.1836382275402494e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.1836382275402494e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.8291984231970737, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.8291984231970737\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6291484720331533, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6291484720331533\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7828166556724088, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7828166556724088\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.023130952631381825, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.023130952631381825\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.8291984231970737, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.8291984231970737\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6291484720331533, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6291484720331533\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7828166556724088, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7828166556724088\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.023130952631381825, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.023130952631381825\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.8291984231970737, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.8291984231970737\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6291484720331533, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6291484720331533\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7828166556724088, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7828166556724088\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.023130952631381825, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.023130952631381825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:03,134]\u001b[0m Trial 138 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=3.8291984231970737, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.8291984231970737\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6291484720331533, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6291484720331533\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7828166556724088, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7828166556724088\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.023130952631381825, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.023130952631381825\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.8291984231970737, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.8291984231970737\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6291484720331533, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6291484720331533\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7828166556724088, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7828166556724088\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.023130952631381825, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.023130952631381825\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0890505054234991, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0890505054234991\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5808766055814915, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5808766055814915\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5260063899011973, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5260063899011973\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.27986101727596e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.27986101727596e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0890505054234991, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0890505054234991\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5808766055814915, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5808766055814915\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5260063899011973, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5260063899011973\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.27986101727596e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.27986101727596e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0890505054234991, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0890505054234991\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5808766055814915, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5808766055814915\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5260063899011973, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5260063899011973\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.27986101727596e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.27986101727596e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=1.0890505054234991, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0890505054234991\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5808766055814915, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5808766055814915\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5260063899011973, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5260063899011973\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.27986101727596e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.27986101727596e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:03,300]\u001b[0m Trial 139 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=1.0890505054234991, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0890505054234991\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5808766055814915, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5808766055814915\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5260063899011973, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5260063899011973\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.27986101727596e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.27986101727596e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.263698367842315e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.263698367842315e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8414613507795401, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8414613507795401\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.679727847306967, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.679727847306967\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0018662167162109343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0018662167162109343\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.263698367842315e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.263698367842315e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8414613507795401, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8414613507795401\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.679727847306967, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.679727847306967\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0018662167162109343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0018662167162109343\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.263698367842315e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.263698367842315e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8414613507795401, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8414613507795401\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.679727847306967, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.679727847306967\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0018662167162109343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0018662167162109343\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.263698367842315e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.263698367842315e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8414613507795401, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8414613507795401\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.679727847306967, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.679727847306967\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0018662167162109343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0018662167162109343\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.263698367842315e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.263698367842315e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8414613507795401, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8414613507795401\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.679727847306967, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.679727847306967\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0018662167162109343, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0018662167162109343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:03,489]\u001b[0m Trial 140 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.4780644117660276e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.4780644117660276e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5512230333169753, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5512230333169753\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7836541387275002, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7836541387275002\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.001980640976683315, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001980640976683315\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.4780644117660276e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.4780644117660276e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5512230333169753, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5512230333169753\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7836541387275002, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7836541387275002\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.001980640976683315, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001980640976683315\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.4780644117660276e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.4780644117660276e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5512230333169753, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5512230333169753\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7836541387275002, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7836541387275002\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.001980640976683315, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001980640976683315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:03,823]\u001b[0m Trial 141 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=2.4780644117660276e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.4780644117660276e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5512230333169753, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5512230333169753\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7836541387275002, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7836541387275002\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.001980640976683315, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001980640976683315\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.4780644117660276e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.4780644117660276e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5512230333169753, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5512230333169753\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7836541387275002, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7836541387275002\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.001980640976683315, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.001980640976683315\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.4375624626365546e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.4375624626365546e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6750092814354093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6750092814354093\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6964779952305066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6964779952305066\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006217017561019461, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006217017561019461\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.4375624626365546e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.4375624626365546e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6750092814354093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6750092814354093\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6964779952305066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6964779952305066\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006217017561019461, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006217017561019461\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.4375624626365546e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.4375624626365546e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6750092814354093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6750092814354093\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6964779952305066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6964779952305066\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006217017561019461, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006217017561019461\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.4375624626365546e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.4375624626365546e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6750092814354093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6750092814354093\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:03,982]\u001b[0m Trial 142 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] feature_fraction is set=0.6964779952305066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6964779952305066\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006217017561019461, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006217017561019461\n",
            "[LightGBM] [Warning] lambda_l1 is set=3.4375624626365546e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.4375624626365546e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6750092814354093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6750092814354093\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6964779952305066, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6964779952305066\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.006217017561019461, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.006217017561019461\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0002340955753186716, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002340955753186716\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4287913320876309, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4287913320876309\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5318889420687334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5318889420687334\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.579186262842363e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.579186262842363e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0002340955753186716, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002340955753186716\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4287913320876309, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4287913320876309\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5318889420687334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5318889420687334\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.579186262842363e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.579186262842363e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0002340955753186716, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002340955753186716\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4287913320876309, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4287913320876309\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5318889420687334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5318889420687334\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.579186262842363e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.579186262842363e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.0002340955753186716, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002340955753186716\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4287913320876309, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4287913320876309\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5318889420687334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5318889420687334\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.579186262842363e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.579186262842363e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:04,194]\u001b[0m Trial 143 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.0002340955753186716, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0002340955753186716\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4287913320876309, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4287913320876309\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5318889420687334, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5318889420687334\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.579186262842363e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.579186262842363e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.07510896315776097, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.07510896315776097\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9094256122408053, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9094256122408053\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.49490675977282045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.49490675977282045\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.033738285292254505, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.033738285292254505\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.07510896315776097, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.07510896315776097\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9094256122408053, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9094256122408053\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.49490675977282045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.49490675977282045\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.033738285292254505, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.033738285292254505\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.07510896315776097, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.07510896315776097\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9094256122408053, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9094256122408053\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.49490675977282045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.49490675977282045\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.033738285292254505, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.033738285292254505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:04,453]\u001b[0m Trial 144 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.07510896315776097, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.07510896315776097\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9094256122408053, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9094256122408053\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.49490675977282045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.49490675977282045\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.033738285292254505, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.033738285292254505\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.07510896315776097, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.07510896315776097\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9094256122408053, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9094256122408053\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.49490675977282045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.49490675977282045\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.033738285292254505, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.033738285292254505\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.531281799067401e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.531281799067401e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8544892388088492, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8544892388088492\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4378445499405041, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4378445499405041\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.2938136769050824e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.2938136769050824e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.531281799067401e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.531281799067401e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8544892388088492, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8544892388088492\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4378445499405041, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4378445499405041\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.2938136769050824e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.2938136769050824e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:04,715]\u001b[0m Trial 145 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=4.531281799067401e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.531281799067401e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8544892388088492, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8544892388088492\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4378445499405041, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4378445499405041\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.2938136769050824e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.2938136769050824e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.531281799067401e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.531281799067401e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8544892388088492, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8544892388088492\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4378445499405041, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4378445499405041\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.2938136769050824e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.2938136769050824e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.531281799067401e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.531281799067401e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8544892388088492, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8544892388088492\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4378445499405041, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4378445499405041\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.2938136769050824e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.2938136769050824e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.007556198811581949, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.007556198811581949\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5406598432750361, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5406598432750361\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6013825874530621, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6013825874530621\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.020001033893719364, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.020001033893719364\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.007556198811581949, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.007556198811581949\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5406598432750361, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5406598432750361\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6013825874530621, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6013825874530621\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.020001033893719364, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.020001033893719364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:04,913]\u001b[0m Trial 146 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.007556198811581949, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.007556198811581949\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5406598432750361, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5406598432750361\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6013825874530621, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6013825874530621\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.020001033893719364, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.020001033893719364\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.007556198811581949, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.007556198811581949\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5406598432750361, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5406598432750361\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6013825874530621, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6013825874530621\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.020001033893719364, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.020001033893719364\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.007556198811581949, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.007556198811581949\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.5406598432750361, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5406598432750361\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6013825874530621, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6013825874530621\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.020001033893719364, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.020001033893719364\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.000515713224380253, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.000515713224380253\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46625097964794715, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46625097964794715\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.40495993036465466, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.40495993036465466\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.882960322752246e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.882960322752246e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.000515713224380253, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.000515713224380253\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46625097964794715, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46625097964794715\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.40495993036465466, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.40495993036465466\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.882960322752246e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.882960322752246e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.000515713224380253, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.000515713224380253\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46625097964794715, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46625097964794715\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.40495993036465466, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.40495993036465466\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.882960322752246e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.882960322752246e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.000515713224380253, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.000515713224380253\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46625097964794715, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46625097964794715\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.40495993036465466, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.40495993036465466\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.882960322752246e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.882960322752246e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:05,071]\u001b[0m Trial 147 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.000515713224380253, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.000515713224380253\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46625097964794715, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46625097964794715\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.40495993036465466, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.40495993036465466\n",
            "[LightGBM] [Warning] lambda_l2 is set=6.882960322752246e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.882960322752246e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.054993831408839e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.054993831408839e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6960526190144604, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6960526190144604\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.602332743552562, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.602332743552562\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.759059715316887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.759059715316887\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.054993831408839e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.054993831408839e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6960526190144604, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6960526190144604\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.602332743552562, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.602332743552562\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.759059715316887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.759059715316887\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.054993831408839e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.054993831408839e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6960526190144604, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6960526190144604\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.602332743552562, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.602332743552562\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.759059715316887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.759059715316887\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.054993831408839e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.054993831408839e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6960526190144604, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6960526190144604\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.602332743552562, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.602332743552562\n",
            "[LightGBM] [Warning] lambda_l2 is set=9.759059715316887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.759059715316887\n",
            "[LightGBM] [Warning] lambda_l1 is set=5.054993831408839e-06, reg_alpha=0.0 will be ignored. Current value: lambda_l1=5.054993831408839e-06\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6960526190144604, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6960526190144604\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.602332743552562, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.602332743552562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:05,289]\u001b[0m Trial 148 failed, because the objective function returned nan.\u001b[0m\n",
            "\u001b[33m[W 2022-03-01 13:35:05,437]\u001b[0m Trial 149 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l2 is set=9.759059715316887, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.759059715316887\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.905157846368559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.905157846368559\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4472730644139068, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4472730644139068\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4666379120889117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4666379120889117\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003588102271750361, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003588102271750361\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.905157846368559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.905157846368559\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4472730644139068, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4472730644139068\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4666379120889117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4666379120889117\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003588102271750361, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003588102271750361\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.905157846368559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.905157846368559\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4472730644139068, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4472730644139068\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4666379120889117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4666379120889117\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003588102271750361, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003588102271750361\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.905157846368559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.905157846368559\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4472730644139068, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4472730644139068\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4666379120889117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4666379120889117\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003588102271750361, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003588102271750361\n",
            "[LightGBM] [Warning] lambda_l1 is set=2.905157846368559, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.905157846368559\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4472730644139068, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4472730644139068\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4666379120889117, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4666379120889117\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0003588102271750361, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0003588102271750361\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.021544892141598517, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.021544892141598517\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7331636129668784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7331636129668784\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.607674690005271, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.607674690005271\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.5611456511701624e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.5611456511701624e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:05,625]\u001b[0m Trial 150 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.021544892141598517, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.021544892141598517\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7331636129668784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7331636129668784\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.607674690005271, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.607674690005271\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.5611456511701624e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.5611456511701624e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.021544892141598517, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.021544892141598517\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7331636129668784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7331636129668784\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.607674690005271, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.607674690005271\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.5611456511701624e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.5611456511701624e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.021544892141598517, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.021544892141598517\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7331636129668784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7331636129668784\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.607674690005271, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.607674690005271\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.5611456511701624e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.5611456511701624e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.021544892141598517, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.021544892141598517\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7331636129668784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7331636129668784\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.607674690005271, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.607674690005271\n",
            "[LightGBM] [Warning] lambda_l2 is set=5.5611456511701624e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=5.5611456511701624e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.016091700267647e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.016091700267647e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4066214471449266, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4066214471449266\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5133504379321278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5133504379321278\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.058242907380513e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.058242907380513e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.016091700267647e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.016091700267647e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4066214471449266, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4066214471449266\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5133504379321278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5133504379321278\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.058242907380513e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.058242907380513e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:05,842]\u001b[0m Trial 151 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=4.016091700267647e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.016091700267647e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4066214471449266, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4066214471449266\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5133504379321278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5133504379321278\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.058242907380513e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.058242907380513e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.016091700267647e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.016091700267647e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4066214471449266, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4066214471449266\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5133504379321278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5133504379321278\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.058242907380513e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.058242907380513e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.016091700267647e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.016091700267647e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4066214471449266, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4066214471449266\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] feature_fraction is set=0.5133504379321278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5133504379321278\n",
            "[LightGBM] [Warning] lambda_l2 is set=1.058242907380513e-06, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.058242907380513e-06\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.4382708129615875e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.4382708129615875e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8712033747634411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8712033747634411\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7306764358362408, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7306764358362408\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.009674148822036651, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.009674148822036651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:06,156]\u001b[0m Trial 152 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=4.4382708129615875e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.4382708129615875e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8712033747634411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8712033747634411\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7306764358362408, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7306764358362408\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.009674148822036651, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.009674148822036651\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.4382708129615875e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.4382708129615875e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8712033747634411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8712033747634411\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7306764358362408, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7306764358362408\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.009674148822036651, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.009674148822036651\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.4382708129615875e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.4382708129615875e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8712033747634411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8712033747634411\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7306764358362408, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7306764358362408\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.009674148822036651, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.009674148822036651\n",
            "[LightGBM] [Warning] lambda_l1 is set=4.4382708129615875e-05, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4.4382708129615875e-05\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8712033747634411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8712033747634411\n",
            "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
            "[LightGBM] [Warning] feature_fraction is set=0.7306764358362408, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7306764358362408\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.009674148822036651, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.009674148822036651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:06,313]\u001b[0m Trial 153 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.09125495784434783, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09125495784434783\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4773782677241345, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4773782677241345\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.623878070417509, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.623878070417509\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0022021621791832326, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0022021621791832326\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.09125495784434783, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09125495784434783\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4773782677241345, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4773782677241345\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.623878070417509, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.623878070417509\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0022021621791832326, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0022021621791832326\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.09125495784434783, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09125495784434783\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4773782677241345, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4773782677241345\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.623878070417509, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.623878070417509\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0022021621791832326, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0022021621791832326\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.09125495784434783, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09125495784434783\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4773782677241345, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4773782677241345\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.623878070417509, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.623878070417509\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0022021621791832326, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0022021621791832326\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.09125495784434783, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.09125495784434783\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.4773782677241345, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4773782677241345\n",
            "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
            "[LightGBM] [Warning] feature_fraction is set=0.623878070417509, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.623878070417509\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.0022021621791832326, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0022021621791832326\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.006369485079807e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.006369485079807e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45448062968452146, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45448062968452146\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8717457304159626, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8717457304159626\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.482279801785502e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.482279801785502e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:06,497]\u001b[0m Trial 154 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=7.006369485079807e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.006369485079807e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45448062968452146, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45448062968452146\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8717457304159626, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8717457304159626\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.482279801785502e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.482279801785502e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.006369485079807e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.006369485079807e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45448062968452146, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45448062968452146\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8717457304159626, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8717457304159626\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.482279801785502e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.482279801785502e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.006369485079807e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.006369485079807e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45448062968452146, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45448062968452146\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8717457304159626, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8717457304159626\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.482279801785502e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.482279801785502e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.006369485079807e-08, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.006369485079807e-08\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.45448062968452146, subsample=1.0 will be ignored. Current value: bagging_fraction=0.45448062968452146\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8717457304159626, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8717457304159626\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.482279801785502e-08, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.482279801785502e-08\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009691884884171945, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009691884884171945\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7768924531711308, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7768924531711308\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8050155448060112, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8050155448060112\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07012727791831193, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07012727791831193\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009691884884171945, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009691884884171945\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7768924531711308, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7768924531711308\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8050155448060112, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8050155448060112\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07012727791831193, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07012727791831193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:06,718]\u001b[0m Trial 155 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.009691884884171945, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009691884884171945\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7768924531711308, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7768924531711308\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8050155448060112, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8050155448060112\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07012727791831193, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07012727791831193\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009691884884171945, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009691884884171945\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7768924531711308, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7768924531711308\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8050155448060112, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8050155448060112\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07012727791831193, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07012727791831193\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.009691884884171945, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.009691884884171945\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.7768924531711308, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7768924531711308\n",
            "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
            "[LightGBM] [Warning] feature_fraction is set=0.8050155448060112, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8050155448060112\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.07012727791831193, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.07012727791831193\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00527626703913376, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00527626703913376\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46090573557258707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46090573557258707\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6826383359613339, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6826383359613339\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.007813779328256979, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007813779328256979\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00527626703913376, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00527626703913376\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46090573557258707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46090573557258707\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6826383359613339, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6826383359613339\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.007813779328256979, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007813779328256979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2022-03-01 13:35:06,953]\u001b[0m Trial 156 failed, because the objective function returned nan.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] lambda_l1 is set=0.00527626703913376, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00527626703913376\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46090573557258707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46090573557258707\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6826383359613339, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6826383359613339\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.007813779328256979, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007813779328256979\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00527626703913376, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00527626703913376\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46090573557258707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46090573557258707\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6826383359613339, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6826383359613339\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.007813779328256979, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007813779328256979\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.00527626703913376, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.00527626703913376\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.46090573557258707, subsample=1.0 will be ignored. Current value: bagging_fraction=0.46090573557258707\n",
            "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
            "[LightGBM] [Warning] feature_fraction is set=0.6826383359613339, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6826383359613339\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.007813779328256979, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.007813779328256979\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.447187759966668e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.447187759966668e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6746625208405252, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6746625208405252\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4855829831516105, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4855829831516105\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.4778387975974107e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4778387975974107e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.447187759966668e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.447187759966668e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6746625208405252, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6746625208405252\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4855829831516105, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4855829831516105\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.4778387975974107e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4778387975974107e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.447187759966668e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.447187759966668e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6746625208405252, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6746625208405252\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4855829831516105, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4855829831516105\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.4778387975974107e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4778387975974107e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.447187759966668e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.447187759966668e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6746625208405252, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6746625208405252\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4855829831516105, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4855829831516105\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.4778387975974107e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4778387975974107e-05\n",
            "[LightGBM] [Warning] lambda_l1 is set=7.447187759966668e-07, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.447187759966668e-07\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.6746625208405252, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6746625208405252\n",
            "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4855829831516105, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4855829831516105\n",
            "[LightGBM] [Warning] lambda_l2 is set=2.4778387975974107e-05, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.4778387975974107e-05\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-157-b9a4ec087d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mLGB_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mLGB_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLGB_objective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-156-e10bb1b92ca7>\u001b[0m in \u001b[0;36mLGB_objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_over\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_over\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_over\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mf1_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcloned_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcloned_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mRead\u001b[0m \u001b[0mmore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mUser\u001b[0m \u001b[0mGuide\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0mParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mestimator\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mimplementing\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0;31m# The iterable was consumed all at once by the above for loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;34m\"\"\"Display the message on stout or stderr depending on verbosity\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# grow. To ensure correct results ordering, .insert is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# used (rather than .append) in the following line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mUser\u001b[0m \u001b[0mGuide\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvarious\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0mstrategies\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mhere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionchanged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.22\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0mchanged\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfold\u001b[0m \u001b[0mto\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0meval_init_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                     categorical_feature=categorical_feature, callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[1;32m    900\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0minit_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m         )\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3021\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3023\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   3024\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qzC_qEwIlYLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nUlA7K1PlYOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_proba = optimized_KNN.predict_proba(test)[:,0]"
      ],
      "metadata": {
        "id": "6jLAphOZeRf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_proba = optimized_RF.predict_proba(test)[:,0]"
      ],
      "metadata": {
        "id": "fgwHPRnCe0Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_proba = optimized_XGB.predict_proba(test)[:,0]"
      ],
      "metadata": {
        "id": "ytZkS6e-e84W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVC_proba = optimized_SVC.predict_proba(test)[:,0]"
      ],
      "metadata": {
        "id": "noI_WpfDvPi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.where((knn_proba + rf_proba + xgb_proba+ SVC_proba)/4>=0.333)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeK9KYhae-w0",
        "outputId": "0f85f2b0-48e5-43d3-96f3-090da383ebe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 20,  40,  48,  60,  62,  67,  84,  91,  99, 122]),)"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.where((knn_proba)>=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQZU7CssfYeX",
        "outputId": "eaa4634f-a513-4df8-801c-3d988767c14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  2,   3,   8,  13,  20,  21,  40,  48,  52,  58,  59,  60,  62,\n",
              "         67,  68,  71,  72,  84,  85,  91,  99, 102, 104, 110, 112, 113,\n",
              "        117, 120, 122, 124, 126]),)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "                                          ('optimized_SVC', optimized_SVC)                                      "
      ],
      "metadata": {
        "id": "6fQV54j80iDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "voting_c_soft = VotingClassifier(estimators = [\n",
        "                                          ('optimized_KNN', optimized_KNN),\n",
        "                                          ('optimized_RF', optimized_RF),\n",
        "                                          ('optimized_XGB', optimized_XGB),\n",
        "                                          ('optimized_SVC', optimized_SVC) \n",
        "                                         ], \n",
        "                            voting = 'soft')"
      ],
      "metadata": {
        "id": "lKpgMAv0gSIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit on training, test on testing set\n",
        "voting_c_soft.fit(X_over, y_over)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3Afk_4ZhCNB",
        "outputId": "116e3c19-41f7-49c0-8180-09c1bf9410c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('optimized_KNN',\n",
              "                              KNeighborsClassifier(algorithm='kd_tree',\n",
              "                                                   n_neighbors=2, p=1)),\n",
              "                             ('optimized_RF',\n",
              "                              RandomForestClassifier(max_depth=4,\n",
              "                                                     max_leaf_nodes=219,\n",
              "                                                     n_estimators=263,\n",
              "                                                     random_state=25)),\n",
              "                             ('optimized_XGB',\n",
              "                              XGBClassifier(colsample_bylevel=0.03680792380971084,\n",
              "                                            colsample_bynode=0.4037780233869643,\n",
              "                                            colsample_bytree=0.3588367800362946,\n",
              "                                            eta=0.011458247652151282,\n",
              "                                            max_depth=20,\n",
              "                                            subsample=0.7466211067904115)),\n",
              "                             ('optimized_SVC',\n",
              "                              SVC(C=514.4366005927387, gamma='auto',\n",
              "                                  probability=True))],\n",
              "                 voting='soft')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = voting_c_soft.predict(test) \n",
        "np.where(y_pred==0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p37JhniuhVmK",
        "outputId": "5c7a0b56-191b-49af-b99c-22571591da2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 62,  84,  91, 122]),)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = voting_c_soft.predict_proba(test) \n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFDxUjlmdyEp",
        "outputId": "3de7d61d-23aa-46c9-c106-315708251607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00562925, 0.99437075],\n",
              "       [0.02456454, 0.97543546],\n",
              "       [0.15958612, 0.84041388],\n",
              "       [0.29052892, 0.70947108],\n",
              "       [0.01557284, 0.98442716],\n",
              "       [0.02094764, 0.97905236],\n",
              "       [0.02430788, 0.97569212],\n",
              "       [0.01891529, 0.98108471],\n",
              "       [0.33216735, 0.66783265],\n",
              "       [0.00461257, 0.99538743],\n",
              "       [0.00522782, 0.99477218],\n",
              "       [0.06250808, 0.93749192],\n",
              "       [0.03029494, 0.96970506],\n",
              "       [0.29766658, 0.70233342],\n",
              "       [0.01184827, 0.98815173],\n",
              "       [0.02473509, 0.97526491],\n",
              "       [0.0306404 , 0.9693596 ],\n",
              "       [0.05161852, 0.94838148],\n",
              "       [0.02788219, 0.97211781],\n",
              "       [0.00643952, 0.99356048],\n",
              "       [0.33482994, 0.66517006],\n",
              "       [0.29858168, 0.70141832],\n",
              "       [0.02524552, 0.97475448],\n",
              "       [0.06049256, 0.93950744],\n",
              "       [0.01879563, 0.98120437],\n",
              "       [0.17296305, 0.82703695],\n",
              "       [0.0073967 , 0.9926033 ],\n",
              "       [0.06788765, 0.93211235],\n",
              "       [0.1350748 , 0.8649252 ],\n",
              "       [0.01348591, 0.98651409],\n",
              "       [0.31290529, 0.68709471],\n",
              "       [0.08444492, 0.91555508],\n",
              "       [0.01234917, 0.98765083],\n",
              "       [0.17272322, 0.82727678],\n",
              "       [0.08662826, 0.91337174],\n",
              "       [0.02515957, 0.97484043],\n",
              "       [0.10842014, 0.89157986],\n",
              "       [0.06644202, 0.93355798],\n",
              "       [0.02180196, 0.97819804],\n",
              "       [0.04270495, 0.95729505],\n",
              "       [0.35509782, 0.64490218],\n",
              "       [0.03681989, 0.96318011],\n",
              "       [0.0894358 , 0.9105642 ],\n",
              "       [0.02967899, 0.97032101],\n",
              "       [0.02355348, 0.97644652],\n",
              "       [0.04169361, 0.95830639],\n",
              "       [0.04803033, 0.95196967],\n",
              "       [0.07942688, 0.92057312],\n",
              "       [0.34787684, 0.65212316],\n",
              "       [0.03054236, 0.96945764],\n",
              "       [0.00660186, 0.99339814],\n",
              "       [0.01574271, 0.98425729],\n",
              "       [0.31696182, 0.68303818],\n",
              "       [0.07192391, 0.92807609],\n",
              "       [0.1434699 , 0.8565301 ],\n",
              "       [0.02100789, 0.97899211],\n",
              "       [0.02599965, 0.97400035],\n",
              "       [0.09432623, 0.90567377],\n",
              "       [0.32949677, 0.67050323],\n",
              "       [0.2878294 , 0.7121706 ],\n",
              "       [0.38037604, 0.61962396],\n",
              "       [0.05990605, 0.94009395],\n",
              "       [0.68506339, 0.31493661],\n",
              "       [0.28308433, 0.71691567],\n",
              "       [0.02731943, 0.97268057],\n",
              "       [0.00880855, 0.99119145],\n",
              "       [0.01857584, 0.98142416],\n",
              "       [0.32189323, 0.67810677],\n",
              "       [0.29145523, 0.70854477],\n",
              "       [0.15906029, 0.84093971],\n",
              "       [0.00620992, 0.99379008],\n",
              "       [0.28990035, 0.71009965],\n",
              "       [0.28990035, 0.71009965],\n",
              "       [0.02093574, 0.97906426],\n",
              "       [0.01470795, 0.98529205],\n",
              "       [0.0105097 , 0.9894903 ],\n",
              "       [0.01523756, 0.98476244],\n",
              "       [0.01169093, 0.98830907],\n",
              "       [0.09108025, 0.90891975],\n",
              "       [0.05017576, 0.94982424],\n",
              "       [0.01618134, 0.98381866],\n",
              "       [0.01160496, 0.98839504],\n",
              "       [0.23011758, 0.76988242],\n",
              "       [0.11046437, 0.88953563],\n",
              "       [0.56874742, 0.43125258],\n",
              "       [0.16376956, 0.83623044],\n",
              "       [0.01705693, 0.98294307],\n",
              "       [0.00877258, 0.99122742],\n",
              "       [0.04187766, 0.95812234],\n",
              "       [0.30590826, 0.69409174],\n",
              "       [0.01079817, 0.98920183],\n",
              "       [0.57344091, 0.42655909],\n",
              "       [0.01404809, 0.98595191],\n",
              "       [0.00871667, 0.99128333],\n",
              "       [0.09090123, 0.90909877],\n",
              "       [0.00603634, 0.99396366],\n",
              "       [0.06168161, 0.93831839],\n",
              "       [0.01876225, 0.98123775],\n",
              "       [0.10181944, 0.89818056],\n",
              "       [0.34590375, 0.65409625],\n",
              "       [0.06031597, 0.93968403],\n",
              "       [0.11262589, 0.88737411],\n",
              "       [0.29380799, 0.70619201],\n",
              "       [0.03764514, 0.96235486],\n",
              "       [0.28718145, 0.71281855],\n",
              "       [0.10725776, 0.89274224],\n",
              "       [0.03379678, 0.96620322],\n",
              "       [0.01537906, 0.98462094],\n",
              "       [0.03943602, 0.96056398],\n",
              "       [0.0234652 , 0.9765348 ],\n",
              "       [0.30847038, 0.69152962],\n",
              "       [0.03459528, 0.96540472],\n",
              "       [0.16441789, 0.83558211],\n",
              "       [0.27592087, 0.72407913],\n",
              "       [0.09647724, 0.90352276],\n",
              "       [0.09643507, 0.90356493],\n",
              "       [0.05784578, 0.94215422],\n",
              "       [0.28378584, 0.71621416],\n",
              "       [0.09238472, 0.90761528],\n",
              "       [0.24370353, 0.75629647],\n",
              "       [0.16121002, 0.83878998],\n",
              "       [0.09936362, 0.90063638],\n",
              "       [0.6405352 , 0.35946481],\n",
              "       [0.04670748, 0.95329252],\n",
              "       [0.32727404, 0.67272596],\n",
              "       [0.0493951 , 0.9506049 ],\n",
              "       [0.33023938, 0.66976062]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv('submission_sample.csv')\n",
        "submission['OC'] = y_pred\n",
        "submission.to_csv('submission_final3.csv')"
      ],
      "metadata": {
        "id": "TjVDFm5ZibLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2DQECC4xw5rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8xHYGo2KFRG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2lzppMMiFRJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "382ca9c1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}